{"title": "Cooperative Learning of Disjoint Syntax and Semantics", "abstract": "There has been considerable attention devoted to models that learn to jointly infer an expression's syntactic structure and its semantics. Yet, Nangia and Bowman (2018) has recently shown that the current best systems fail to learn the correct parsing strategy on mathematical expressions generated from a simple context-free grammar. In this work, we present a recursive model inspired by Choi et al. (2018) that reaches near perfect accuracy on this task. Our model is composed of two separated modules for syntax and semantics. They are cooperatively trained with standard continuous and discrete optimisation schemes. Our model does not require any linguistic structure for supervision, and its recursive nature allows for out-of-domain generalisation. Additionally, our approach performs competitively on several natural language tasks, such as Natural Language Inference and Sentiment Analysis. * Work done while the author was an intern at Facebook AI Research.", "text": [{"id": 0, "string": "Introduction Standard linguistic theories propose that natural language is structured as nested constituents organised in the form of a tree (Partee et al., 1990) ."}, {"id": 1, "string": "However, most popular models, such as the Long Sort-Term Memory network (LSTM) (Hochreiter and Schmidhuber, 1997) , process text without imposing a grammatical structure."}, {"id": 2, "string": "To bridge this gap between theory and practice models that process linguistic expressions in a tree-structured manner have been considered in recent work (Socher et al., 2013; Tai et al., 2015; Zhu et al., 2015; Bowman et al., 2016) ."}, {"id": 3, "string": "These tree-based models explicitly require access to the syntactic structure for the text, which is not entirely satisfactory."}, {"id": 4, "string": "Indeed, parse tree level supervision requires a significant amount of annotations from expert lin-guists."}, {"id": 5, "string": "These trees have been annotated with different goals in mind than the tasks we are using them for."}, {"id": 6, "string": "Such discrepancy may result in a deterioration of the performance of models relying on them."}, {"id": 7, "string": "Recently, several attempts were made to learn these models without explicit supervision for the parser (Yogatama et al., 2016; Maillard et al., 2017; Choi et al., 2018) ."}, {"id": 8, "string": "However, Williams et al."}, {"id": 9, "string": "(2018a) has recently shown that the structures learned by these models cannot be ascribed to discovering meaningful syntactic structure."}, {"id": 10, "string": "These models even fail to learn the simple context-free grammar of nested mathematical operations (Nangia and Bowman, 2018) ."}, {"id": 11, "string": "In this work, we present an extension of Choi et al."}, {"id": 12, "string": "(2018) , that successfully learns these simple grammars while preserving competitive performance on several standard linguistic tasks."}, {"id": 13, "string": "Contrary to previous work, our model makes a clear distinction between the parser and the compositional function."}, {"id": 14, "string": "These two modules are trained with different algorithms, cooperating to build a semantic representation that optimises the objective function."}, {"id": 15, "string": "The parser's goal is to generate a tree structure for the sentence."}, {"id": 16, "string": "The compositional function follows this structure to produce the sentence representation."}, {"id": 17, "string": "Our model contains a continuous component, the compositional function, and a discrete one, the parser."}, {"id": 18, "string": "The whole system is trained end-to-end with a mix of reinforcement learning and gradient descent."}, {"id": 19, "string": "Drozdov and Bowman (2017) has noticed the difficulty of mixing these two optimisation schemes without one dominating the other."}, {"id": 20, "string": "This typically leads to the \"coadaptation problem\" where the parser simply follows the compositional function and fails to produce meaningful syntactic structures."}, {"id": 21, "string": "In this work, we show that this pitfall can be avoided by synchronising the learning paces of the two optimisation schemes."}, {"id": 22, "string": "This is achieved by com-bining several recent advances in reinforcement learning."}, {"id": 23, "string": "First, we use input-dependent control variates to reduce the variance of our gradient estimates (Ross, 1997) ."}, {"id": 24, "string": "Then, we apply multiple gradient steps to the parser's policy while controlling for its learning pace using the Proximal Policy Optimization (PPO) of Schulman et al."}, {"id": 25, "string": "(2017) ."}, {"id": 26, "string": "The code for our model is publicly available 1 ."}, {"id": 27, "string": "Preliminaries In this section, we present existing works on Recursive Neural Networks and their training in the absence of supervision on the syntactic structures."}, {"id": 28, "string": "Recursive Neural Networks A Recursive Neural Network (RvNN) has its architecture defined by a directed acyclic graph (DAG) given alongside with an input sequence (Goller and Kuchler, 1996) ."}, {"id": 29, "string": "RvNNs are commonly used in NLP to generate sentence representation that leverages available syntactic information, such as a constituency or a dependency parse trees (Socher et al., 2011) ."}, {"id": 30, "string": "Given an input sequence and its associated DAG, a RvNN processes the sequence by applying a transformation to the representations of the tokens lying on the lowest levels of the DAG."}, {"id": 31, "string": "This transformation, or compositional function, merges these representations into representations for the nodes on the next level of the DAG."}, {"id": 32, "string": "This process is repeated recursively along the graph structure until the top-level nodes are reached."}, {"id": 33, "string": "In this work, we assume that the compositional function is the same for every node in the graph."}, {"id": 34, "string": "Tree-LSTM."}, {"id": 35, "string": "We focus on a specific type of RvNNs, the tree-based long short-term memory network (Tree-LSTM) of Tai et al."}, {"id": 36, "string": "(2015) and Zhu et al."}, {"id": 37, "string": "(2015) ."}, {"id": 38, "string": "Its compositional function generalizes the LSTM cell of Hochreiter and Schmidhuber (1997) to tree-structured topologies, i.e., \uf8ee \uf8ef \uf8ef \uf8ef \uf8ef \uf8f0 z i f l f r o \uf8f9 \uf8fa \uf8fa \uf8fa \uf8fa \uf8fb = \uf8ee \uf8ef \uf8ef \uf8ef \uf8ef \uf8f0 tanh \u03c3 \u03c3 \u03c3 \u03c3 \uf8f9 \uf8fa \uf8fa \uf8fa \uf8fa \uf8fb R h l h r + b , c p = z i + c l f l + c r f r , h p = tanh(c p ) o, where \u03c3 and tanh are the sigmoid and hyperbolic tangent functions."}, {"id": 39, "string": "Tree-LSTM cell is differentiable with respect to its recursion matrix R, bias b and its input."}, {"id": 40, "string": "The gradients of a Tree-LSTM can thus be computed with backpropagation through structure (BPTS) (Goller and Kuchler, 1996) ."}, {"id": 41, "string": "Learning with RvNNs A tree-based RvNN is a function f \u03b8 parameterized by a d dimensional vector \u03b8 that predicts an output y given an input x and a tree t. Given a dataset D of N triplets (x, t, y), the parameters of the RvNN are learned with the following minimisation problem: min \u03b8\u2208R d 1 N (x,t,y)\u2208D (f \u03b8 (x, t), y), (1) where is a logistic regression function."}, {"id": 42, "string": "These models need an externally provided parsing tree for each input sentence during both training and evaluation."}, {"id": 43, "string": "Alternatives, such as the shift-reducebased SPINN model of Bowman et al."}, {"id": 44, "string": "(2016) , learn an internal parser from the given trees."}, {"id": 45, "string": "While these solutions do not need external trees during evaluation, they still require tree level annotations for training."}, {"id": 46, "string": "More recent work has focused on learning a latent parser with no direct supervision."}, {"id": 47, "string": "Latent tree models Latent tree models aim at jointly learning the compositional function f \u03b8 and a parser without supervision on the syntactic structures (Yogatama et al., 2016; Maillard et al., 2017; Choi et al., 2018) ."}, {"id": 48, "string": "The latent parser is defined as a parametric probability distribution over trees conditioned on the input sequence."}, {"id": 49, "string": "The parameters of this tree distribution p \u03c6 (.|x) are represented by a vector \u03c6."}, {"id": 50, "string": "Given a dataset D of pairs of input sequences x and outputs y, the parameters \u03b8 and \u03c6 are jointly learned by minimising the following objective function: min \u03b8,\u03c6 L(\u03b8, \u03c6) = 1 N (x,y) (E \u03c6 [f \u03b8 (x, t)], y), (2) where E \u03c6 is the expectation with respect to the p \u03c6 (.|x) distribution."}, {"id": 51, "string": "Directly minimising this objective function is often difficult due to expensive marginalisation of the unobserved trees."}, {"id": 52, "string": "Hence, when is a convex function (e.g."}, {"id": 53, "string": "cross entropy of an exponential family) usually an upper bound of Eq."}, {"id": 54, "string": "(2) can be derived by applying Jensen's inequality: L(\u03b8, \u03c6) = 1 N (x,y) E \u03c6 [ (f \u03b8 (x, t), y)]."}, {"id": 55, "string": "(3) Learning a distribution over a set of discrete items involves a discrete optimisation scheme."}, {"id": 56, "string": "For example, the RL-SPINN model of Yogatama et al."}, {"id": 57, "string": "(2016) uses a mix of gradient descent for \u03b8 and REINFORCE for \u03c6 (Williams et al., 2018a) ."}, {"id": 58, "string": "Drozdov and Bowman (2017) has recently observed that this optimisation strategy tends to produce poor parsers, e.g., parsers that only generate left-branching trees."}, {"id": 59, "string": "The effect, called the coadaptation issue, is caused by both bias in the parsing strategy and a difference in convergence paces of continuous and discrete optimisers."}, {"id": 60, "string": "Typically, the parameters \u03b8 are learned more rapidly than \u03c6."}, {"id": 61, "string": "This limits the exploration of the search space to parsing strategies similar to those found at the beginning of the training."}, {"id": 62, "string": "Gumbel Tree-LSTM In their Gumbel Tree-LSTM model, Choi et al."}, {"id": 63, "string": "(2018) propose an alternative parsing strategy to avoid the coadaptation issue."}, {"id": 64, "string": "Their parser incrementally merges a pair of consecutive constituents until a single one remains."}, {"id": 65, "string": "This strategy reduces the bias towards certain tree configurations observed with RL-SPINN."}, {"id": 66, "string": "Each word i of the input sequence is represented by an embedding vector."}, {"id": 67, "string": "A leaf transformation maps this vector to pair of vectors r 0 i =(h 0 i , c 0 i )."}, {"id": 68, "string": "We considered three types of leaf transformations: affine transformation, LSTM and bidirectional LSTM."}, {"id": 69, "string": "The resulting representations form the initial states of the Tree-LSTM."}, {"id": 70, "string": "In the absence of supervision, the tree is built in a bottomup fashion by recursively merging consecutive constituents (i, i + 1) based on merge-candidate scores."}, {"id": 71, "string": "On each level k of the bottom-up derivation, the merge-candidate score of the pair (i, i+1) is computed as follow: s k (i) = q, Tree-LSTM(r k i , r k i+1 ) , where q is a trainable query vector and r k i is the constituent representation at position i after k mergings."}, {"id": 72, "string": "We merge a pair (i * , i * + 1) sampled from the Categorical distribution built on the merge-candidate scores."}, {"id": 73, "string": "The representations of the constituents are then updated as follow: r k+1 i = \uf8f1 \uf8f4 \uf8f2 \uf8f4 \uf8f3 r k i , i < i * , Tree-LSTM(r k i , r k i+1 ) i = i * , r k i+1 i > i * ."}, {"id": 74, "string": "This procedure is repeated until one constituent remains."}, {"id": 75, "string": "Its hidden state is the input sentence representation."}, {"id": 76, "string": "This procedure is non-differentiable."}, {"id": 77, "string": "Choi et al."}, {"id": 78, "string": "(2018) use an approximation based on the Gumbel-Softmax distribution (Maddison et al., 2016; Jang et al., 2016) and the reparametrization trick (Kingma and Welling, 2013) ."}, {"id": 79, "string": "This relaxation makes the problem differentiable at the cost of a bias in the gradient estimates (Jang et al., 2016) ."}, {"id": 80, "string": "This difference between the real objective function and their approximation could explain why their method cannot recover simple context-free grammars (Nangia and Bowman, 2018) ."}, {"id": 81, "string": "We investigate this question by proposing an alternative optimisation scheme that directly aims for the correct objective function."}, {"id": 82, "string": "Our model We consider the problem defined in Eq."}, {"id": 83, "string": "(3) to jointly learn a composition function and an internal parser."}, {"id": 84, "string": "Our model is composed of the parser of Choi et al."}, {"id": 85, "string": "(2018) and the Tree-LSTM for the composition function."}, {"id": 86, "string": "As suggested in past work Schulman et al., 2017) , we added an entropy H over the tree distribution to the objective function: min \u03b8, \u03c6L (\u03b8, \u03c6) \u2212 \u03bb x H(t | x), (4) where \u03bb > 0."}, {"id": 87, "string": "This regulariser improves exploration by preventing early convergence to a suboptimal deterministic parsing strategy."}, {"id": 88, "string": "The new objective function is differentiable with respect to \u03b8, but not \u03c6, the parameters of the parser."}, {"id": 89, "string": "Learning \u03b8 follows the same procedure with BPTS as if the tree would be externally given."}, {"id": 90, "string": "In the rest of this section, we discuss the optimization of the parser and a cooperative training strategy to reduce the coadaptation issue."}, {"id": 91, "string": "Unbiased gradient estimation We cast the training of the parser as a reinforcement learning problem."}, {"id": 92, "string": "The parser is an agent whose reward function is the negative of the loss function defined in Eq."}, {"id": 93, "string": "(3)."}, {"id": 94, "string": "Its action space is the space of binary trees."}, {"id": 95, "string": "The agent's policy is a probability distribution over binary trees that decomposes as a sequence of K merging actions: p \u03c6 (t|x) = K k=0 \u03c0 \u03c6 (a i k |r k ), (5) where r k = (r k 0 , ."}, {"id": 96, "string": "."}, {"id": 97, "string": "."}, {"id": 98, "string": ", r k K\u2212k )."}, {"id": 99, "string": "The loss function is optimised with respect to \u03c6 with REIN-FORCE (Williams, 1992) ."}, {"id": 100, "string": "REINFORCE requires a considerable number of random samples to obtain a gradient estimate with a reasonable level of variance."}, {"id": 101, "string": "This number is positively correlated with the size of the search space, which is exponentially large in the case of binary trees."}, {"id": 102, "string": "We consider several extensions of REINFORCE to circumvent this problem."}, {"id": 103, "string": "Variance reduction."}, {"id": 104, "string": "An alternative solution to increasing the number of samples is the control variates method (Ross, 1997) ."}, {"id": 105, "string": "It takes advantage of random variables with known expected values and positive correlation with the quantity whose expectation is tried to be estimated."}, {"id": 106, "string": "Given an input-output pair (x, y) and tree t sampled from p \u03c6 (t|x) , let's define the random variable G as: G(t) = (f \u03b8 (x, t), y) \u2202log p \u03c6 (t|x) \u2202\u03c6 ."}, {"id": 107, "string": "(6) According to REINFORCE, calculating the gradient with respect to \u03c6 for the pair (x, y) is then equivalent to determining the unknown mean of the random variable G(t) 2 ."}, {"id": 108, "string": "Let's assume there is a control variate, i.e., a random variable b(t) that positively correlates with G and has known expected value with respect to p \u03c6 (.|x)."}, {"id": 109, "string": "Given N samples of the G(t) and the control variate b(t), the new gradient estimator is: G CV = E p \u03c6 (t|x) [b(t)] + 1 N N i=1 (G(t i ) \u2212 b(t i )) ."}, {"id": 110, "string": "A popular control variate, or baseline, used in REINFORCE is the moving average of recent rewards multiplied by the score function (Ross, 1997) : b(t) = c\u2207 \u03c6 log p \u03c6 (t|x)."}, {"id": 111, "string": "It has a zero mean under the p \u03c6 (.|x) distribution and it positively correlates with G(t)."}, {"id": 112, "string": "2 Note that while we are computing the gradients using , we could also directly optimise the parser with respect to downstream accuracy."}, {"id": 113, "string": "Surrogate loss."}, {"id": 114, "string": "REINFORCE often is implemented via a surrogate loss defined as follow: E t [r \u03c6 (t) (f \u03b8 (x, t), y)] , (7) where\u00ca t is the empirical average over a finite batch of samples and r \u03c6 (t) = p \u03c6 (t|x) p \u03c6 old (t|x) is the probability ratio with \u03c6 old standing for the parameters before the update."}, {"id": 115, "string": "Input-dependent baseline."}, {"id": 116, "string": "The moving average baseline cannot detect changes in rewards caused by structural differences in the inputs."}, {"id": 117, "string": "In our case, a long arithmetic expression is much harder to parse than a short one, systematically leading to their lower rewards."}, {"id": 118, "string": "This structural differences in the rewards aggravate the credit assignment problem by encouraging REINFORCE to discard actions sampled for longer sequences even though there might be some subsequences of actions that produce correct parsing subtrees."}, {"id": 119, "string": "A solution is to make the baseline inputdependent."}, {"id": 120, "string": "In particular, we use the self-critical training (SCT) baseline of Rennie et al."}, {"id": 121, "string": "(2017) , defined as: b(t, x) = c \u03b8,\u03c6 (x)\u2207 \u03c6 log p \u03c6 (t | x), where c \u03b8,\u03c6 is the reward obtained with the policy used at test time, i.e.,t = arg max p \u03c6 (t|x)."}, {"id": 122, "string": "This control variate has a zero mean under the p \u03c6 (t|x) distribution and correlates positively with the gradients."}, {"id": 123, "string": "Computing the arg max of a policy among all possible binary trees has exponential complexity."}, {"id": 124, "string": "We replace it with a simpler greedy decoding, i.e, a tree t is selected by following a sequence of greedy actions\u00e2 k : a k = arg max \u03c0 \u03c6 (a k |r k )."}, {"id": 125, "string": "This approximation is very efficient and computing the baseline requires only one additional forward pass."}, {"id": 126, "string": "Gradient normalization."}, {"id": 127, "string": "We empirically observe significant fluctuations in the gradient norms."}, {"id": 128, "string": "This creates instability that can not be reduced by additive terms, such as the inputdependent baselines."}, {"id": 129, "string": "A solution is to divide the gradients by a coarse approximation of their norm, e.g., a running estimate of the reward standard deviation (Mnih and Gregor, 2014) ."}, {"id": 130, "string": "This trick ensures that the rewards remain approximately in the unit ball, making the learning process less sensitive to steep changes in the loss."}, {"id": 131, "string": "Synchronizing syntax and semantics learning with PPO The gradients of the loss function from the Eq."}, {"id": 132, "string": "(4) are calculated using two different schemes, BPST for the composition function parameters \u03b8 and RE-INFORCE for the parser parameters \u03c6."}, {"id": 133, "string": "Then, both are updated with SGD."}, {"id": 134, "string": "The estimate of the gradient with respect to \u03c6 has higher variance compared to the estimate with respect to \u03b8."}, {"id": 135, "string": "Hence, using the same learning rate schedule does not necessarily correspond to the same real pace of learning."}, {"id": 136, "string": "It is \u03c6 parameters that are harder to optimise, so to improve training stability and convergence it is reasonable to aim for such updates that does not change the policy too much or too little."}, {"id": 137, "string": "A simple yet effective solution is the Proximal Policy Optimization (PPO) of Schulman et al."}, {"id": 138, "string": "(2017) ."}, {"id": 139, "string": "It considers the next surrogate loss: E t max r \u03c6 (t) (f \u03b8 (x, t), y) , r c \u03c6 (t) (f \u03b8 (x, t), y) , Where r c \u03c6 (t) = clip (r \u03c6 (t), 1 \u2212 , 1 + ) and is a real number in (0; 0.5]."}, {"id": 140, "string": "The first argument of the max is the surrogate loss for REINFORCE."}, {"id": 141, "string": "The clipped ratio in the second argument disincentivises the optimiser from performing updates resulting in large tree probability changes."}, {"id": 142, "string": "With this, the policy parameters can be optimised with repeated K steps of SGD to ensure a similar \"pace\" of learning between the parser and the compositional function."}, {"id": 143, "string": "Related work Besides the works mentioned in Sec."}, {"id": 144, "string": "2 and Sec."}, {"id": 145, "string": "3, there is a vast literature on learning latent parsers."}, {"id": 146, "string": "Early connectionist work in inferring context-free grammars proposed stack-augmented models and relied on explicit supervision on the strings that belonged to the target language and those that did not (Giles et al., 1989; Sun, 1990; Mozer and Das, 1992) ."}, {"id": 147, "string": "More recently, new stackaugmented models were shown to learn latent grammars from positive evidence alone (Joulin and Mikolov, 2015) ."}, {"id": 148, "string": "In parallel to these, other statistical approaches were proposed to automatically induce grammars from unparsed text (Sampson, 1986; Magerman and Marcus, 1990; Carroll and Charniak, 1992; Brill, 1993; Klein and Manning, 2002) ."}, {"id": 149, "string": "Our work departs from these approaches in that we aim at learning a latent grammar in the context of performing some given task."}, {"id": 150, "string": "Socher et al."}, {"id": 151, "string": "(2011) uses a surrogate autoencoder objective to search for a constituency structure, merging nodes greedily based on the reconstruction loss."}, {"id": 152, "string": "Maillard et al."}, {"id": 153, "string": "(2017) defines a relaxation of a CYK-like chart parser that is trained for a particular task."}, {"id": 154, "string": "A similar idea is introduced in Le and Zuidema (2015) where an automatic parser prunes the chart to reduce the overall complexity of the algorithm."}, {"id": 155, "string": "Another strategy, similar in nature, has been recently proposed by Corro and Titov (2018) , where Gumbel noise is used with differentiable dynamic programming to generate dependency trees."}, {"id": 156, "string": "In contrast, Yogatama et al."}, {"id": 157, "string": "(2016) learns a Shift-Reduce parser using reinforcement learning."}, {"id": 158, "string": "Maillard and Clark (2018) further proposes a beam search strategy to overcome learning trivial trees."}, {"id": 159, "string": "On a different vein, Vlad Niculae (2018) proposes a quadratic penalty term over the posterior distribution of nonprojective dependency trees to enforce sparsity of the relaxation."}, {"id": 160, "string": "Finally, there is a large body of work in Reinforcement Learning that aims at discovering how to combine elementary modules to solve complex tasks (Singh, 1992; Chang et al., 2018; Sahni et al., 2017) ."}, {"id": 161, "string": "Due to the limited space, we will not discuss them in further details."}, {"id": 162, "string": "Experiments We conducted experiments on three different tasks: evaluating mathematical expressions on the ListOps dataset (Nangia and Bowman, 2018) , sentiment analysis on the SST dataset (Socher et al., 2013) and natural language inference task on the SNLI (Bowman et al., 2015) and MultiNLI (Williams et al., 2018b) datasets."}, {"id": 163, "string": "Technical details."}, {"id": 164, "string": "For ListOps, we follow the experimental protocol of Nangia and Bowman (2018) , i.e., a 128 dimensional model and a tenway softmax classifier."}, {"id": 165, "string": "However, we replace their multi-layer perceptron (MLP) by a linear classifier."}, {"id": 166, "string": "The validation set is composed of 1k examples randomly selected from the training set."}, {"id": 167, "string": "For SST and NLI, we follow the setup of Choi et al."}, {"id": 168, "string": "(2018) : we initialise the word vectors with GloVe300D (Pennington et al., 2014) and train an MLP classifier on the sentence representations."}, {"id": 169, "string": "The hyperparameters are selected on the validation set using 5 random seeds for each configuration."}, {"id": 170, "string": "Our hyperparameters are the learning rate, weight decay, the regularisation parameter \u03bb, the leaf transformations, variance reduction hyperpa-  rameters and the number of updates K in PPO."}, {"id": 171, "string": "We use an adadelta optimizer (Zeiler, 2012)."}, {"id": 172, "string": "ListOps The ListOps dataset probes the syntax learning ability of latent tree models (Nangia and Bowman, 2018) ."}, {"id": 173, "string": "It is designed to have a single correct parsing strategy that a model must learn in order to succeed."}, {"id": 174, "string": "It is composed of prefix arithmetic expressions and the goal is to predict the numerical output associated with the evaluation of the expression."}, {"id": 175, "string": "The sequences are made of integers in [0, 9] Table 2 , the current leading latent tree models are unable to learn the correct parsing strategy on ListOps (Nangia and Bowman, 2018) ."}, {"id": 176, "string": "They even achieve performance worse than purely sequential recurrent networks."}, {"id": 177, "string": "On the other hand, our model achieves near perfect accuracy on this task, suggesting that our model is able to discover the correct parsing strategy."}, {"id": 178, "string": "Our model differs in several ways from the Gumbel Tree-LSTM of Choi et al."}, {"id": 179, "string": "(2018) that could explain this gap in performance."}, {"id": 180, "string": "In the rest of this section, we perform an ablation study on our model to understand the importance of each of these differences."}, {"id": 181, "string": "Impact of the baseline and PPO."}, {"id": 182, "string": "We report the impact of our design choices on the performance in Table 1 ."}, {"id": 183, "string": "Our model without baseline nor PPO is vanilla REINFORCE."}, {"id": 184, "string": "The baselines only improve performance when PPO is used."}, {"id": 185, "string": "Furthermore, these ablated models without PPO perform on-par with the RL-SPINN model (see Table 2 )."}, {"id": 186, "string": "This confirms our expectations for models that fail to synchronise syntax and semantics learning."}, {"id": 187, "string": "Interestingly, using PPO has a positive impact on both baselines, but accuracy remains low with the moving average baseline."}, {"id": 188, "string": "The reduction of variance induced by the SCT baseline leads to a near-perfect recovery of the good parsing strategy in all five experiments."}, {"id": 189, "string": "This shows the importance of this baseline for the stability of our approach."}, {"id": 190, "string": "Sensitivity to hyperparameters."}, {"id": 191, "string": "Our model is relatively robust to hyperparameters changes when we use the SCT baseline and PPO."}, {"id": 192, "string": "For example, changing the leaf transformation or dimensionality of the model has a minor impact on performance."}, {"id": 193, "string": "However, we have observed that the choice of the optimiser has a significant impact."}, {"id": 194, "string": "For example, the average performance drops to 73.0% if we replace Adadelta by Adam (Kingma and Ba, 2014 )."}, {"id": 195, "string": "Yet, the maximum value out of 5 runs remains relatively high, 99.0%."}, {"id": 196, "string": "Untied parameters."}, {"id": 197, "string": "As opposed to previous work, the parameters of the parser and the composition function are not tied in our model."}, {"id": 198, "string": "Without this separation between syntax and semantics, it would be impossible to update one module with- out changing the other."}, {"id": 199, "string": "The gradient direction is then dominated by the low variance signal from the semantic component, making it hard to learn the parser."}, {"id": 200, "string": "We confirmed experimentally that our model with tied parameters fails to find the correct parser and its accuracy drops to 64.7%."}, {"id": 201, "string": "Extrapolation and Grammaticality."}, {"id": 202, "string": "Recursive models have the potential to generalise to any sequence length."}, {"id": 203, "string": "Our model was trained with sequences of length up to 130 tokens."}, {"id": 204, "string": "We test the ability of the model to generalise to longer sequences by generating additional expressions of lengths 200 to 1000."}, {"id": 205, "string": "As shown in Fig.1 , our model has a little loss in accuracy as the length increases to ten times the maximum length seen during training."}, {"id": 206, "string": "On the other hand, we notice that final representations produced by the parser are very similar to each other."}, {"id": 207, "string": "Indeed, the cosine similarity between these vectors for the test set has a mean value of 0.998 with a standard deviation of 0.002."}, {"id": 208, "string": "There are two possible explanations for this observation: either our model assigns similar representations to valid expressions, or it produces a trivial uninformative representation regardless of the expression."}, {"id": 209, "string": "To verify which explanation is correct, we generate ungrammatical expressions by removing either one operation token or one closing bracket symbol for each sequence in the test set."}, {"id": 210, "string": "As shown in Figure 2 , in contrast to grammatical expressions, ungrammatical ones tend to be very different from each other: \"Happy families are all alike; every unhappy family is unhappy in its own way.\""}, {"id": 211, "string": "The only exception, marked by a mode near 1, come from ungrammatical expressions that represent incomplete expressions because of missing a closing bracket at the end."}, {"id": 212, "string": "This kind of sequences were seen by the parser during training and they indeed have to be represented by the same vector."}, {"id": 213, "string": "These observations show that our model does not produce a trivial representation, but identifies the rules and constraints of the grammar."}, {"id": 214, "string": "Moreover, vectors for grammatical sequences are so different from vectors for ungrammatical ones that you can tell them apart with 99.99% accuracy by simply measuring their cosine similarity to a randomly chosen grammatical vector from the training set."}, {"id": 215, "string": "Interestingly, we have not observed a similar signal from the vectors generated by the composition function."}, {"id": 216, "string": "Even learning a naive classifier between grammatical and ungrammatical expressions on top of these representations achieves an accuracy of only 75%."}, {"id": 217, "string": "This suggests that most of the syntactic information is captured by the parser, not the composition function."}, {"id": 218, "string": "Natural Language Inference We next evaluate our model on natural language inference using the Stanford Natural Language Inference (SNLI) (Bowman et al., 2015) and MultiNLI (Williams et al., 2018b) datasets."}, {"id": 219, "string": "Natural language inference consists in predicting the relationship between two sentences which can be either entailment, contradiction, or neutral."}, {"id": 220, "string": "The task can be formulated as a three-way classification problem."}, {"id": 221, "string": "The results are shown in Tables  3 and 4 ."}, {"id": 222, "string": "When training the model on MultiNLI dataset we augment the training data with the SNLI data and use matched versions of the de-   velopment and test sets."}, {"id": 223, "string": "Surprisingly, two out of four models for MultiNLI task collapsed to leftbranching parsing strategies."}, {"id": 224, "string": "This collapse can be explained by the absence of the entropy regularisation and the small number of PPO updates K = 1, which were determined to be optimal via hyperparameter optimisation."}, {"id": 225, "string": "As with ListOps, using an Adadelta optimizer significantly improves the training of the model."}, {"id": 226, "string": "Sentiment Analysis We evaluate our model on a sentiment classification task using the Stanford Sentiment Treebank (SST) of Socher et al."}, {"id": 227, "string": "(2013) ."}, {"id": 228, "string": "All sentences in SST are represented as binary parse trees, and each subtree of a parse tree is annotated with the corresponding sentiment score."}, {"id": 229, "string": "There are two versions of the dataset, with either binary labels, \"negative\" or \"positive\", (SST-2) or five labels, representing fine-grained sentiments (SST-5)."}, {"id": 230, "string": "As shown in Ta-  ble 5, our results are in line with previous work, confirming the benefits of using latent syntactic parse trees instead of the predefined syntax."}, {"id": 231, "string": "We noticed that all models trained on NLI or sentiment analysis tasks have parsing policies with relatively high entropy."}, {"id": 232, "string": "This indicates that the algorithm does not prefer any specific grammar."}, {"id": 233, "string": "Indeed, generated trees are very similar to balanced ones."}, {"id": 234, "string": "This result is in line with Shi et al."}, {"id": 235, "string": "(2018) where they observe that binary balanced tree encoder gets the best results on most classification tasks."}, {"id": 236, "string": "We also compare with state-of-the-art sequence-based models."}, {"id": 237, "string": "For the most part, these models are pre-trained on larger datasets and fine-tuned on these tasks."}, {"id": 238, "string": "Nonetheless, they outperform recursive models by a significant margin."}, {"id": 239, "string": "Performance on these datasets is more impacted by pre-training than by learning the syntax."}, {"id": 240, "string": "It would be interesting to see if a similar pre-training would also improve the performance of recursive models with latent tree learning."}, {"id": 241, "string": "Conclusion In this paper, we have introduced a novel model for learning latent tree parsers."}, {"id": 242, "string": "Our approach relies on a separation between syntax and semantics."}, {"id": 243, "string": "This allows dedicated optimisation schemes for each module."}, {"id": 244, "string": "In particular, we found that it is important to have an unbiased estimator of the parser gradients and to allow multiple gradient steps with PPO."}, {"id": 245, "string": "When tested on a CFG, our learned parser generalises to sequences of any length and distinguishes grammatical from ungrammatical expressions by forming meaningful representations for well-formed expressions."}, {"id": 246, "string": "For natural language tasks, instead, the model prefers to fall back to trivial strategies, in line with what was previously observed by Shi et al."}, {"id": 247, "string": "(2018) ."}, {"id": 248, "string": "Additionally, our approach performs competitively on several real natural language tasks."}, {"id": 249, "string": "In the future, we would like to explore further relaxation-based techniques for learning the parser, such as REBAR (Tucker et al., 2017) or ReLAX (Grathwohl et al., 2017) ."}, {"id": 250, "string": "Finally, we plan to look into applying recursive approaches to language modelling as a pre-training step and measure if it has the same impact on downstream tasks as sequential models."}], "headers": [{"section": "Introduction", "n": "1", "start": 0, "end": 24}, {"section": "Preliminaries", "n": "2", "start": 25, "end": 27}, {"section": "Recursive Neural Networks", "n": "2.1", "start": 28, "end": 40}, {"section": "Learning with RvNNs", "n": "2.2", "start": 41, "end": 46}, {"section": "Latent tree models", "n": "2.3", "start": 47, "end": 61}, {"section": "Gumbel Tree-LSTM", "n": "2.3.1", "start": 62, "end": 81}, {"section": "Our model", "n": "3", "start": 82, "end": 90}, {"section": "Unbiased gradient estimation", "n": "3.1", "start": 91, "end": 130}, {"section": "Synchronizing syntax and semantics learning with PPO", "n": "3.2", "start": 131, "end": 142}, {"section": "Related work", "n": "4", "start": 143, "end": 161}, {"section": "Experiments", "n": "5", "start": 162, "end": 171}, {"section": "ListOps", "n": "5.1", "start": 172, "end": 217}, {"section": "Natural Language Inference", "n": "5.2", "start": 218, "end": 225}, {"section": "Sentiment Analysis", "n": "5.3", "start": 226, "end": 240}, {"section": "Conclusion", "n": "6", "start": 241, "end": 250}], "figures": [{"filename": "../figure/image/1314-Table1-1.png", "caption": "Table 1: Accuracy on ListOps test set for our model with three different baselines, with and without PPO. We use K = 15 for PPO.", "page": 5, "bbox": {"x1": 79.67999999999999, "x2": 518.4, "y1": 68.64, "y2": 148.32}}, {"filename": "../figure/image/1314-Table2-1.png", "caption": "Table 2: Accuracy on the ListOps dataset. All models have 128 dimensions. Results for models with * are taken from Nangia and Bowman (2018).", "page": 5, "bbox": {"x1": 99.84, "x2": 262.08, "y1": 206.4, "y2": 292.32}}, {"filename": "../figure/image/1314-Table4-1.png", "caption": "Table 4: Results on MultiNLI. \u2020: results are taken from Williams et al. (2018a).", "page": 7, "bbox": {"x1": 90.72, "x2": 271.2, "y1": 337.44, "y2": 437.28}}, {"filename": "../figure/image/1314-Table3-1.png", "caption": "Table 3: Results on SNLI. *: publicly available code and hyperparameter optimization was used to obtain results. \u2020: results are taken from Williams et al. (2018a)", "page": 7, "bbox": {"x1": 77.75999999999999, "x2": 284.15999999999997, "y1": 62.4, "y2": 276.0}}, {"filename": "../figure/image/1314-Table5-1.png", "caption": "Table 5: Accuracy results of models on the SST. All the numbers are from Choi et al. (2018) but \u2217 where we used their publicly available code and performed hyperparameter optimization.", "page": 7, "bbox": {"x1": 306.71999999999997, "x2": 529.4399999999999, "y1": 63.36, "y2": 296.15999999999997}}, {"filename": "../figure/image/1314-Figure1-1.png", "caption": "Figure 1: Blue crosses depict an average accuracy of five models on the test examples that have lengths within certain range. Black circles illustrate individual models.", "page": 6, "bbox": {"x1": 72.0, "x2": 299.03999999999996, "y1": 61.44, "y2": 224.16}}, {"filename": "../figure/image/1314-Figure2-1.png", "caption": "Figure 2: The distributions of cosine similarity for elements from the different sets of mathematical expressions. A logarithmic scale is used for y-axis.", "page": 6, "bbox": {"x1": 306.71999999999997, "x2": 535.1999999999999, "y1": 61.44, "y2": 213.12}}]}