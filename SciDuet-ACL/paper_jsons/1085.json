{"title": "Toshiba MT System Description for the WAT2015 Workshop", "abstract": "This paper provides the system description of Toshiba Machine Translation System for the 2nd Workshop on Asian Translation (WAT2015). We participated in all tasks that consist of \"scientific papers subtask\" and \"patents subtask\". We submitted statistically post edited translation (SPE) results based on our rule based translation system and SMT for each language pair. In addition, we submitted system combination results between SPE and SMT with a recurrent neural language model (RNNLM). In experimental results, the system combination achieved higher BLEU scores than single system with reranking. We also obtained improvements in Chinese translation in crowdsourcing evaluations.", "text": [{"id": 0, "string": "Introduction Recently, statistical machine translation (SMT) has been broadly developed and successfully used in the portion of practicable systems."}, {"id": 1, "string": "However, it is costly to make a large volume of parallel corpora in a wide range of domains for commercial use."}, {"id": 2, "string": "For this reason, we have developed rule based machine translation (RBMT) system using a monolingual corpus in the target language."}, {"id": 3, "string": "For example, target word selection is possible based on co-occurrence relationship extracted from a monolingual corpus (Suzuki et al., 2005) ."}, {"id": 4, "string": "Furthermore, we have developed a word sense disambiguation based on a monolingual corpus in the target domain, and it has been applied to Japanese-Korean and Korean-Japanese translation systems (Kumano 2013, ."}, {"id": 5, "string": "On the other hand, open Asian parallel corpora including ASPEC 1 , NTCIR PatentMT 2 and JPO Patent Corpus 3 are available for the research of machine translation systems."}, {"id": 6, "string": "By using the parallel corpora, we have confirmed advantages which apply statistical post editing (SPE) to RBMT in domain adaptation (Suzuki, 2011) ."}, {"id": 7, "string": "In the last workshop (Nakazawa et al., 2014) , we participated in Japanese-English and Japanese-Chinese tasks with SPE approach and obtained higher evaluation results than RBMT."}, {"id": 8, "string": "Meanwhile, RBMT showed better performance than SPE in the direct and relative comparison ."}, {"id": 9, "string": "In this workshop (WAT2015), we participated in all tasks including Japanese-English (ja-en), English-Japanese (en-ja), Japanese-Chinese (ja-zh) and Chinese-Japanese (zh-ja) for \"scientific paper subtask\", and Chinese-Japanese (JPCzh-ja) and Korean-Japanese (JPCko-ja) for \"patents subtask\"."}, {"id": 10, "string": "Patents subtask is newly added, and its parallel corpus has 4 sections (Chemistry, Electricity, Mechanical Engineering and Physics)."}, {"id": 11, "string": "In all the tasks, we submitted SPE translation results based on our RBMT and SMT."}, {"id": 12, "string": "In addition, we submitted system combination results between SPE and SMT with recurrent neural language model (RNNLM; Mikolov el al., 2010) ."}, {"id": 13, "string": "Section 2 and 3 describe the overview of our systems and some pre/post processing."}, {"id": 14, "string": "The experimental results and official results are shown in Section 4 and 5."}, {"id": 15, "string": "The analysis for the official results is discussed in Section 6 and finally, Section 7 concludes this paper."}, {"id": 16, "string": "As for a contextaware translation, the description was omitted because our baseline system is the same as the last workshop (see ."}, {"id": 17, "string": "Overview of Toshiba System RBMT System Our RBMT system is basically a transfer-based machine translation (Izuha et al., 2008) ."}, {"id": 18, "string": "The core framework consists of morphological analysis, syntactic/semantic analysis, target word selection, structural transfer, syntactic generation and morphological generation."}, {"id": 19, "string": "Furthermore, huge amount of rules as translation knowledge including word dictionaries can realize both high translation performance and flexibility of customization."}, {"id": 20, "string": "As for Japanese-Korean translation, syntactic analysis and transfer are omitted because the languages are grammatically similar."}, {"id": 21, "string": "Statistical Post Editing SPE using phrase-based SMT has been proposed and it is an efficient framework which is able to adapt translation output to target domains (Michel et al., 2007) ."}, {"id": 22, "string": "We first translated source sentences of training data in ASPEC and JPO Patent Corpus by RBMT."}, {"id": 23, "string": "Then we trained phrase-based model between translated sentences and reference sentences using Moses toolkit (Kohen et al., 2007) ."}, {"id": 24, "string": "In the training, we used 1M sentences for ja-en, en-ja, JPCzh-ja and JPCko-ja, 0.67M for ja-zh and zh-ja in the training data."}, {"id": 25, "string": "Japanese sentences were tokenized by JUMAN 4 , and Moses tokenizer for English, and Kytea (Neubig et."}, {"id": 26, "string": "al, 2011) for Chinese."}, {"id": 27, "string": "We also trained 5-gram language models using KenLM (Heafield et al., 2013) ."}, {"id": 28, "string": "In tuning and decoding, we set distortion limit to 0 for JPOko-ja in consideration of grammatical similarity and 6 for other language pairs."}, {"id": 29, "string": "System Combination using RNNLM Although both SPE and SMT are based on a statistical model from the given corpora, they generate different translation candidates because SPE has some features from RBMT."}, {"id": 30, "string": "If a better system can be selected from the candidates in each translation, we can get a better translation result."}, {"id": 31, "string": "Thus, we realized a system combination between SPE and SMT as n-best reranking using a RNNLM."}, {"id": 32, "string": "The n-best reranking can be achieved using both basic features and RNNLM score."}, {"id": 33, "string": "In tuning, we combined 100-best candidates of both SPE and SMT for dev-set, and ran MERT tuning by adding the RNNLM score to the basic features."}, {"id": 34, "string": "In decoding, we re-ranked combined candidates by product-sum of the features including RNNLM score and tuned weights."}, {"id": 35, "string": "For ja-en, en-ja, ja-zh and zh-ja, we used RNNLMs trained by the first 500k sentences in the training data of ASPEC."}, {"id": 36, "string": "For JPOzh-ja and JPOko-ja, we used 500k sentences which were evenly extracted from 4 sections in JPO Patent Corpus."}, {"id": 37, "string": "All RNNLMs were trained with 500 hidden layers and 50 classes by RNNLM toolkit 5 ."}, {"id": 38, "string": "Tuning RBMT and pre/postprocessing Technical Term Dictionaries As the preparation for each task, we selected technical term dictionaries by the same principle in the last workshop (Sonoh el al., 2014) ."}, {"id": 39, "string": "For JPOzh-ja, we used an additional patent dictionary, which is extracted from JPO Chinese-Japanese dictionary 6 ."}, {"id": 40, "string": "Furthermore, for JPOko-ja, we used n-gram probability dictionary, which was made from monolingual patent resources, in order to resolve word sense disambiguation ."}, {"id": 41, "string": "English Word Correction To improve translation of sentences including misspelled words in English, we applied correction processing based on an edited distance."}, {"id": 42, "string": "We replaced the word considered as misspelling with a word which had the smallest edited distance in the training data."}, {"id": 43, "string": "However, because SMT and SPE basically have robustness to the misspelling, we confined words to be replaced to words which remain as unknown words in SMT and SPE results."}, {"id": 44, "string": "Japanese KATAKANA Normalization In the case where a target language is Japanese; we applied normalization of KATAKANA notation."}, {"id": 45, "string": "In advance of translation, we counted the frequency of KATAKANA notation, which has fluctuations of prolonged sound mark, in the target sentences of the training data."}, {"id": 46, "string": "In the translation results, KATAKANA fluctuations were replaced with those of highly-frequent notations, such as \"from \u30b9\u30af\u30ea\u30e5 to \u30b9\u30af\u30ea\u30e5\u30fc\" and \"from \u30b5\u30fc\u30d0\u30fc to \u30b5\u30fc\u30d0\"."}, {"id": 47, "string": "By applying normalization, we got improvements of about 0.5 BLEU in RBMT."}, {"id": 48, "string": "Furthermore, we replaced the ideographic comma \"\u3001\" in number expression with a normal comma \",\" for translation results in Japanese."}, {"id": 49, "string": "Other Post Processing In order to reduce unknown words in SMT, we applied RBMT to SMT results."}, {"id": 50, "string": "For example, in ja-zh, we translated KATAKANA words, which remain in SMT results, into Chinese or English words, if the words were found in RBMT dictionaries."}, {"id": 51, "string": "Also, Hangul words in SMT results of JPOko-ja were translated into Japanese words."}, {"id": 52, "string": "Experimental Results This section shows experimental results of our translation systems."}, {"id": 53, "string": "Table 1 and 2 show the overall BLEU and RIBES scores for \"scientific papers subtask\" and \"patents subtask\", respectively."}, {"id": 54, "string": "COMB means results of the system combination and Rerank means results of reranking using RNNLM (100best for SMT and SPE, 200-best for COMB)."}, {"id": 55, "string": "In all tasks, SPE improves translation results of RBMT on the BLEU and RIBES."}, {"id": 56, "string": "In tasks except JPOko-ja, SPE achieves performance equal to or better than phrase-based SMT."}, {"id": 57, "string": "Moreover, in most tasks, Rerank improves about 0.3-0.5 BLEU score, and COMB shows better performance than other systems."}, {"id": 58, "string": "In JPOko-ja, SMT, SPE and COMB show very high performances which are close to 70 BLEU, and SMT with reranking achieves the highest BLEU and RIBES scores."}, {"id": 59, "string": "In ja-en, en-ja, ja-zh and zh-ja, more than half of translations selected from SPE and the others selected from SMT."}, {"id": 60, "string": "In particular SPE accounted for about 80% translations in ja-en, en-ja and zh-ja."}, {"id": 61, "string": "On the other hand, more than half of translations selected from SMT in JPOzh-ja and JPOko-ja."}, {"id": 62, "string": "Table 3 shows the translation examples that COMB achieves better results than SPE with reranking in sentence-level BLEU."}, {"id": 63, "string": "Finally, we compared between phrase-based model and hierarchical phrase-based model."}, {"id": 64, "string": "Table 4 shows comparison in ja-zh task."}, {"id": 65, "string": "In all systems including SPE, hierarchical phrase-based model improves about 0.4 BLEU."}, {"id": 66, "string": "We applied hierarchical phrase-based model to ja-zh only, because significant improvements were not confirmed in other language pairs."}, {"id": 67, "string": ".716 31.82 0.770 29.60 0.810 37 .47 0.827 Official Results This section shows official results of our translation systems."}, {"id": 68, "string": "We basically submitted two results, one is SPE 7 and the other is the system combination between SPE and SMT."}, {"id": 69, "string": "Furthermore, top two systems on the BLEU scores were evaluated by the crowdsourcing."}, {"id": 70, "string": "In the crowdsourcing evaluation, pair-wise evaluation against the baseline system (phrase-based SMT) was performed by 5 evaluators, and HUMAN score was calculated 7 In JPOko-ja, because SMT showed higher BLEU score than SPE, we submitted SMT result."}, {"id": 71, "string": "(Nakazawa et al., 2014) ."}, {"id": 72, "string": "In WAT2015 results (Nakazawa et al., 2015) , we note that Toshiba systems were ranked as one of the top three systems in human evaluation in ja-en, ja-zh and JPOzh-ja."}, {"id": 73, "string": "Especially, ja-zh achieved the highest score although the BLEU score is lower than other systems."}, {"id": 74, "string": "On the other hand, as for JPOko-ja, we got a comparatively high BLEU score, but were disappointed by its low HUMAN score."}, {"id": 75, "string": "Table 5 and 6 are the overall official results for each task, respectively."}, {"id": 76, "string": "In ja-zh and zh-ja, COMB shows higher HUMAN score than SPE."}, {"id": 77, "string": "On the other hand, SPE or SMT is higher than COMB in ja-en, JPOzh-ja and JPOko-ja."}, {"id": 78, "string": "These results indicate that the system combination improves human evaluation of Chinese translation in the scientific documents, at least."}, {"id": 79, "string": "We guess that the system combination between equivalent systems achieves complementary translation to improve human evaluations."}, {"id": 80, "string": "For example, BLEU scores of SPE and SMT are nearly equal in ja-zh and zh-ja (shown in Table 1 )."}, {"id": 81, "string": "Discussion On receiving the crowdsourcing results, we analyzed differences between our system and Online A, which obtained the highest HUMAN score in JPOko-ja."}, {"id": 82, "string": "Table 7 shows the comparison between our system (COMB) and Online A."}, {"id": 83, "string": "Here, 'Baseline' column is the HUMAN score in the result of crowdsourcing (official results) and the other was evaluated by inner evaluators."}, {"id": 84, "string": "The inner evaluation was conducted excluding expressional differences as described in detail below."}, {"id": 85, "string": "Although Online A achieves a very high HUMAN score to the baseline system, superior results of COMB over Online A are shown in the pair-wise evaluation."}, {"id": 86, "string": "We hypothesize that the significant difference between the crowdsourcing and the inner evaluators occurs from the evaluation of the number expressions, such as \"\u30b7\u30b9\u30c6\u30e0(100)\" and \"\u30b7\u30b9\u30c6\u30e0 100\"."}, {"id": 87, "string": "In the training data of JPOko-ja, a lot of brackets of numbers in the source sentences disappear in the target sentences."}, {"id": 88, "string": "Thus, brackets are dropped in SPE and SMT."}, {"id": 89, "string": "As for well-translated target sentences such as JPOko-ja, it is possible that evaluators in the crowdsourcing judged faithful translation as better by focusing on existence of brackets."}, {"id": 90, "string": "Conclusion The overview of Toshiba machine translation systems, which applied the statistical post editing and the system combination with RNNLM, is described in this paper."}, {"id": 91, "string": "SPE and reranking with RNNLM achieved higher BLEU than phrase-based SMT in most language pairs."}, {"id": 92, "string": "Furthermore, the system combination between SPE and SMT improved BLEU score in Japanese-English pair and Japanese-Chinese pair."}, {"id": 93, "string": "In the other hand, a straightforward correlation between automatic evaluation  and human evaluation is not confirmed in our system."}, {"id": 94, "string": "We need to establish the combination of multi-systems for practical use purpose, taking advantage of their characteristics and qualities."}], "headers": [{"section": "Introduction", "n": "1", "start": 0, "end": 16}, {"section": "RBMT System", "n": "2.1", "start": 17, "end": 20}, {"section": "Statistical Post Editing", "n": "2.2", "start": 21, "end": 28}, {"section": "System Combination using RNNLM", "n": "2.3", "start": 29, "end": 36}, {"section": "Tuning", "n": "3", "start": 37, "end": 37}, {"section": "Technical Term Dictionaries", "n": "3.1", "start": 38, "end": 40}, {"section": "English Word Correction", "n": "3.2", "start": 41, "end": 43}, {"section": "Japanese KATAKANA Normalization", "n": "3.3", "start": 44, "end": 48}, {"section": "Other Post Processing", "n": "3.4", "start": 49, "end": 51}, {"section": "Experimental Results", "n": "4", "start": 52, "end": 66}, {"section": "Official Results", "n": "5", "start": 67, "end": 80}, {"section": "Discussion", "n": "6", "start": 81, "end": 89}, {"section": "Conclusion", "n": "7", "start": 90, "end": 94}], "figures": [{"filename": "../figure/image/1085-Table1-1.png", "caption": "Table 1: Overall BLEU and RIBES scores for \u201cscientific papers subtask\u201d.", "page": 2, "bbox": {"x1": 94.56, "x2": 500.15999999999997, "y1": 95.52, "y2": 215.04}}, {"filename": "../figure/image/1085-Table2-1.png", "caption": "Table 2: Overall BLEU and RIBES scores for \u201cpatents subtask\u201d.", "page": 2, "bbox": {"x1": 174.72, "x2": 420.47999999999996, "y1": 238.56, "y2": 358.08}}, {"filename": "../figure/image/1085-Table6-1.png", "caption": "Table 6: Overall official results for \u201cpatents subtask\u201d.", "page": 4, "bbox": {"x1": 174.72, "x2": 421.44, "y1": 191.04, "y2": 268.32}}, {"filename": "../figure/image/1085-Table7-1.png", "caption": "Table 7: The relationship between automatic evaluations and human evaluations.", "page": 4, "bbox": {"x1": 303.84, "x2": 530.4, "y1": 509.76, "y2": 618.72}}, {"filename": "../figure/image/1085-Table5-1.png", "caption": "Table 5: Overall official results for \u201cscientific papers subtask\u201d. B, R and H mean BLEU, RIBES, HUMAN, respectively. HUMAN was evaluated by 5 evaluators using crowdsorcing.", "page": 4, "bbox": {"x1": 67.67999999999999, "x2": 527.04, "y1": 108.0, "y2": 169.44}}, {"filename": "../figure/image/1085-Table4-1.png", "caption": "Table 4: A Comparison of Phrase-based Model.", "page": 3, "bbox": {"x1": 64.8, "x2": 293.28, "y1": 575.04, "y2": 694.0799999999999}}, {"filename": "../figure/image/1085-Table3-1.png", "caption": "Table 3: Translation examples indicating that COMB achieves better results than SPE in sentece-level BLEU.", "page": 3, "bbox": {"x1": 67.67999999999999, "x2": 527.04, "y1": 106.56, "y2": 548.16}}]}