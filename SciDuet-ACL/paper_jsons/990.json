{"title": "Practical Semantic Parsing for Spoken Language Understanding", "abstract": "Executable semantic parsing is the task of converting natural language utterances into logical forms that can be directly used as queries to get a response. We build a transfer learning framework for executable semantic parsing. We show that the framework is effective for Question Answering (Q&A) as well as for Spoken Language Understanding (SLU). We further investigate the case where a parser on a new domain can be learned by exploiting data on other domains, either via multitask learning between the target domain and an auxiliary domain or via pre-training on the auxiliary domain and fine-tuning on the target domain. With either flavor of transfer learning, we are able to improve performance on most domains; we experiment with public data sets such as Overnight and NLmaps as well as with commercial SLU data. The experiments carried out on data sets that are different in nature show how executable semantic parsing can unify different areas of NLP such as Q&A and SLU.", "text": [{"id": 0, "string": "Introduction Due to recent advances in speech recognition and language understanding, conversational interfaces such as Alexa, Cortana, and Siri are becoming more common."}, {"id": 1, "string": "They currently have two large uses cases."}, {"id": 2, "string": "First, a user can use them to complete a specific task, such as playing music."}, {"id": 3, "string": "Second, a user can use them to ask questions where the questions are answered by querying knowledge graph or database back-end."}, {"id": 4, "string": "Typically, under a common interface, there exist two disparate systems that can handle each use cases."}, {"id": 5, "string": "The system underlying the first use case is known as a spoken language understanding (SLU) system."}, {"id": 6, "string": "Typical commercial SLU systems rely on predicting a coarse user intent and then tagging each word in the utterance to * Work conducted while interning at Amazon Alexa AI."}, {"id": 7, "string": "the intent's slots."}, {"id": 8, "string": "This architecture is popular due to its simplicity and robustness."}, {"id": 9, "string": "On the other hand, Q&A, which need systems to produce more complex structures such as trees and graphs, requires a more comprehensive understanding of human language."}, {"id": 10, "string": "One possible system that can handle such a task is an executable semantic parser (Liang, 2013; Kate et al., 2005) ."}, {"id": 11, "string": "Given a user utterance, an executable semantic parser can generate tree or graph structures that represent logical forms that can be used to query a knowledge base or database."}, {"id": 12, "string": "In this work, we propose executable semantic parsing as a common framework for both uses cases by framing SLU as executable semantic parsing that unifies the two use cases."}, {"id": 13, "string": "For Q&A, the input utterances are parsed into logical forms that represent the machine-readable representation of the question, while in SLU, they represent the machine-readable representation of the user intent and slots."}, {"id": 14, "string": "One added advantage of using parsing for SLU is the ability to handle more complex linguistic phenomena such as coordinated intents that traditional SLU systems struggle to handle (Agarwal et al., 2018) ."}, {"id": 15, "string": "Our parsing model is an extension of the neural transition-based parser of Cheng et al."}, {"id": 16, "string": "(2017) ."}, {"id": 17, "string": "A major issue with semantic parsing is the availability of the annotated logical forms to train the parsers, which are expensive to obtain."}, {"id": 18, "string": "A solution is to rely more on distant supervisions such as by using question-answer pairs (Clarke et al., 2010; ."}, {"id": 19, "string": "Alternatively, it is possible to exploit annotated logical forms from a different domain or related data set."}, {"id": 20, "string": "In this paper, we focus on the scenario where data sets for several domains exist but only very little data for a new one is available and apply transfer learning techniques to it."}, {"id": 21, "string": "A common way to implement transfer learning is by first pre-training the model on a domain on which a large data set is available and subsequently fine-tuning the model on the target domain (Thrun, 1996; Zoph et al., 2016) ."}, {"id": 22, "string": "We also consider a multi-task learning (MTL) approach."}, {"id": 23, "string": "MTL refers to machine learning models that improve generalization by training on more than one task."}, {"id": 24, "string": "MTL has been used for a number of NLP problems such as tagging (Collobert and Weston, 2008) , syntactic parsing (Luong et al., 2015) , machine translation Luong et al., 2015) and semantic parsing (Fan et al., 2017) ."}, {"id": 25, "string": "See Caruana (1997) and Ruder (2017) for an overview of MTL."}, {"id": 26, "string": "A good Q&A data set for our domain adaptation scenario is the Overnight data set (Wang et al., 2015b) , which contains sentences annotated with Lambda Dependency-Based Compositional Semantics (Lambda DCS; Liang 2013) for eight different domains."}, {"id": 27, "string": "However, it includes only a few hundred sentences for each domain, and its vocabularies are relatively small."}, {"id": 28, "string": "We also experiment with a larger semantic parsing data set (NLmaps; Lawrence and Riezler 2016) ."}, {"id": 29, "string": "For SLU, we work with data from a commercial conversational assistant that has a much larger vocabulary size."}, {"id": 30, "string": "One common issue in parsing is how to deal with rare or unknown words, which is usually addressed by either delexicalization or by implementing a copy mechanism (Gulcehre et al., 2016) ."}, {"id": 31, "string": "We show clear differences in the outcome of these and other techniques when applied to data sets of varying sizes."}, {"id": 32, "string": "Our contributions are as follows: \u2022 We propose a common semantic parsing framework for Q&A and SLU and demonstrate its broad applicability and effectiveness."}, {"id": 33, "string": "\u2022 We report parsing baselines for Overnight for which exact match parsing scores have not been yet published."}, {"id": 34, "string": "\u2022 We show that SLU greatly benefits from a copy mechanism, which is also beneficial for NLmaps but not Overnight."}, {"id": 35, "string": "\u2022 We investigate the use of transfer learning and show that it can facilitate parsing on lowresource domains."}, {"id": 36, "string": "Transition-based Parser Transition-based parsers are widely used for dependency parsing (Nivre, 2008; Dyer et al., 2015) and they have been also applied to semantic parsing tasks (Wang et al., 2015a; Cheng et al., 2017) ."}, {"id": 37, "string": "In syntactic parsing, a transition system is usually defined as a quadruple: T = {S, A, I, E}, where S is a set of states, A is a set of actions, I is the initial state, and E is a set of end states."}, {"id": 38, "string": "A state is composed of a buffer, a stack, and a set of arcs: S = (\u03b2, \u03c3, A)."}, {"id": 39, "string": "In the initial state, the buffer contains all the words in the input sentence while the stack and the set of subtrees are empty: S 0 = (w 0 | ."}, {"id": 40, "string": "."}, {"id": 41, "string": "."}, {"id": 42, "string": "|w N , \u2205, \u2205)."}, {"id": 43, "string": "Terminal states have empty stack and buffer: S T = (\u2205, \u2205, A)."}, {"id": 44, "string": "During parsing, the stack stores words that have been removed from the buffer but have not been fully processed yet."}, {"id": 45, "string": "Actions can be performed to advance the transition system's state: they can either consume words in the buffer and move them to the stack (SHIFT) or combine words in the stack to create new arcs (LEFT-ARC and RIGHT-ARC, depending on the direction of the arc) 1 ."}, {"id": 46, "string": "Words in the buffer are processed left-toright until an end state is reached, at which point the set of arcs will contain the full output tree."}, {"id": 47, "string": "The parser needs to be able to predict the next action based on its current state."}, {"id": 48, "string": "Traditionally, supervised techniques are used to learn such classifiers, using a parallel corpus of sentences and their output trees."}, {"id": 49, "string": "Trees can be converted to states and actions using an oracle system."}, {"id": 50, "string": "For a detailed explanation of transition-based parsing, see Nivre (2003) and Nivre (2008) ."}, {"id": 51, "string": "Neural Transition-based Parser with Stack-LSTMs In this paper, we consider the neural executable semantic parser of Cheng et al."}, {"id": 52, "string": "(2017) , which follows the transition-based parsing paradigm."}, {"id": 53, "string": "Its transition system differs from traditional systems as the words are not consumed from the buffer because in executable semantic parsing, there are no strict alignments between words in the input and nodes in the tree."}, {"id": 54, "string": "The neural architecture encodes the buffer using a Bi-LSTM (Graves, 2012) and the stack as a Stack-LSTM (Dyer et al., 2015) , a recurrent network that allows for push and pop operations."}, {"id": 55, "string": "Additionally, the previous actions are also represented with an LSTM."}, {"id": 56, "string": "The output of these networks is fed into feed-forward layers and softmax layers are used to predict the next action given the current state."}, {"id": 57, "string": "The possible actions are REDUCE, which pops an item from the stack, TER, which creates a terminal node (i.e., a leaf in the tree), and NT, which creates a non-terminal node."}, {"id": 58, "string": "When the next action is either TER or NT, additional softmax layers predict the output token to be generated."}, {"id": 59, "string": "Since the buffer does not change while parsing, an attention mechanism is used to focus on specific words given the current state of the parser."}, {"id": 60, "string": "We extend the model of Cheng et al."}, {"id": 61, "string": "(2017) by adding character-level embeddings and a copy mechanism."}, {"id": 62, "string": "When using only word embeddings, out-of-vocabulary words are usually mapped to one embedding vector and do not exploit morphological features."}, {"id": 63, "string": "Our model encodes words by feeding each character embedding onto an LSTM and concatenate its output to the word embedding: x = {e w ; h M c }, (1) where e w is the word embedding of the input word w and h M c is the last hidden state of the characterlevel LSTM over the characters of the input word w = c 0 , ."}, {"id": 64, "string": "."}, {"id": 65, "string": "."}, {"id": 66, "string": ", c M ."}, {"id": 67, "string": "Rare words are usually handled by either delexicalizing the output or by using a copy mechanism."}, {"id": 68, "string": "Delexicalization involves substituting named entities with a specific token in an effort to reduce the number of rare and unknown words."}, {"id": 69, "string": "Copy relies on the fact that when rare or unknown words must be generated, they usually appear in the same form in the input sentence and they can be therefore copied from the input itself."}, {"id": 70, "string": "Our copy implementation follows the strategy of Fan et al."}, {"id": 71, "string": "(2017) , where the output of the generation layer is concatenated to the scores of an attention mechanism (Bahdanau et al., 2015) , which express the relevance of each input word with respect to the current state of the parser."}, {"id": 72, "string": "In the experiments that follow, we compare delexicalization with copy mechanism on different setups."}, {"id": 73, "string": "A depiction of the full model is shown in Figure 1 ."}, {"id": 74, "string": "Transfer learning We consider the scenario where large training corpora are available for some domains and we want to bootstrap a parser for a new domain where little training data is available."}, {"id": 75, "string": "We investigate the use of two transfer learning approaches: pre-training and multi-task learning."}, {"id": 76, "string": "Figure 1 : The full neural transition-based parsing model."}, {"id": 77, "string": "x 0 , x 1 , ."}, {"id": 78, "string": "."}, {"id": 79, "string": "."}, {"id": 80, "string": ", x n HISTORY ."}, {"id": 81, "string": "."}, {"id": 82, "string": "."}, {"id": 83, "string": "BUFFER ."}, {"id": 84, "string": "."}, {"id": 85, "string": "."}, {"id": 86, "string": "STACK ."}, {"id": 87, "string": "."}, {"id": 88, "string": "."}, {"id": 89, "string": "ATTENTION FEED-FORWARD LAYERS TER RED NT t 0 ."}, {"id": 90, "string": "."}, {"id": 91, "string": "."}, {"id": 92, "string": "t n x 0 ."}, {"id": 93, "string": "."}, {"id": 94, "string": "."}, {"id": 95, "string": "x n TER COPY nt 0 ."}, {"id": 96, "string": "."}, {"id": 97, "string": "."}, {"id": 98, "string": "nt n NT Representations of stack, buffer, and previous actions are used to predict the next action."}, {"id": 99, "string": "When the TER or NT actions are chosen, further layers are used to predict (or copy) the token."}, {"id": 100, "string": "For MTL, the different tasks share most of the architecture and only the output layers, which are responsible for predicting the output tokens, are separate for each task."}, {"id": 101, "string": "When multi-tasking across domains of the same data set, we expect that most layers of the neural parser, such as the ones responsible for learning the word embeddings and the stack and buffer representation, will learn similar features and can, therefore, be shared."}, {"id": 102, "string": "We implement two different MTL setups: a) when separate heads are used for both the TER classifier and the NT classifier, which is expected to be effective when transferring across tasks that do not share output vocabulary; and b) when a separate head is used only for the TER classifier, more appropriate when the non-terminals space is mostly shared."}, {"id": 103, "string": "Data In order to investigate the flexibility of the executable semantic parsing framework, we evaluate models on Q&A data sets as well as on commercial SLU data sets."}, {"id": 104, "string": "For Q&A, we consider Overnight (Wang et al., 2015b) and NLmaps (Lawrence and Riezler, 2016) ."}, {"id": 105, "string": "Overnight It contains sentences annotated with Lambda DCS (Liang, 2013) ."}, {"id": 106, "string": "The sentences are divided into eight domains: calendar, blocks, housing, restaurants, publications, recipes, socialnetwork, and basketball."}, {"id": 107, "string": "As shown in Table 1 , the number of sentences and the terminal vocabularies are small, which makes the learning more challenging, preventing us from using data-hungry approaches such as sequence-to-sequence models."}, {"id": 108, "string": "The current state-of-the-art results, to the best of our knowledge, are reported by Su and Yan (2017) ."}, {"id": 109, "string": "Previous work on this data set use denotation accuracy as a metric."}, {"id": 110, "string": "In this paper, we use logical form exact match accuracy across all data sets."}, {"id": 111, "string": "NLmaps It contains more than two thousand questions about geographical facts, retrieved from OpenStreetMap (Haklay and Weber, 2008) ."}, {"id": 112, "string": "Unfortunately, this data set is not divided into subdomains."}, {"id": 113, "string": "While NLmaps has comparable sizes with some of the Overnight domains, its vocabularies are much larger: containing 160 terminals, 24 non-terminals and 280 word types (Table 1) ."}, {"id": 114, "string": "The current state-of-the-art results on this data set are reported by Duong et al."}, {"id": 115, "string": "(2017) ."}, {"id": 116, "string": "SLU We select five domains from our SLU data set: search, recipes, cinema, bookings, and closet."}, {"id": 117, "string": "In order to investigate the use case of a new lowresource domain exploiting a higher-resource domain, we selected a mix of high-resource and lowresource domains."}, {"id": 118, "string": "Details are shown in Table 1 ."}, {"id": 119, "string": "We extracted shallow trees from data originally collected for intent/slot tagging: intents become the root of the tree, slot types are attached to the roots as their children and slot values are in turn attached to their slot types as their children."}, {"id": 120, "string": "An example is shown in Figure 2 ."}, {"id": 121, "string": "A similar approach to transform intent/slot data into tree structures has been recently employed by Gupta et al."}, {"id": 122, "string": "(2018b) ."}, {"id": 123, "string": "Experiments We first run experiments on single-task semantic parsing to observe the differences among the three different data sources discussed in Section 4."}, {"id": 124, "string": "Specifically, we explore the impact of an attention mechanism on the performance as well as the comparison between delexicalization and a copy mechanism for dealing with data sparsity."}, {"id": 125, "string": "The metric used to evaluate parsers is the exact match accuracy, defined as the ratio of sentences cor-  rectly parsed."}, {"id": 126, "string": "Attention Because the buffer is not consumed as in traditional transition-based parsers, Cheng et al."}, {"id": 127, "string": "(2017) use an additive attention mechanism (Bahdanau et al., 2015) to focus on the more relevant words in the buffer for the current state of the stack."}, {"id": 128, "string": "In order to find the impact of attention on the different data sets, we run ablation experiments, as shown in Table 2 (left side)."}, {"id": 129, "string": "We found that attention between stack and buffer is not always beneficial: it appears to be helpful for larger data sets while harmful for smaller data sets."}, {"id": 130, "string": "Attention is, however, useful for NLmaps, regardless of the  data size."}, {"id": 131, "string": "Even though NLmaps data is similarly sized to some of the Overnight domains, its terminal space is considerably larger, perhaps making attention more important even with a smaller data set."}, {"id": 132, "string": "On the other hand, the high-resource SLU's cinema domain is not able to benefit from the attention mechanism."}, {"id": 133, "string": "We note that the performance of this model on NLmaps falls behind the state of the art (Duong et al., 2017) ."}, {"id": 134, "string": "The hyper-parameters of our model were however not tuned on this data set."}, {"id": 135, "string": "Handling Sparsity A popular way to deal with the data sparsity problem is to delexicalize the data, that is replacing rare and unknown words with coarse categories."}, {"id": 136, "string": "In our experiment, we use a named entity recognition system 2 to replace names with their named entity types."}, {"id": 137, "string": "Alternatively, it is possible to use a copy mechanism to enable the decoder to copy rare words from the input rather than generating them from its limited vocabulary."}, {"id": 138, "string": "We compare the two solutions across all data sets on the right side of Table 2 ."}, {"id": 139, "string": "Regardless of the data set, the copy mechanism generally outperforms delexicalization."}, {"id": 140, "string": "We also note that delexi-2 https://spacy.io calization has unexpected catastrophic effects on exact match accuracy for calendar and housing."}, {"id": 141, "string": "For Overnight, however, the system with copy mechanism is outperformed by the system without attention."}, {"id": 142, "string": "This is unsurprising as the copy mechanism is based on attention, which is not effective on Overnight (Section 5.1)."}, {"id": 143, "string": "The inefficacy of copy mechanisms on the Overnight data set was also discussed in Jia and Liang (2016) , where answer accuracy, rather than parsing accuracy, was used as a metric."}, {"id": 144, "string": "As such, the results are not directly comparable."}, {"id": 145, "string": "For NLmaps and all SLU domains, using a copy mechanism results in an average accuracy improvement of 16% over the baseline."}, {"id": 146, "string": "It is worth noting that the copy mechanism is unsurprisingly effective for SLU data due to the nature of the data set: the SLU trees were obtained from data collected for slot tagging, and as such, each leaf in the tree has to be copied from the input sentence."}, {"id": 147, "string": "Even though Overnight often yields different conclusions, most likely due to its small vocabulary size, the similar behaviors observed for NLmaps and SLU is reassuring, confirming that it is possible to unify Q&A and SLU under the same umbrella framework of executable semantic parsing."}, {"id": 148, "string": "In order to compare the NLmaps results with Lawrence and Riezler (2016) , we also compute F1 scores for the data set."}, {"id": 149, "string": "Our baseline outperforms previous results, achieving a score of 0.846."}, {"id": 150, "string": "Our best F1 results are also obtained when adding the copy mechanism, achieving a score of 0.874."}, {"id": 151, "string": "Transfer Learning The first set of experiments involve transfer learning across Overnight domains."}, {"id": 152, "string": "For this data set, the non-terminal vocabulary is mostly shared across domains."}, {"id": 153, "string": "As such, we use the architecture where only the TER output classifier is not shared."}, {"id": 154, "string": "Selecting the best auxiliary domain by maximizing the overlap with the main domain was not successful, and we instead performed an exhaustive search over the domain pairs on the development set."}, {"id": 155, "string": "In the interest of space, for each main domain, we report results for the best auxiliary domain (Table 3)."}, {"id": 156, "string": "We note that MTL and pre-training provide similar results and provide an average improvement of 4%."}, {"id": 157, "string": "As expected, we observe more substantial improvements for smaller domains."}, {"id": 158, "string": "We performed the same set of experiments on   the SLU domains, as shown in Table 4 ."}, {"id": 159, "string": "In this case, the non-terminal vocabulary can vary significantly across domains."}, {"id": 160, "string": "We therefore choose to use the MTL architecture where both TER and NT output classifiers are not shared."}, {"id": 161, "string": "Also for SLU, there is no clear winner between pre-training and MTL."}, {"id": 162, "string": "Nevertheless, they always outperform the baseline, demonstrating the importance of transfer learning, especially for smaller domains."}, {"id": 163, "string": "While the focus of this transfer learning framework is in exploiting high-resource domains annotated in the same way as a new low-resource domain, we also report a preliminary experiment on transfer learning across tasks."}, {"id": 164, "string": "We selected the recipes domain, which exists in both Overnight and SLU."}, {"id": 165, "string": "While the SLU data set is significantly different from Overnight, deriving from a corpus annotated with intent/slot labels, as discussed in Section 4, we found promising results using pre-training, increasing the accuracy from 58.3 to 61.1."}, {"id": 166, "string": "A full investigation of transfer learning across domains belonging to heterogeneous data sets is left for future work."}, {"id": 167, "string": "The experiments on transfer learning demon- Related work A large collection of logical forms of different nature exist in the semantic parsing literature: semantic role schemes (Palmer et al., 2005; Meyers et al., 2004; Baker et al., 1998) , syntax/semantics interfaces (Steedman, 1996) , executable logical forms (Liang, 2013; Kate et al., 2005) , and general purpose meaning representations (Banarescu et al., 2013; Abend and Rappoport, 2013 Cheng et al."}, {"id": 168, "string": "(2017) , which is inspired by Recurrent Neural Network Grammars (Dyer et al., 2016) ."}, {"id": 169, "string": "We extend the model with ideas inspired by Gulcehre et al."}, {"id": 170, "string": "(2016) and Luong and Manning (2016) ."}, {"id": 171, "string": "We build our multi-task learning architecture upon the rich literature on the topic."}, {"id": 172, "string": "MTL was first introduce in Caruana (1997) ."}, {"id": 173, "string": "It has been since used for a number of NLP problems such as tagging (Collobert and Weston, 2008) , syntactic parsing (Luong et al., 2015) , and machine translation Luong et al., 2015) ."}, {"id": 174, "string": "The closest to our work is Fan et al."}, {"id": 175, "string": "(2017) , where MTL architectures are built on top of an attentive sequenceto-sequence model (Bahdanau et al., 2015) ."}, {"id": 176, "string": "We instead focus on transfer learning across domains of the same data sets and employ a different architecture which promises to be less data-hungry than sequence-to-sequence models."}, {"id": 177, "string": "Typical SLU systems rely on domain-specific semantic parsers that identify intents and slots in a sentence."}, {"id": 178, "string": "Traditionally, these tasks were performed by linear machine learning models (Sha and Pereira, 2003) but more recently jointlytrained DNN models are used (Mesnil et al., 2015; Hakkani-T\u00fcr et al., 2016) with differing contexts (Gupta et al., 2018a; Vishal Ishwar Naik, 2018) ."}, {"id": 179, "string": "More recently there has been work on extending the traditional intent/slot framework using targeted parsing to handle more complex linguistic phenomenon like coordination (Gupta et al., 2018c; Agarwal et al., 2018) ."}, {"id": 180, "string": "Conclusions We framed SLU as an executable semantic parsing task, which addresses a limitation of current commercial SLU systems."}, {"id": 181, "string": "By applying our framework to different data sets, we demonstrate that the framework is effective for Q&A as well as for SLU."}, {"id": 182, "string": "We explored a typical scenario where it is necessary to learn a semantic parser for a new domain with little data, but other high-resource domains are available."}, {"id": 183, "string": "We show the effectiveness of our system and both pre-training and MTL on different domains and data sets."}, {"id": 184, "string": "Preliminary experiment results on transfer learning across domains belonging to heterogeneous data sets suggest future work in this area."}], "headers": [{"section": "Introduction", "n": "1", "start": 0, "end": 35}, {"section": "Transition-based Parser", "n": "2", "start": 36, "end": 50}, {"section": "Neural Transition-based Parser with", "n": "2.1", "start": 51, "end": 73}, {"section": "Transfer learning", "n": "3", "start": 74, "end": 102}, {"section": "Data", "n": "4", "start": 103, "end": 122}, {"section": "Experiments", "n": "5", "start": 123, "end": 125}, {"section": "Attention", "n": "5.1", "start": 126, "end": 134}, {"section": "Handling Sparsity", "n": "5.2", "start": 135, "end": 150}, {"section": "Transfer Learning", "n": "5.3", "start": 151, "end": 166}, {"section": "Related work", "n": "6", "start": 167, "end": 179}, {"section": "Conclusions", "n": "7", "start": 180, "end": 184}], "figures": [{"filename": "../figure/image/990-Figure1-1.png", "caption": "Figure 1: The full neural transition-based parsing model. Representations of stack, buffer, and previous actions are used to predict the next action. When the TER or NT actions are chosen, further layers are used to predict (or copy) the token.", "page": 2, "bbox": {"x1": 309.59999999999997, "x2": 515.04, "y1": 66.24, "y2": 317.76}}, {"filename": "../figure/image/990-Table4-1.png", "caption": "Table 4: Transfer learning results for SLU domains. BL + Copy is the model without transfer learning. PRETR. stands for pre-training. Again, the numbers are exact match accuracy.", "page": 5, "bbox": {"x1": 80.64, "x2": 284.15999999999997, "y1": 263.52, "y2": 340.32}}, {"filename": "../figure/image/990-Table3-1.png", "caption": "Table 3: Transfer learning results for the Overnight domains. BL \u2212 Att is the model without transfer learning. PRETR. stands for pre-training. Again, we report exact match accuracy.", "page": 5, "bbox": {"x1": 93.6, "x2": 271.2, "y1": 62.4, "y2": 190.07999999999998}}, {"filename": "../figure/image/990-Table2-1.png", "caption": "Table 2: Left side: Ablation experiments on attention mechanism. Right side: Comparison between delexicalization and copy mechanism. BL is the model of Section 2.1, \u2212Att refers to the same model without attention, +Delex is the system with delexicalization and in +Copy we use a copy mechanism instead. The scores indicate the percentage of correct parses.", "page": 4, "bbox": {"x1": 75.84, "x2": 289.44, "y1": 62.4, "y2": 280.32}}, {"filename": "../figure/image/990-Table1-1.png", "caption": "Table 1: Details of training data. # is the number of sentences, TER is the terminal vocabulary size, NT is the nonterminal vocabulary size and Words is the input vocabulary size.", "page": 3, "bbox": {"x1": 317.76, "x2": 517.4399999999999, "y1": 242.39999999999998, "y2": 486.24}}, {"filename": "../figure/image/990-Figure2-1.png", "caption": "Figure 2: Conversion from intent/slot tags to tree for the sentence Which cinemas screen Star Wars tonight?", "page": 3, "bbox": {"x1": 336.47999999999996, "x2": 506.4, "y1": 89.75999999999999, "y2": 197.28}}]}