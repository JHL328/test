{"title": "Self-disclosure topic model for classifying and analyzing Twitter conversations", "abstract": "Self-disclosure, the act of revealing oneself to others, is an important social behavior that strengthens interpersonal relationships and increases social support. Although there are many social science studies of self-disclosure, they are based on manual coding of small datasets and questionnaires. We conduct a computational analysis of self-disclosure with a large dataset of naturally-occurring conversations, a semi-supervised machine learning algorithm, and a computational analysis of the effects of self-disclosure on subsequent conversations. We use a longitudinal dataset of 17 million tweets, all of which occurred in conversations that consist of five or more tweets directly replying to the previous tweet, and from dyads with twenty of more conversations each. We develop self-disclosure topic model (SDTM), a variant of latent Dirichlet allocation (LDA) for automatically classifying the level of self-disclosure for each tweet. We take the results of SDTM and analyze the effects of self-disclosure on subsequent conversations. Our model significantly outperforms several comparable methods on classifying the level of selfdisclosure, and the analysis of the longitudinal data using SDTM uncovers significant and positive correlation between selfdisclosure and conversation frequency and length.", "text": [{"id": 0, "string": "Introduction Self-disclosure is an important and pervasive social behavior."}, {"id": 1, "string": "People disclose personal information about themselves to improve and maintain * This work was done when JinYeong Bak was a visiting student at Microsoft Research, Beijing, China."}, {"id": 2, "string": "relationships (Jourard, 1971; Joinson and Paine, 2007) ."}, {"id": 3, "string": "A common instance of self-disclosure is the start of a conversation with an exchange of names and additional self-introductions."}, {"id": 4, "string": "Another example of self-disclosure, shown in Figure 1c , where the information disclosed about a family member's serious illness, is much more personal than the exchange of names."}, {"id": 5, "string": "In this paper, we seek to understand this important social behavior using a large-scale Twitter conversation data, automatically classifying the level of self-disclosure using machine learning and correlating the patterns with conversational behaviors which can serve as proxies for measuring intimacy between two conversational partners."}, {"id": 6, "string": "Twitter conversation data, explained in more detail in section 4.1, enable an extremely large scale study of naturally-occurring self-disclosure behavior, compared to traditional social science studies."}, {"id": 7, "string": "One challenge of such large scale study, though, remains in the lack of labeled groundtruth data of self-disclosure level."}, {"id": 8, "string": "That is, naturally-occurring Twitter conversations do not come tagged with the level of self-disclosure in each conversation."}, {"id": 9, "string": "To overcome that challenge, we propose a semi-supervised machine learning approach using probabilistic topic modeling."}, {"id": 10, "string": "Our self-disclosure topic model (SDTM) assumes that self-disclosure behavior can be modeled using a combination of simple linguistic features (e.g., pronouns) with automatically discovered semantic themes (i.e., topics)."}, {"id": 11, "string": "For instance, an utterance \"I am finally through with this disastrous relationship\" uses a first-person pronoun and contains a topic about personal relationships."}, {"id": 12, "string": "In comparison with various other models, SDTM shows the highest accuracy, and the resulting conversation frequency and length patterns on self-disclosure are shown different over time."}, {"id": 13, "string": "Our contributions to the research community include the following: \u2022 We present key features and prior knowledge for identifying self-disclosure level, and show relevance of it with experiment results (Sec."}, {"id": 14, "string": "2)."}, {"id": 15, "string": "\u2022 We present a topic model that explicitly includes the level of self-disclosure in a conversation using linguistic features and the latent semantic topics (Sec."}, {"id": 16, "string": "3)."}, {"id": 17, "string": "\u2022 We collect a large dataset of Twitter conversations over three years and annotate a small subset with self-disclosure level (Sec."}, {"id": 18, "string": "4)."}, {"id": 19, "string": "\u2022 We compare the classification accuracy of SDTM with other models and show that it performs the best (Sec."}, {"id": 20, "string": "5)."}, {"id": 21, "string": "\u2022 We correlate the self-disclosure patterns and conversation behaviors to show that there is significant relationship over time (Sec."}, {"id": 22, "string": "6)."}, {"id": 23, "string": "Self-Disclosure In this section, we look at social science literature for definition of the levels of self-disclosure."}, {"id": 24, "string": "Using that definition, we devise an approach to automatically identify the levels of self-disclosure in a large corpus of OSN conversations."}, {"id": 25, "string": "We discuss three approaches, first, using first-person pronoun features, and second, extracting seed words and phrases from the Twitter conversation corpus, and third, extracting seed words and phrases from an external corpus of anonymously posted secrets, and we demonstrate the efficacy of those approaches with an annotated corpus."}, {"id": 26, "string": "Self-disclosure (SD) level To analyze self-disclosure, researchers categorize self-disclosure language into three levels: G (general) for no disclosure, M for medium disclosure, and H for high disclosure (Vondracek and Von dracek, 1971; Barak and Gluck-Ofri, 2007 G Level of Self-Disclosure An obvious clue of self-disclosure is the use of first-person pronouns."}, {"id": 27, "string": "For example, phrases such as 'I live' or 'My name is' indicate that the utterance contains personal information."}, {"id": 28, "string": "In previous research, the simple method of counting first-person pronouns was used to measure the degree of self-disclosure (Joinson, 2001; Barak and Gluck-Ofri, 2007) ."}, {"id": 29, "string": "Consequently, the absence of a first-person pronoun signals that the utterance belongs in the G level of self-disclosure."}, {"id": 30, "string": "We verify this pattern with a dataset of Tweets annotated with G, M, and H levels."}, {"id": 31, "string": "We divide the annotated Tweets into two classes, G and M/H."}, {"id": 32, "string": "Then we compute mutual information of each unigram, bigram, or trigram feature to see which features are most discriminative."}, {"id": 33, "string": "As Table 1 shows, 18 out of 30 M Level of Self-Disclosure Utterances with M level include two types: 1) information related with past events and future plans, and 2) general information about self (Barak and Gluck-Ofri, 2007) ."}, {"id": 34, "string": "For the former, we add as seed trigrams 'I have been' and 'I will'."}, {"id": 35, "string": "For the latter, we use seven types of information generally accepted to be personally identifiable information (McCallister, 2010) , as listed in the left column of Table 2 ."}, {"id": 36, "string": "To find the appropriate trigrams for those, we take Twitter conversation data (described in Section 4.1) and look for trigrams that begin with 'I' and 'my' and occur more than 200 times."}, {"id": 37, "string": "We then check each one to see whether it is related with any of the seven types listed in the table."}, {"id": 38, "string": "As a result, we find 57 seed trigrams for M level."}, {"id": 39, "string": "H Level of Self-Disclosure Utterances with H level express secretive wishes or sensitive information that exposes self or someone close (Barak and Gluck-Ofri, 2007) ."}, {"id": 40, "string": "These are generally kept as secrets."}, {"id": 41, "string": "With this intuition, we crawled 26,523 posts from Six Billion Secrets 1 site where users post secrets anonymously 2 ."}, {"id": 42, "string": "We  call this external dataset SECRET."}, {"id": 43, "string": "Unlike G and M levels, evidence of H level of self-disclosure tends to be topical, such as physical appearance, mental and physical illnesses, and family problems, so we take an approach of fitting a topic model driven by seed words."}, {"id": 44, "string": "A similar approach has been successful in sentiment classification (Jo and Oh, 2011; Kim et al., 2013) ."}, {"id": 45, "string": "A critical component of this approach is the set of seed words with which to drive the discovery of topics that are most indicative of H level selfdisclosure."}, {"id": 46, "string": "To extract the seed words that express secretive personal information, we compute mutual information (Manning et al., 2008) with SE-CRET and 24,610 randomly selected tweets."}, {"id": 47, "string": "We select 1,000 words with high mutual information and filter out stop words."}, {"id": 48, "string": "Table 3 shows some of these words."}, {"id": 49, "string": "To extract seed trigrams of secretive wishes, we again look for trigrams that start with 'I' or 'my', occur more than 200 times, and select trigrams of wishful thinking, such as 'I want to', and 'I wish I'."}, {"id": 50, "string": "In total, there are 88 seed words and 8 seed trigrams for H. Since SECRET is quite different from Twitter, we must show that posts in SECRET are semantically similar to the H level Tweets."}, {"id": 51, "string": "Rather than directly comparing SECRET posts and Tweets, we use the same method of extracting discriminative word features from the annotated H level Tweets (see Section 4.2)."}, {"id": 52, "string": "Table 3 shows the seed words extracted from SECRET as well as the annotated Tweets."}, {"id": 53, "string": "Because the annotated dataset consists of only 200 conversations, the coverage of the topics seems narrower than the much larger SECRETS, but both datasets show similarities in the topics."}, {"id": 54, "string": "This, combined with the results of the model with the two sets of seed words (see Section 5 for the results), shows that SECRETS is an effective and simple-to-obtain substitute for an annotated corpus of H level of self-disclosure."}, {"id": 55, "string": "This section describes our model, the selfdisclosure topic model (SDTM), for classifying self-disclosure level and discovering topics for each self-disclosure level."}, {"id": 56, "string": "SD level of tweet ct \u03c0c SD level proportion of conversation c \u03b8 G c ; \u03b8 M c ; \u03b8 H c Topic proportion of {G; M; H} in con- versation c \u03c6 G ; \u03c6 M ; \u03c6 H Word distribution of {G; M; H} \u03b1; \u03b3 Dirichlet prior for \u03b8; \u03c0 \u03b2 G , \u03b2 M ; \u03b2 H Dirichlet prior for \u03c6 G ; \u03c6 M ; \u03c6 H n cl Model In section 2, we discussed different approaches to identifying each level of self-disclosure, based on social science literature, annotated and unannotated Tweets, and an external corpus of secret posts."}, {"id": 57, "string": "In this section, we describe our self-disclosure topic model, based on the widely used latent Dirichlet allocation (Blei et al., 2003) , which incorporates those approaches."}, {"id": 58, "string": "Figure 2 illustrates the graphical model of 1."}, {"id": 59, "string": "For each level l \u2208 {G, M, H}: For each topic k \u2208 {1, ."}, {"id": 60, "string": "."}, {"id": 61, "string": "."}, {"id": 62, "string": ", K l }: Draw \u03c6 l k \u223c Dir(\u03b2 l ) 2."}, {"id": 63, "string": "For each conversation c \u2208 {1, ."}, {"id": 64, "string": "."}, {"id": 65, "string": "."}, {"id": 66, "string": ", C}: (a) Draw \u03b8 G c \u223c Dir(\u03b1) (b) Draw \u03b8 M c \u223c Dir(\u03b1) (c) Draw \u03b8 H c \u223c Dir(\u03b1) (d) Draw \u03c0 c \u223c Dir(\u03b3) (e) For each message t \u2208 {1, ."}, {"id": 67, "string": "."}, {"id": 68, "string": "."}, {"id": 69, "string": ", T }: i."}, {"id": 70, "string": "Observe first-person pronouns features x ct ii."}, {"id": 71, "string": "Draw \u03c9 ct \u223c M axEnt(x ct , \u03bb) iii."}, {"id": 72, "string": "Draw y ct \u223c Bernoulli(\u03c9 ct ) iv."}, {"id": 73, "string": "If y ct = 0 which is G level: A."}, {"id": 74, "string": "Draw z ct \u223c M ult(\u03b8 G c ) B."}, {"id": 75, "string": "For each word n \u2208 {1, ."}, {"id": 76, "string": "."}, {"id": 77, "string": "."}, {"id": 78, "string": ", N }: Draw word w ctn \u223c M ult(\u03c6 G zct ) Else which can be M or H level: A."}, {"id": 79, "string": "Draw r ct \u223c M ult(\u03c0 c ) B."}, {"id": 80, "string": "Draw z ct \u223c M ult(\u03b8 rct c ) C. For each word n \u2208 {1, ."}, {"id": 81, "string": "."}, {"id": 82, "string": "."}, {"id": 83, "string": ", N }: Draw word w ctn \u223c M ult(\u03c6 rct zct ) Figure 3: Generative process of SDTM."}, {"id": 84, "string": "SDTM and how those approaches are embodied in it."}, {"id": 85, "string": "The first approach based on the first-person pronouns is implemented by the observed variable x ct and the parameters \u03bb from a maximum entropy classifier for G vs. M/H level."}, {"id": 86, "string": "The approach of seed words and phrases for levels M and H is implemented by the three separate word-topic probability vectors for the three levels of SD: \u03c6 l which has a Bayesian informative prior \u03b2 l where l \u2208 {G, M, H}, the three levels of self-disclosure."}, {"id": 87, "string": "Table 4 lists the notations used in the model and the generative process, and Figure 3 describes the generative process."}, {"id": 88, "string": "Classifying G vs M/H levels Classifying the SD level for each tweet is done in two parts, and the first part classifies G vs. M/H levels with first-person pronouns (I, my, me)."}, {"id": 89, "string": "In the graphical model, y is the latent variable that represents this classification, and \u03c9 is the distribution over y. x is the observation of the firstperson pronoun in the tweets, and \u03bb are the parameters learned from the maximum entropy classifier."}, {"id": 90, "string": "With the annotated Twitter conversation dataset (described in Section 4.2), we experimented with several classifiers (Decision tree, Naive Bayes) and chose the maximum entropy classifier because it performed the best, similar to other joint topic models (Zhao et al., 2010; Mukherjee et al., 2013) ."}, {"id": 91, "string": "Classifying M vs H levels The second part of the classification, the M and the H level, is driven by informative priors with seed words and seed trigrams."}, {"id": 92, "string": "In the graphical model, r is the latent variable that represents this classification, and \u03c0 is the distribution over r. \u03b3 is a non-informative prior for \u03c0, and \u03b2 l is an informative prior for each SD level by seed words."}, {"id": 93, "string": "For example, we assign a high value for the seed word 'acne' for \u03b2 H , and a low value for 'My name is'."}, {"id": 94, "string": "This approach is the same as joint models of topic and sentiment (Jo and Oh, 2011; Kim et al., 2013) ."}, {"id": 95, "string": "Inference For posterior inference of SDTM, we use collapsed Gibbs sampling which integrates out latent random variables \u03c9, \u03c0, \u03b8, and \u03c6."}, {"id": 96, "string": "Then we only need to compute y, r and z for each tweet."}, {"id": 97, "string": "We compute full conditional distribution p(y ct = j , r ct = l , z ct = k |y \u2212ct , r \u2212ct , z \u2212ct , w, x) for tweet ct as follows: p(y ct = 0, z ct = k |y \u2212ct , r \u2212ct , z \u2212ct , w, x) \u221d exp(\u03bb 0 \u00b7 x ct ) 1 j=0 exp(\u03bb j \u00b7 x ct ) g(c, t, l , k ), p(y ct = 1, r ct = l , z ct = k |y \u2212ct , r \u2212ct , z \u2212ct , w, x) \u221d exp(\u03bb 1 \u00b7 x ct ) 1 j=0 exp(\u03bb j \u00b7 x ct ) (\u03b3 l + n (\u2212ct) cl ) g(c, t, l , k ), where z \u2212ct , r \u2212ct , y \u2212ct are z, r, y without tweet ct, m ctk (\u00b7) is the marginalized sum over word v of m ctk v and the function g(c, t, l , k ) as follows: g(c, t, l , k ) = \u0393( V v=1 \u03b2 l v + n l \u2212(ct) k v ) \u0393( V v=1 \u03b2 l v + n l \u2212(ct) k v + m ctk (\u00b7) ) \u03b1 k + n l (\u2212ct) ck K k=1 \u03b1 k + n l ck V v=1 \u0393(\u03b2 l v + n l \u2212(ct) k v + m ctk v ) \u0393(\u03b2 l v + n l \u2212(ct) k v ) ."}, {"id": 98, "string": "Data Collection and Annotation To test our self-disclosure topic model, we use a large dataset of conversations consisting of Tweets over three years such that we can analyze the relationship between self-disclosure behavior and conversation frequency and length over time."}, {"id": 99, "string": "We chose to crawl Twitter because it offers a practical and large source of conversations (Ritter et al., 2010) ."}, {"id": 100, "string": "Others have also analyzed Twitter conversations for natural language and social media Conv's  Tweets  101,686 61,451 1,956,993 17,178,638   Table 5 : Dataset of Twitter conversations."}, {"id": 101, "string": "We chose conversations consisting of five or more tweets each."}, {"id": 102, "string": "We chose dyads with twenty or more conversations."}, {"id": 103, "string": "Users Dyads research (boyd et al., 2010; Danescu-Niculescu-Mizil et al., 2011) , but we collect conversations from the same set of dyads over several months for a unique longitudinal dataset."}, {"id": 104, "string": "We also make sure that each conversation is at least five tweets, and that each dyad has at least twenty conversations."}, {"id": 105, "string": "Collecting Twitter conversations We define a Twitter conversation as a chain of tweets where two users are consecutively replying to each other's tweets using the Twitter reply button."}, {"id": 106, "string": "We initialize the set of users by randomly sampling thirteen users who reply to other users in English from the Twitter public streams 3 ."}, {"id": 107, "string": "Then we crawl each user's public tweets, and look at users who are mentioned in those tweets."}, {"id": 108, "string": "It is a breadth-first search in the network defined by users as nodes and edges as conversations."}, {"id": 109, "string": "We run this search for dyads until the depth of four, and filter out users who tweet in a non-English language."}, {"id": 110, "string": "We use an open source tool for detecting English tweets 4 ."}, {"id": 111, "string": "To protect users' privacy, we replace Twitter userid, usernames and url in tweets with random strings."}, {"id": 112, "string": "This dataset consists of 101,686 users, 61,451 dyads, 1,956,993 conversations and 17,178,638 tweets which were posted between August 2007 to July 2013."}, {"id": 113, "string": "Table 5 summarizes the dataset."}, {"id": 114, "string": "Annotating self-disclosure level To measure the accuracy of our model, we randomly sample 301 conversations, each with ten or fewer tweets, and ask three judges, fluent in English and graduate students/researchers, to annotate each tweet with the level of self-disclosure."}, {"id": 115, "string": "Judges first read and discussed the definitions and examples of self-disclosure level shown in (Barak and Gluck-Ofri, 2007) , then they worked separately on a Web-based platform."}, {"id": 116, "string": "As a result of annotation, there are 122 G level converstaions, 147 M level and 32 H level con- versations, and inter-rater agreement using Fleiss kappa (Fleiss, 1971 ) is 0.68, which is substantial agreement result (Landis and Koch, 1977) ."}, {"id": 117, "string": "Classification of Self-Disclosure Level This section describes experiments and results of SDTM as well as several other methods for classification of self-disclosure level."}, {"id": 118, "string": "We first start with the annotated dataset in section 4.2 in which each tweet is annotated with SD level."}, {"id": 119, "string": "We then aggregate all of the tweets of a conversation, and we compute the proportions of tweets in each SD level."}, {"id": 120, "string": "When the proportion of tweets at M or H level is equal to or greater than 0.2, we take the level of the larger proportion and assign that level to the conversation."}, {"id": 121, "string": "When the proportions of tweets at M or H level are both less than 0.2, we assign G to the SD level."}, {"id": 122, "string": "The reason for setting 0.2 as the threshold is that a conversation containing tweets with H or M level of selfdisclosure usually starts with a greeting or a general comment, and contains one or more questions or comments before or after the self-disclosure tweet."}, {"id": 123, "string": "We compare SDTM with the following methods for classifying conversations for SD level: \u2022 LDA (Blei et al., 2003) : A Bayesian topic model."}, {"id": 124, "string": "Each conversation is treated as a document."}, {"id": 125, "string": "Used in previous work (Bak et al., 2012) ."}, {"id": 126, "string": "\u2022 MedLDA (Zhu et al., 2012) : A supervised topic model for document classification."}, {"id": 127, "string": "Each conversation is treated as a document and response variable can be mapped to a SD level."}, {"id": 128, "string": "\u2022 LIWC (Tausczik and Pennebaker, 2010): Word counts of particular categories 5 ."}, {"id": 129, "string": "Used in previous work (Houghton and Joinson, 2012)."}, {"id": 130, "string": "\u2022 Bag of Words + Bigrams + Trigrams (BOW+): A bag of words, bigram and trigram features."}, {"id": 131, "string": "We exclude features that appear only once or twice."}, {"id": 132, "string": "\u2022 Seed words and trigrams (SEED): Occurrences of seed words/trigrams from SECRET which are described in section 3.3."}, {"id": 133, "string": "\u2022 SDTM with seed words from annotated Tweets (SDTM\u2212): To compare with SDTM below using seed words from SECRET, this uses seed words from the annotated data described in section 2.4."}, {"id": 134, "string": "\u2022 ASUM (Jo and Oh, 2011 ): A joint model of sentiments and topics."}, {"id": 135, "string": "We map each SD level to one sentiment and use the same seed words/trigrams from SECRET as in SDTM below."}, {"id": 136, "string": "Used in previous work (Bak et al., 2012) ."}, {"id": 137, "string": "\u2022 First-person pronouns (FirstP): Occurrence of first-person pronouns which are described in section 3.2."}, {"id": 138, "string": "To identify first-person pronouns, we tagged parts of speech in each tweet with the Twitter POS tagger (Owoputi et al., 2013) ."}, {"id": 139, "string": "\u2022 First-person pronouns + Seed words/trigrams (FP+SE1): First-person pronouns and seed words/trigrams from SECRET."}, {"id": 140, "string": "\u2022 Two stage classifier with First-person pronouns + Seed words/trigrams (FP+SE2): A Method Acc G F 1 M F 1 H F Table 6 : SD level classification accuracies and Fmeasures using annotated data."}, {"id": 141, "string": "Acc is accuracy, and G F 1 is F-measure for classifying the G level."}, {"id": 142, "string": "Avg F 1 is the macroaveraged value of G F 1 , M F 1 and H F 1 ."}, {"id": 143, "string": "SDTM outperforms all other methods compared."}, {"id": 144, "string": "The difference between SDTM and FirstP is statistically significant (p-value < 0.05 for accuracy, < 0.0001 for Avg F 1 )."}, {"id": 145, "string": "two stage classifier with first-person pronouns and seed words/trigrams from SE-CRET."}, {"id": 146, "string": "In the first stage, the classifier identifies G with first-person pronouns."}, {"id": 147, "string": "Then in the second stage, the classifier uses seed words and trigrams to identify M and H levels."}, {"id": 148, "string": "\u2022 SDTM: Our model with first-person pronouns and seed words/trigrams from SE-CRET."}, {"id": 149, "string": "SEED, LIWC, LDA and FirstP cannot be used directly for classification, so we use Maximum entropy model with outputs of each of those models as features 6 ."}, {"id": 150, "string": "BOW+ uses SVM with a radial basis kernel which performs better than all other settings tried including maximum entropy."}, {"id": 151, "string": "We split the data randomly into 80/20 for train/test."}, {"id": 152, "string": "We run MedLDA, ASUM and SDTM 20 times each and compute the average accuracies and F-measure for each level."}, {"id": 153, "string": "We run LDA and MedLDA with various number of topics from 80 to 140, and 120 topics shows best outputs."}, {"id": 154, "string": "So we set 120 topics for LDA, MedLDA and ASUM, 60; 40; 40 topics for SDTM K G , K M and K H respectively which is best perform from 40; 40; 40 to 60; 60; 60 topics."}, {"id": 155, "string": "We assume that a conversation has few topics and self-disclosure levels, so we set \u03b1 = \u03b3 = 0.1 (Tang et al., 2014) ."}, {"id": 156, "string": "To incorporate the seed words and trigrams into ASUM and SDTM, we initialize \u03b2 G , \u03b2 M and \u03b2 H differently."}, {"id": 157, "string": "We assign a high value of 2.0 for each seed word and trigram for that level, and a low value of 10 \u22126 for each word that is a seed word for another level, and a default value of 0.01 for all other words."}, {"id": 158, "string": "This approach is the same as previous papers (Jo and Oh, 2011; Kim et al., 2013) ."}, {"id": 159, "string": "As Table 6 shows, SDTM performs better than the other methods for accuracy as well as Fmeasure."}, {"id": 160, "string": "LDA and MedLDA generally show the lowest performance, which is not surprising given these models are quite general and not tuned specifically for this type of semi-supervised classification task."}, {"id": 161, "string": "BOW which is simple word features also does not perform well, showing especially low F-measure for the H level."}, {"id": 162, "string": "LIWC and SEED perform better than LDA, but these have quite low F-measure for G and H levels."}, {"id": 163, "string": "ASUM shows better performance for classifying H level than others, confirming the effectiveness of a topic modeling approach to this difficult task, but not as well as SDTM."}, {"id": 164, "string": "FirstP shows good F-measure for the G level, but the H level F-measure is quite low, even lower than SEED."}, {"id": 165, "string": "Combining first-person pronouns and seed words and trigrams (FP+SE1) shows better than each feature alone, and the two stage classifier (FP+SE2) which is a similar approach taken in SDTM shows better results."}, {"id": 166, "string": "Finally, SDTM classifies G and M level at a similar accuracy with FirstP, FP+SE1 and FP+SE2, but it significantly improves accuracy for the H level compared to all other methods."}, {"id": 167, "string": "Relations of Self-Disclosure and Conversation Behaviors In this section, we investigate whether there is a relationship between self-disclosure and conversation behaviors over time."}, {"id": 168, "string": "Self-disclosure is one way to maintain and improve relationships (Jourard, 1971; Joinson and Paine, 2007) ."}, {"id": 169, "string": "So two people's intimacy changes over time has relationship with self-disclosure in their conversation."}, {"id": 170, "string": "However, it is hard to identify intimacy between users in large scale online social network."}, {"id": 171, "string": "So we choose conversation behaviors such as conversation frequency and length which can be treated as proxies for measuring intimacy between two people (Emmers- Sommer, 2004; Bak et al., 2012) ."}, {"id": 172, "string": "With SDTM, we can automatically classify the SD level of a large number of conversations, so we investigate whether there is a similar relationship between self-disclosure in conversations and subsequent conversation behaviors with the same partner on Twitter."}, {"id": 173, "string": "For comparing conversation behaviors over time, we divided the conversations into two sets for each dyad."}, {"id": 174, "string": "For the initial period, we include conversations from the dyad's first conversation to 20 days later."}, {"id": 175, "string": "And for the subsequent period, we include conversations during the subsequent 10 days."}, {"id": 176, "string": "We compute proportions of conversation for each SD level for each dyad in the initial and subsequent periods."}, {"id": 177, "string": "More specifically, we ask the following three questions: 1."}, {"id": 178, "string": "If a dyad shows high conversation frequency at a particular time period, would they display higher SD in their subsequent conversations?"}, {"id": 179, "string": "2."}, {"id": 180, "string": "If a dyad displays high SD level in their conversations at a particular time period, would their subsequent conversations be longer?"}, {"id": 181, "string": "3."}, {"id": 182, "string": "If a dyad displays high overall SD level, would their conversations increase in length over time more than dyads with lower overall SD level?"}, {"id": 183, "string": "Experiment Setup We first run SDTM with all of our Twitter conversation data with 150; 120; 120 topics for SDTM K G , K M and K H respectively."}, {"id": 184, "string": "The hyper-parameters are the same as in section 5."}, {"id": 185, "string": "To handle a large dataset, we employ a distributed algorithm (Newman et al., 2009) , and run with 28 threads."}, {"id": 186, "string": "Table 7 shows some of the topics that were prominent in each SD level by KL-divergence."}, {"id": 187, "string": "As expected, G level includes general topics such as food, celebrity, soccer and IT devices, M level includes personal communication and birthday, and finally, H level includes sickness and profanity."}, {"id": 188, "string": "We define a new measurement, SD level score for a dyad in the period, which is a weighted sum of each conversation with SD levels mapped to 1, 2, and 3, for the levels G, M, and H, respectively."}, {"id": 189, "string": "Figure 5 : Relationship between initial conversation frequency and subsequent SD level."}, {"id": 190, "string": "The solid line is the linear regression line, and the coefficient is 0.0020 with p < 0.0001, which shows a significant positive relationship."}, {"id": 191, "string": "Subsequent SD level 6.2 Does high frequency of conversation lead to more self-disclosure?"}, {"id": 192, "string": "We investigate whether the initial conversation frequency is correlated with the SD level in the subsequent period."}, {"id": 193, "string": "We run linear regression with the initial conversation frequency as the independent variable, and SD level in the subsequent period as the dependent variable."}, {"id": 194, "string": "The regression coefficient is 0.0020 with low pvalue (p < 0.0001)."}, {"id": 195, "string": "Figure 5 shows the scatter plot."}, {"id": 196, "string": "We can see that the slope of the regression line is positive."}, {"id": 197, "string": "Does high self-disclosure lead to longer conversations?"}, {"id": 198, "string": "Now we investigate the effect of the selfdisclosure level to conversation length."}, {"id": 199, "string": "We run linear regression with the intial SD level score as the independent variable, and the rate of change in conversation length between initial period and subsequent period as the dependent variable."}, {"id": 200, "string": "Conversation length is measured by the number of tweets in a conversation."}, {"id": 201, "string": "The result of regression is that the independent variable's coefficient is 0.048 with a low p-value (p < 0.0001)."}, {"id": 202, "string": "Figure 6 shows the scatter plot with the regression line, and we can see that the slope of regression line is positive."}, {"id": 203, "string": "H level  101  184  176  36  104  82  113  33  19  chocolate  obama  league  send  twitter  going  ass  better  lips  butter  he's  win  email  follow  party  bitch  sick  kisses  good  romney  game  i'll  tumblr  weekend  fuck  feel  love  cake  vote  season  sent  tweet  day  yo  throat smiles  peanut  right  team  dm  following  night  shit  cold  softly  milk  president  cup  address  account  dinner  fucking hope  hand  sugar  people  city  know  fb  birthday  lmao  pain  eyes  cream  good  arsenal  check  followers Now we investigate the conversation length changes over time with three groups, low, medium, and high, by overall SD level."}, {"id": 204, "string": "Then we investigate changes in conversation length over time."}, {"id": 205, "string": "Figure 7 shows the results of this investigation."}, {"id": 206, "string": "First, conversations are generally lengthier when SD level is high."}, {"id": 207, "string": "This phenomenon is also ob- We divide dyads into three groups by SD level score as low, medium, and high."}, {"id": 208, "string": "Conversation length noticeably increases over time in the medium and high groups, but only slight in the low group."}, {"id": 209, "string": "served in figure 6 , but here we can see it as a long-term persistent pattern."}, {"id": 210, "string": "Second, conversation length increases consistently and significantly for the high and medium groups, but for the low SD group, there is not a significant increase of conversation length over time."}, {"id": 211, "string": "G level M level Related Work Prior work on quantitatively analyzing selfdisclosure has relied on user surveys (Ledbetter et al., 2011; Trepte and Reinecke, 2013) or human annotation (Barak and Gluck-Ofri, 2007; Courtney Walton and Rice, 2013) ."}, {"id": 212, "string": "These methods consume much time and effort, so they are not suitable for large-scale studies."}, {"id": 213, "string": "In prior work closest to ours, Bak et al."}, {"id": 214, "string": "(2012) showed that a topic model can be used to identify self-disclosure, but that work applies a two-step process in which a basic topic model is first applied to find the topics, and then the topics are post-processed for binary classification of self-disclosure."}, {"id": 215, "string": "We improve upon this work by applying a single unified model of topics and self-disclosure for high accuracy in classifying the three levels of self-disclosure."}, {"id": 216, "string": "Subjectivity which is aspect of expressing opinions (Pang and Lee, 2008; Wiebe et al., 2004) is related with self-disclosure, but they are different dimensions of linguistic behavior."}, {"id": 217, "string": "Because there indeed are many high self-disclosure tweets that are subjective, but there are also counter examples in annotated dataset."}, {"id": 218, "string": "The tweet \"England manager is Roy Hodgson.\""}, {"id": 219, "string": "is low self-disclosure and low subjectivity, \"I have barely any hair left.\""}, {"id": 220, "string": "is high self-disclosure but low subjectivity, and \"Senator stop lying!\""}, {"id": 221, "string": "is low self-disclosure but high subjectivity."}, {"id": 222, "string": "Conclusion and Future Work In this paper, we have presented the self-disclosure topic model (SDTM) for discovering topics and classifying SD levels from Twitter conversation data."}, {"id": 223, "string": "We devised a set of effective seed words and trigrams, mined from a dataset of secrets."}, {"id": 224, "string": "We also annotated Twitter conversations to make a ground-truth dataset for SD level."}, {"id": 225, "string": "With annotated data, we showed that SDTM outperforms previous methods in classification accuracy and Fmeasure."}, {"id": 226, "string": "We publish the source code of SDTM and the dataset include annotated Twitter conversations and SECRET publicly 7 ."}, {"id": 227, "string": "We also analyzed the relationship between SD level and conversation behaviors over time."}, {"id": 228, "string": "We found that there is a positive correlation between initial SD level and subsequent conversation length."}, {"id": 229, "string": "Also, dyads show higher level of SD if they initially display high conversation frequency."}, {"id": 230, "string": "Finally, dyads with overall medium and high SD level will have longer conversations over time."}, {"id": 231, "string": "These results support previous results in so-7 http://uilab.kaist.ac.kr/research/ EMNLP2014 cial psychology research with more robust results from a large-scale dataset, and show the effectiveness of computationally analyzing at SD behavior."}, {"id": 232, "string": "There are several future directions for this research."}, {"id": 233, "string": "First, we can improve our modeling for higher accuracy and better interpretability."}, {"id": 234, "string": "For instance, SDTM only considers first-person pronouns and topics."}, {"id": 235, "string": "Naturally, there are other linguistic patterns that can be identified by humans but not captured by pronouns and topics."}, {"id": 236, "string": "Second, the number of topics for each level is varied, and so we can explore nonparametric topic models (Teh et al., 2006) which infer the number of topics from the data."}, {"id": 237, "string": "Third, we can look at the relationship between self-disclosure behavior and general online social network usage beyond conversations."}, {"id": 238, "string": "We will explore these directions in our future work."}], "headers": [{"section": "Introduction", "n": "1", "start": 0, "end": 22}, {"section": "Self-Disclosure", "n": "2", "start": 23, "end": 25}, {"section": "Self-disclosure (SD) level", "n": "2.1", "start": 26, "end": 26}, {"section": "G Level of Self-Disclosure", "n": "2.2", "start": 27, "end": 32}, {"section": "M Level of Self-Disclosure", "n": "2.3", "start": 33, "end": 38}, {"section": "H Level of Self-Disclosure", "n": "2.4", "start": 39, "end": 55}, {"section": "Model", "n": "3.1", "start": 56, "end": 87}, {"section": "Classifying G vs M/H levels", "n": "3.2", "start": 88, "end": 90}, {"section": "Classifying M vs H levels", "n": "3.3", "start": 91, "end": 94}, {"section": "Inference", "n": "3.4", "start": 95, "end": 97}, {"section": "Data Collection and Annotation", "n": "4", "start": 98, "end": 104}, {"section": "Collecting Twitter conversations", "n": "4.1", "start": 105, "end": 113}, {"section": "Annotating self-disclosure level", "n": "4.2", "start": 114, "end": 116}, {"section": "Classification of Self-Disclosure Level", "n": "5", "start": 117, "end": 166}, {"section": "Relations of Self-Disclosure and Conversation Behaviors", "n": "6", "start": 167, "end": 182}, {"section": "Experiment Setup", "n": "6.1", "start": 183, "end": 197}, {"section": "Does high self-disclosure lead to longer conversations?", "n": "6.3", "start": 198, "end": 210}, {"section": "Related Work", "n": "7", "start": 211, "end": 221}, {"section": "Conclusion and Future Work", "n": "8", "start": 222, "end": 238}], "figures": [{"filename": "../figure/image/1188-Figure4-1.png", "caption": "Figure 4: Screenshot of annotation web-based platform. Annotators read a Twitter conversation and annotate self-disclosure level to each tweet.", "page": 5, "bbox": {"x1": 72.0, "x2": 295.2, "y1": 61.44, "y2": 330.24}}, {"filename": "../figure/image/1188-Figure1-1.png", "caption": "Figure 1: An example of a Twitter conversation (from annotated dataset) with G, M and H level of self-disclosure.", "page": 1, "bbox": {"x1": 306.71999999999997, "x2": 526.0799999999999, "y1": 61.44, "y2": 327.84}}, {"filename": "../figure/image/1188-Table6-1.png", "caption": "Table 6: SD level classification accuracies and Fmeasures using annotated data. Acc is accuracy, and G F1 is F-measure for classifying the G level. Avg F1 is the macroaveraged value of G F1, M F1 and H F1. SDTM outperforms all other methods compared. The difference between SDTM and FirstP is statistically significant (p-value < 0.05 for accuracy, < 0.0001 for Avg F1).", "page": 6, "bbox": {"x1": 72.0, "x2": 298.08, "y1": 61.44, "y2": 226.07999999999998}}, {"filename": "../figure/image/1188-Table1-1.png", "caption": "Table 1: High ranked words and expressions by mutual information between G and M/H level in annotated conversations.", "page": 2, "bbox": {"x1": 72.0, "x2": 295.2, "y1": 61.44, "y2": 137.28}}, {"filename": "../figure/image/1188-Table2-1.png", "caption": "Table 2: Example seed trigrams for identifying M level of SD. There are 51 of these used in SDTM.", "page": 2, "bbox": {"x1": 80.64, "x2": 282.24, "y1": 491.52, "y2": 575.04}}, {"filename": "../figure/image/1188-Table3-1.png", "caption": "Table 3: Example words for identifying H level of SD from secret posts (2nd column) and annotated data (3rd column). Categories are hand-labeled.", "page": 2, "bbox": {"x1": 306.71999999999997, "x2": 534.24, "y1": 61.44, "y2": 136.32}}, {"filename": "../figure/image/1188-Figure5-1.png", "caption": "Figure 5: Relationship between initial conversation frequency and subsequent SD level. The solid line is the linear regression line, and the coefficient is 0.0020 with p < 0.0001, which shows a significant positive relationship.", "page": 7, "bbox": {"x1": 310.08, "x2": 527.52, "y1": 65.75999999999999, "y2": 232.79999999999998}}, {"filename": "../figure/image/1188-Table4-1.png", "caption": "Table 4: Summary of notations used in SDTM", "page": 3, "bbox": {"x1": 72.0, "x2": 295.2, "y1": 215.51999999999998, "y2": 497.28}}, {"filename": "../figure/image/1188-Figure3-1.png", "caption": "Figure 3: Generative process of SDTM.", "page": 3, "bbox": {"x1": 309.12, "x2": 519.84, "y1": 63.839999999999996, "y2": 286.56}}, {"filename": "../figure/image/1188-Figure2-1.png", "caption": "Figure 2: Graphical model of SDTM", "page": 3, "bbox": {"x1": 102.72, "x2": 260.15999999999997, "y1": 62.879999999999995, "y2": 180.0}}, {"filename": "../figure/image/1188-Table7-1.png", "caption": "Table 7: High ranked topics in each level by comparing KL-divergence with other level\u2019s topics", "page": 8, "bbox": {"x1": 75.84, "x2": 521.28, "y1": 61.44, "y2": 295.2}}, {"filename": "../figure/image/1188-Figure6-1.png", "caption": "Figure 6: Relationship between initial SD level and conversation length changes over time. The solid line is the linear regression line, and the coefficient is 0.048 with p < 0.0001, which shows a significant positive relationship.", "page": 8, "bbox": {"x1": 74.88, "x2": 292.32, "y1": 338.88, "y2": 509.28}}, {"filename": "../figure/image/1188-Figure7-1.png", "caption": "Figure 7: Changes in conversation length over time. We divide dyads into three groups by SD level score as low, medium, and high. Conversation length noticeably increases over time in the medium and high groups, but only slight in the low group.", "page": 8, "bbox": {"x1": 310.08, "x2": 527.04, "y1": 340.8, "y2": 504.96}}, {"filename": "../figure/image/1188-Table5-1.png", "caption": "Table 5: Dataset of Twitter conversations. We chose conversations consisting of five or more tweets each. We chose dyads with twenty or more conversations.", "page": 4, "bbox": {"x1": 312.0, "x2": 521.28, "y1": 61.44, "y2": 91.2}}]}