{"title": "Probing the Need for Visual Context in Multimodal Machine Translation", "abstract": "Current work on multimodal machine translation (MMT) has suggested that the visual modality is either unnecessary or only marginally beneficial. We posit that this is a consequence of the very simple, short and repetitive sentences used in the only available dataset for the task (Multi30K), rendering the source text sufficient as context. In the general case, however, we believe that it is possible to combine visual and textual information in order to ground translations. In this paper we probe the contribution of the visual modality to state-of-the-art MMT models by conducting a systematic analysis where we partially deprive the models from source-side textual context. Our results show that under limited textual context, models are capable of leveraging the visual input to generate better translations. This contradicts the current belief that MMT models disregard the visual modality because of either the quality of the image features or the way they are integrated into the model.", "text": [{"id": 0, "string": "Introduction Multimodal Machine Translation (MMT) aims at designing better translation systems which take into account auxiliary inputs such as images."}, {"id": 1, "string": "Initially organized as a shared task within the First Conference on Machine Translation (WMT16) , MMT has so far been studied using the Multi30K dataset , a multilingual extension of Flickr30K (Young et al., 2014) with translations of the English image descriptions into German, French and Czech (Elliott et al., 2017; Barrault et al., 2018) ."}, {"id": 2, "string": "The three editions of the shared task have seen many exciting approaches that can be broadly categorized as follows: (i) multimodal attention using convolutional features (Caglayan et al., 2016; Calixto et al., 2016; Libovick\u00fd and Helcl, 2017; Helcl et al., 2018 ) (ii) cross-modal interactions with spatially-unaware global features (Calixto and Liu, 2017; Ma et al., 2017; Caglayan et al., 2017a; Madhyastha et al., 2017) and (iii) the integration of regional features from object detection networks (Huang et al., 2016; Gr\u00f6nroos et al., 2018) ."}, {"id": 3, "string": "Nevertheless, the conclusion about the contribution of the visual modality is still unclear: Gr\u00f6nroos et al."}, {"id": 4, "string": "(2018) consider their multimodal gains \"modest\" and attribute the largest gain to the usage of external parallel corpora."}, {"id": 5, "string": "Lala et al."}, {"id": 6, "string": "(2018) observe that their multimodal word-sense disambiguation approach is not significantly different than the monomodal counterpart."}, {"id": 7, "string": "The organizers of the latest edition of the shared task concluded that the multimodal integration schemes explored so far resulted in marginal changes in terms of automatic metrics and human evaluation (Barrault et al., 2018) ."}, {"id": 8, "string": "In a similar vein, Elliott (2018) demonstrated that MMT models can translate without significant performance losses even in the presence of features from unrelated images."}, {"id": 9, "string": "These empirical findings seem to indicate that images are ignored by the models and hint at the fact that this is due to representation or modeling limitations."}, {"id": 10, "string": "We conjecture that the most plausible reason for the linguistic dominance is that -at least in Multi30K -the source text is sufficient to perform the translation, eventually preventing the visual information from intervening in the learning process."}, {"id": 11, "string": "To investigate this hypothesis, we introduce several input degradation regimes (Section 2) and revisit state-of-the-art MMT models (Section 3) to assess their behavior under degraded regimes."}, {"id": 12, "string": "We further probe the visual sensitivity by deliberately feeding features from unrelated images."}, {"id": 13, "string": "Our results (Section 4) show that MMT models successfully exploit the visual modality when the linguistic context is scarce, but indeed tend to be less sensitive to this modality when exposed to complete sentences."}, {"id": 14, "string": "Input Degradation In this section we propose several degradations to the input language modality to simulate conditions where sentences may miss crucial information."}, {"id": 15, "string": "We denote a set of translation pairs by D and indicate degraded variants with subscripts."}, {"id": 16, "string": "Both the training and the test sets are degraded."}, {"id": 17, "string": "Color Deprivation."}, {"id": 18, "string": "We consistently replace source words that refer to colors with a special token [v] (D C in Table 1 )."}, {"id": 19, "string": "Our hypothesis is that a monomodal system will have to rely on sourceside contextual information and biases, while a multimodal architecture could potentially capitalize on color information extracted by exploiting the image and thus obtain better performance."}, {"id": 20, "string": "This affects 3.3% and 3.1% of the words in the training and the test set, respectively."}, {"id": 21, "string": "Entity Masking."}, {"id": 22, "string": "The Flickr30K dataset, from which Multi30K is derived, has also been extended with coreference chains to tag mentions of visually depictable entities in image descriptions (Plummer et al., 2015) ."}, {"id": 23, "string": "We use these to mask out the head nouns in the source sentences (D N in Table 1)."}, {"id": 24, "string": "This affects 26.2% of the words in both the training and the test set."}, {"id": 25, "string": "We hypothesize that a multimodal system should heavily rely on the images to infer the missing parts."}, {"id": 26, "string": "Progressive Masking."}, {"id": 27, "string": "A progressively degraded variant D k replaces all but the first k tokens of source sentences with [v] ."}, {"id": 28, "string": "Unlike the color deprivation and entity masking, masking out suffixes does not guarantee systematic removal of visual context, but rather simulates an increasingly low-resource scenario."}, {"id": 29, "string": "Overall, we form 16 degraded variants D k (Table 1) where k \u2208 {0, 2, ."}, {"id": 30, "string": "."}, {"id": 31, "string": "."}, {"id": 32, "string": ", 30}."}, {"id": 33, "string": "We stop at D 30 since 99.8% of the sentences in Multi30K are shorter than 30 words with an average sentence length of 12 words."}, {"id": 34, "string": "D 0 -where the only remaining information is the source sentence length -is an interesting case from two perspectives: a neural machine translation (NMT) model trained on it resembles a target language model, while an MMT model becomes an image captioner with access to \"expected length information\"."}, {"id": 35, "string": "Visual Sensitivity."}, {"id": 36, "string": "Inspired by Elliott (2018) , we experiment with incongruent decoding in order to understand how sensitive the multimodal systems are to the visual modality."}, {"id": 37, "string": "This is achieved D a lady in a blue dress singing D C a lady in a [v] dress singing D N a [v] in a blue [v] singing D 4 a lady in a [v] [v] [v] D 2 a lady [v] [v] [v] [v] [v] D 0 [v] [v] [v] [v] [v] [v] [v] Experimental Setup Dataset."}, {"id": 38, "string": "We conduct experiments on the English\u2192French part of Multi30K."}, {"id": 39, "string": "The models are trained on the concatenation of the train and val sets (30K sentences) whereas test2016 (dev) and test2017 (test) are used for early-stopping and model evaluation, respectively."}, {"id": 40, "string": "For entity masking, we revert to the default Flickr30K splits and perform the model evaluation on test2016, since test2017 is not annotated for entities."}, {"id": 41, "string": "We use word-level vocabularies of 9,951 English and 11,216 French words."}, {"id": 42, "string": "We use Moses (Koehn et al., 2007) scripts to lowercase, normalize and tokenize the sentences with hyphen splitting."}, {"id": 43, "string": "The hyphens are stitched back prior to evaluation."}, {"id": 44, "string": "Visual Features."}, {"id": 45, "string": "We use a ResNet-50 CNN (He et al., 2016) trained on ImageNet (Deng et al., 2009 ) as image encoder."}, {"id": 46, "string": "Prior to feature extraction, we center and standardize the images using ImageNet statistics, resize the shortest edge to 256 pixels and take a center crop of size 256x256."}, {"id": 47, "string": "We extract spatial features of size 2048x8x8 from the final convolutional layer and apply L 2 normalization along the depth dimension (Caglayan et al., 2018) ."}, {"id": 48, "string": "For the non-attentive model, we use the 2048-dimensional global average pooled version (pool5) of the above convolutional features."}, {"id": 49, "string": "Models."}, {"id": 50, "string": "Our baseline NMT is an attentive model  with a 2-layer bidirectional GRU encoder ) and a 2-layer conditional GRU decoder (Sennrich et al., 2017) ."}, {"id": 51, "string": "The second layer of the decoder receives the output of the attention layer as input."}, {"id": 52, "string": "For the MMT model, we explore the basic multimodal attention (DIRECT) (Caglayan et al., 2016) and its hierarchical (HIER) extension (Libovick\u00fd and Helcl, 2017) ."}, {"id": 53, "string": "The former linearly projects the concatenation of textual and visual context vectors to obtain the multimodal context vector, while the latter replaces the concatenation with another attention layer."}, {"id": 54, "string": "Finally, we also experiment with encoder-decoder initialization (INIT) (Calixto and Liu, 2017; Caglayan et al., 2017a) where we initialize both the encoder and the decoder using a non-linear transformation of the pool5 features."}, {"id": 55, "string": "Hyperparameters."}, {"id": 56, "string": "The encoder and decoder GRUs have 400 hidden units and are initialized with 0 except the multimodal INIT system."}, {"id": 57, "string": "All embeddings are 200-dimensional and the decoder embeddings are tied (Press and Wolf, 2016) ."}, {"id": 58, "string": "A dropout of 0.4 and 0.5 is applied on source embeddings and encoder/decoder outputs, respectively (Srivastava et al., 2014) ."}, {"id": 59, "string": "The weights are decayed with a factor of 1e\u22125."}, {"id": 60, "string": "We use ADAM (Kingma and Ba, 2014) with a learning rate of 4e\u22124 and mini-batches of 64 samples."}, {"id": 61, "string": "The gradients are clipped if the total norm exceeds 1 (Pascanu et al., 2013) ."}, {"id": 62, "string": "The training is early-stopped if dev set ME-TEOR (Denkowski and Lavie, 2014) does not improve for ten epochs."}, {"id": 63, "string": "All experiments are conducted with nmtpytorch 1 (Caglayan et al., 2017b) ."}, {"id": 64, "string": "Results We train all systems three times each with different random initialization in order to perform significance testing with multeval (Clark et al., 2011) ."}, {"id": 65, "string": "Throughout the section, we always report the mean over three runs (and the standard deviation) of the considered metrics."}, {"id": 66, "string": "We decode the translations with a beam size of 12."}, {"id": 67, "string": "We first present test2017 METEOR scores for the baseline NMT and MMT systems, when trained on the full dataset D ( Table 2 )."}, {"id": 68, "string": "The first column indicates that, although MMT models perform slightly better on average, they are not significantly better than the baseline NMT."}, {"id": 69, "string": "We now introduce and discuss the results obtained under the proposed degradation schemes."}, {"id": 70, "string": "Please refer to Table 5 and the appendix for qualitative examples."}, {"id": 71, "string": "Color Deprivation Unlike the inconclusive results for D, we observe that all MMT models are significantly better than NMT when color deprivation is applied (D C in Table 2 )."}, {"id": 72, "string": "If we further focus on the subset of the test set subjected to color deprivation (247 sentences), the gain increases to 1.6 METEOR for HIER."}, {"id": 73, "string": "For the latter subset, we also computed the average color accuracy per sentence and found that the attentive models are 12% better than the NMT (32.5\u219244.5) whereas the INIT model only brings 4% (32.5\u219236.5) improvement."}, {"id": 74, "string": "This shows that more complex MMT models are better at integrating visual information to perform better."}, {"id": 75, "string": "Entity Masking The gains are much more prominent with entity masking, where the degradation occurs at a larger scale: Attentive MMT models show up to 4.2 ME-TEOR improvement over NMT (Figure 1) ."}, {"id": 76, "string": "We observed a large performance drop with incongruent decoding, suggesting that the visual modality is Czech +1.4 (\u2193 2.9) +1.7 (\u2193 3.5) +1.7 (\u2193 4.1) German +2.1 (\u2193 4.7) +2.5 (\u2193 5.9) +2.7 (\u2193 6.5) French +3.4 (\u2193 6.5) +3.9 (\u2193 9.0) +4.2 (\u2193 9.7) Table 3 : Entity masking results across three languages: all MMT models perform significantly better than their NMT counterparts (p-value \u2264 0.01)."}, {"id": 77, "string": "The incongruence drop applies on top of the MMT score."}, {"id": 78, "string": "now much more important than previously demonstrated (Elliott, 2018) ."}, {"id": 79, "string": "A comparison of attention maps produced by the baseline and masked MMT models reveals that the attention weights are more consistent in the latter."}, {"id": 80, "string": "An interesting example is given in Figure 2 where the masked MMT model attends to the correct region of the image and successfully translates a dropped word that was otherwise a spelling mistake (\"son\"\u2192\"song\")."}, {"id": 81, "string": "Czech and German."}, {"id": 82, "string": "In order to understand whether the above observations are also consistent across different languages, we extend the entity masking experiments to German and Czech parts of Multi30K."}, {"id": 83, "string": "Table 3 shows the gain of each MMT system with respect to the NMT model and the subsequent drop caused by incongruent decoding 3 ."}, {"id": 84, "string": "First, we see that the multimodal benefits clearly hold for German and Czech, although the gains are lower than for French 4 ."}, {"id": 85, "string": "Second, when we compute the average drop from using incongruent images across all languages, we see how conservative the INIT system is (\u2193 4.7) compared   to HIER (\u2193 6.1) and DIRECT (\u2193 6.8)."}, {"id": 86, "string": "This raises a follow-up question as to whether the hidden state initialization eventually loses its impact throughout the recurrence where, as a consequence, the only modality processed is the text."}, {"id": 87, "string": "Progressive Masking Finally, we discuss the results of the progressive masking experiments for French."}, {"id": 88, "string": "Figure 3 clearly shows that as the sentences are progressively degraded, all MMT systems are able to leverage the visual modality."}, {"id": 89, "string": "When the multimodal task becomes image captioning at k=0, MMT models improve over the language-model counterpart by \u223c7 METEOR."}, {"id": 90, "string": "Further qualitative examples show that the systems perform surprisingly well by producing visually plausible sentences (see Table 5 and the Appendix)."}, {"id": 91, "string": "To get a sense of the visual sensitivity, we pick the DIRECT models trained on four degraded variants and perform incongruent decoding."}, {"id": 92, "string": "We notice that as the amount of linguistic information increases, the gap narrows down: the MMT system gradually becomes less perplexed by the incongruence or, put in other words, less sensitive to the visual modality (Table 4) ."}, {"id": 93, "string": "[v] [v] NMT: une femme\u00e2g\u00e9e avec un t-shirt blanc et des lunettes de soleil est assise sur un banc (an older woman with a white t-shirt and sunglasses is sitting on a bank) MMT: une femme\u00e2g\u00e9e en maillot de bain rose est assise sur un rocher au bord de l'eau (an older woman with a pink swimsuit is sitting on a rock at the seaside) REF: une femme\u00e2g\u00e9e en bikini bronze sur un rocher au bord de l'oc\u00e9an (an older woman in bikini is tanning on a rock at the edge of the ocean)  We then conduct a contrastive \"blinding\" experiment where the DIRECT models are not only fed with incongruent features at decoding time but also trained with them from scratch."}, {"id": 94, "string": "The results suggest that the blinded models learn to ignore the visual modality."}, {"id": 95, "string": "In fact, their performance is equivalent to NMT models."}, {"id": 96, "string": "SRC: an older woman in [v][v][v][v][v][v][v][v][v] Discussion and Conclusions We presented an in-depth study on the potential contribution of images for multimodal machine translation."}, {"id": 97, "string": "Specifically, we analysed the behavior of state-of-the-art MMT models under several degradation schemes in the Multi30K dataset, in order to reveal and understand the impact of textual predominance."}, {"id": 98, "string": "Our results show that the models explored are able to integrate the visual modality if the available modalities are complementary rather than redundant."}, {"id": 99, "string": "In the latter case, the primary modality (text) sufficient to accomplish the task."}, {"id": 100, "string": "This dominance effect corroborates the seminal work of Colavita (1974) in Psychophysics where it has been demonstrated that visual stimuli dominate over the auditory stimuli when humans are asked to perform a simple audiovisual discrimination task."}, {"id": 101, "string": "Our investigation using source degradation also suggests that visual grounding can increase the robustness of machine translation systems by mitigating input noise such as errors in the source text."}, {"id": 102, "string": "In the future, we would like to devise models that can learn when and how to integrate multiple modalities by taking care of the complementary and redundant aspects of them in an intelligent way."}, {"id": 103, "string": "A Qualitative Examples In this appendix, we provide further translation examples for color deprivation (Table 6) , entity masking (Table 7) and progressive masking (Table 8)."}, {"id": 104, "string": "Specifically for the entity masking experiments, we also give further examples to showcase the behavior of the visual attention in Figure 4 and Figure 5 ."}], "headers": [{"section": "Introduction", "n": "1", "start": 0, "end": 13}, {"section": "Input Degradation", "n": "2", "start": 14, "end": 37}, {"section": "Experimental Setup", "n": "3", "start": 38, "end": 63}, {"section": "Results", "n": "4", "start": 64, "end": 70}, {"section": "Color Deprivation", "n": "4.1", "start": 71, "end": 74}, {"section": "Entity Masking", "n": "4.2", "start": 75, "end": 86}, {"section": "Progressive Masking", "n": "4.3", "start": 87, "end": 95}, {"section": "Discussion and Conclusions", "n": "5", "start": 96, "end": 104}], "figures": [{"filename": "../figure/image/997-Figure5-1.png", "caption": "Figure 5: Attention example from entity masking experiments where terrier, grass and fence are dropped from the source sentence: (a) Baseline MMT is not able to shift attention from the salient dog to the grass and fence, (b) the attention produced by the masked MMT first shifts to the background area while translating \u201con lush green [v]\u201d then focuses on the fence.", "page": 10, "bbox": {"x1": 76.32, "x2": 512.16, "y1": 175.68, "y2": 594.24}}, {"filename": "../figure/image/997-Table1-1.png", "caption": "Table 1: An example of the proposed input degradation schemes: D is the original sentence.", "page": 1, "bbox": {"x1": 307.68, "x2": 525.12, "y1": 62.4, "y2": 147.35999999999999}}, {"filename": "../figure/image/997-Figure4-1.png", "caption": "Figure 4: Attention example from entity masking experiments: (a) Baseline MMT translates the misspelled \u201cson\u201d (song \u2192 chanson) while (b) the masked MMT achieves a correct translation ([v]\u2192 enfant) by exploiting the visual modality.", "page": 9, "bbox": {"x1": 76.32, "x2": 512.16, "y1": 204.48, "y2": 576.9599999999999}}, {"filename": "../figure/image/997-Figure1-1.png", "caption": "Figure 1: Entity masking: all masked MMT models are significantly better than the masked NMT (dashed). Incongruent decoding severely worsens all systems. The vanilla NMT baseline is 75.92.", "page": 2, "bbox": {"x1": 312.96, "x2": 519.36, "y1": 62.879999999999995, "y2": 204.0}}, {"filename": "../figure/image/997-Table2-1.png", "caption": "Table 2: Baseline and color-deprivation METEOR scores: bold systems are significantly different from the NMT system within the same column (p-value\u2264 0.03).", "page": 2, "bbox": {"x1": 87.84, "x2": 274.08, "y1": 62.879999999999995, "y2": 139.2}}, {"filename": "../figure/image/997-Table6-1.png", "caption": "Table 6: Color deprivation examples from the English\u2192French models: bold indicates correctly predicted cases. The colors generated by the models are shown in English for the sake of clarity.", "page": 7, "bbox": {"x1": 93.6, "x2": 503.03999999999996, "y1": 179.51999999999998, "y2": 612.0}}, {"filename": "../figure/image/997-Table4-1.png", "caption": "Table 4: The impact of incongruent decoding for progressive masking: all METEOR differences are against the DIRECT model. The blinded systems are both trained and decoded using incongruent features.", "page": 3, "bbox": {"x1": 306.71999999999997, "x2": 526.0799999999999, "y1": 238.56, "y2": 319.2}}, {"filename": "../figure/image/997-Figure3-1.png", "caption": "Figure 3: Multimodal gain in absolute METEOR for progressive masking: the dashed gray curve indicates the percentage of non-masked words in the training set.", "page": 3, "bbox": {"x1": 308.15999999999997, "x2": 525.12, "y1": 60.96, "y2": 180.95999999999998}}, {"filename": "../figure/image/997-Table3-1.png", "caption": "Table 3: Entity masking results across three languages: all MMT models perform significantly better than their NMT counterparts (p-value\u2264 0.01). The incongruence drop applies on top of the MMT score.", "page": 3, "bbox": {"x1": 75.84, "x2": 286.08, "y1": 238.56, "y2": 322.08}}, {"filename": "../figure/image/997-Figure2-1.png", "caption": "Figure 2: Baseline MMT (top) translates the misspelled \u201cson\u201d while the masked MMT (bottom) correctly produces \u201cenfant\u201d (child) by focusing on the image.", "page": 3, "bbox": {"x1": 89.75999999999999, "x2": 272.15999999999997, "y1": 61.44, "y2": 181.92}}, {"filename": "../figure/image/997-Table8-1.png", "caption": "Table 8: English\u2192French progressive masking examples: underlined and bold words highlight bad and good lexical choices, respectively. English translations are provided in parentheses. MMT is an attentive model.", "page": 11, "bbox": {"x1": 73.92, "x2": 524.16, "y1": 116.16, "y2": 676.3199999999999}}, {"filename": "../figure/image/997-Table7-1.png", "caption": "Table 7: Entity masking examples from the English\u2192French models: underlined and bold words highlight bad and good lexical choices, respectively. English translations are provided in parentheses. MMT is an attentive model.", "page": 8, "bbox": {"x1": 116.64, "x2": 481.44, "y1": 184.79999999999998, "y2": 607.1999999999999}}, {"filename": "../figure/image/997-Table5-1.png", "caption": "Table 5: Qualitative examples from progressive masking, entity masking and color deprivation, respectively. Underlined and bold words highlight the bad and good lexical choices. MMT is an attentive system.", "page": 4, "bbox": {"x1": 82.56, "x2": 515.04, "y1": 68.16, "y2": 310.08}}]}