{"title": "Stock Movement Prediction from Tweets and Historical Prices", "abstract": "Stock movement prediction is a challenging problem: the market is highly stochastic, and we make temporally-dependent predictions from chaotic data. We treat these three complexities and present a novel deep generative model jointly exploiting text and price signals for this task. Unlike the case with discriminative or topic modeling, our model introduces recurrent, continuous latent variables for a better treatment of stochasticity, and uses neural variational inference to address the intractable posterior inference. We also provide a hybrid objective with temporal auxiliary to flexibly capture predictive dependencies. We demonstrate the stateof-the-art performance of our proposed model on a new stock movement prediction dataset which we collected. 1", "text": [{"id": 0, "string": "Introduction Stock movement prediction has long attracted both investors and researchers (Frankel, 1995; Edwards et al., 2007; Bollen et al., 2011; Hu et al., 2018) ."}, {"id": 1, "string": "We present a model to predict stock price movement from tweets and historical stock prices."}, {"id": 2, "string": "In natural language processing (NLP), public news and social media are two primary content resources for stock market prediction, and the models that use these sources are often discriminative."}, {"id": 3, "string": "Among them, classic research relies heavily on feature engineering (Schumaker and Chen, 2009; Oliveira et al., 2013) ."}, {"id": 4, "string": "With the prevalence of deep neural networks (Le and Mikolov, 2014) , eventdriven approaches were studied with structured event representations (Ding et al., 2014 (Ding et al., , 2015 ."}, {"id": 5, "string": "More recently, Hu et al."}, {"id": 6, "string": "(2018) propose to mine news sequence directly from text with hierarchical attention mechanisms for stock trend prediction."}, {"id": 7, "string": "However, stock movement prediction is widely considered difficult due to the high stochasticity of the market: stock prices are largely driven by new information, resulting in a random-walk pattern (Malkiel, 1999) ."}, {"id": 8, "string": "Instead of using only deterministic features, generative topic models were extended to jointly learn topics and sentiments for the task (Si et al., 2013; Nguyen and Shirai, 2015) ."}, {"id": 9, "string": "Compared to discriminative models, generative models have the natural advantage in depicting the generative process from market information to stock signals and introducing randomness."}, {"id": 10, "string": "However, these models underrepresent chaotic social texts with bag-of-words and employ simple discrete latent variables."}, {"id": 11, "string": "In essence, stock movement prediction is a time series problem."}, {"id": 12, "string": "The significance of the temporal dependency between movement predictions is not addressed in existing NLP research."}, {"id": 13, "string": "For instance, when a company suffers from a major scandal on a trading day d 1 , generally, its stock price will have a downtrend in the coming trading days until day d 2 , i.e."}, {"id": 14, "string": "[d 1 , d 2 ]."}, {"id": 15, "string": "2 If a stock predictor can recognize this decline pattern, it is likely to benefit all the predictions of the movements during [d 1 , d 2 ]."}, {"id": 16, "string": "Otherwise, the accuracy in this interval might be harmed."}, {"id": 17, "string": "This predictive dependency is a result of the fact that public information, e.g."}, {"id": 18, "string": "a company scandal, needs time to be absorbed into movements over time (Luss and d'Aspremont, 2015) , and thus is largely shared across temporally-close predictions."}, {"id": 19, "string": "Aiming to tackle the above-mentioned outstanding research gaps in terms of modeling high market stochasticity, chaotic market information and temporally-dependent prediction, we propose StockNet, a deep generative model for stock movement prediction."}, {"id": 20, "string": "To better incorporate stochastic factors, we generate stock movements from latent driven factors modeled with recurrent, continuous latent variables."}, {"id": 21, "string": "Motivated by Variational Auto-Encoders (VAEs; Kingma and Welling, 2013; Rezende et al., 2014) , we propose a novel decoder with a variational architecture and derive a recurrent variational lower bound for end-to-end training (Section 5.2)."}, {"id": 22, "string": "To the best of our knowledge, StockNet is the first deep generative model for stock movement prediction."}, {"id": 23, "string": "To fully exploit market information, StockNet directly learns from data without pre-extracting structured events."}, {"id": 24, "string": "We build market sources by referring to both fundamental information, e.g."}, {"id": 25, "string": "tweets, and technical features, e.g."}, {"id": 26, "string": "historical stock prices (Section 5.1)."}, {"id": 27, "string": "3 To accurately depict predictive dependencies, we assume that the movement prediction for a stock can benefit from learning to predict its historical movements in a lag window."}, {"id": 28, "string": "We propose trading-day alignment as the framework basis (Section 4), and further provide a novel multi-task learning objective (Section 5.3)."}, {"id": 29, "string": "We evaluate StockNet on a stock movement prediction task with a new dataset that we collected."}, {"id": 30, "string": "Compared with strong baselines, our experiments show that StockNet achieves state-of-the-art performance by incorporating both data from Twitter and historical stock price listings."}, {"id": 31, "string": "Problem Formulation We aim at predicting the movement of a target stock s in a pre-selected stock collection S on a target trading day d. Formally, we use the market information comprising of relevant social media corpora M, i.e."}, {"id": 32, "string": "tweets, and historical prices, in the lag [d \u2212 \u2206d, d \u2212 1] where \u2206d is a fixed lag size."}, {"id": 33, "string": "We estimate the binary movement where 1 denotes rise and 0 denotes fall, y = 1 p c d > p c d\u22121 (1) where p c d denotes the adjusted closing price adjusted for corporate actions affecting stock prices, e.g."}, {"id": 34, "string": "dividends and splits."}, {"id": 35, "string": "4 The adjusted closing 3 To a fundamentalist, stocks have their intrinsic values that can be derived from the behavior and performance of their company."}, {"id": 36, "string": "On the contrary, technical analysis considers only the trends and patterns of the stock price."}, {"id": 37, "string": "4 Technically, d \u2212 1 may not be an eligible trading day and thus has no available price information."}, {"id": 38, "string": "In the rest of this price is widely used for predicting stock price movement (Xie et al., 2013) or financial volatility (Rekabsaz et al., 2017) ."}, {"id": 39, "string": "Data Collection In finance, stocks are categorized into 9 industries: Basic Materials, Consumer Goods, Healthcare, Services, Utilities, Conglomerates, Financial, Industrial Goods and Technology."}, {"id": 40, "string": "5 Since high-tradevolume-stocks tend to be discussed more on Twitter, we select the two-year price movements from 01/01/2014 to 01/01/2016 of 88 stocks to target, coming from all the 8 stocks in Conglomerates and the top 10 stocks in capital size in each of the other 8 industries (see supplementary material)."}, {"id": 41, "string": "We observe that there are a number of targets with exceptionally minor movement ratios."}, {"id": 42, "string": "In a three-way stock trend prediction task, a common practice is to categorize these movements to another \"preserve\" class by setting upper and lower thresholds on the stock price change (Hu et al., 2018) ."}, {"id": 43, "string": "Since we aim at the binary classification of stock changes identifiable from social media, we set two particular thresholds, -0.5% and 0.55% and simply remove 38.72% of the selected targets with the movement percents between the two thresholds."}, {"id": 44, "string": "Samples with the movement percents \u2264-0.5% and >0.55% are labeled with 0 and 1, respectively."}, {"id": 45, "string": "The two thresholds are selected to balance the two classes, resulting in 26,614 prediction targets in the whole dataset with 49.78% and 50.22% of them in the two classes."}, {"id": 46, "string": "We split them temporally and 20,339 movements between 01/01/2014 and 01/08/2015 are for training, 2,555 movements from 01/08/2015 to 01/10/2015 are for development, and 3,720 movements from 01/10/2015 to 01/01/2016 are for test."}, {"id": 47, "string": "There are two main components in our dataset: 6 a Twitter dataset and a historical price dataset."}, {"id": 48, "string": "We access Twitter data under the official license of Twitter, then retrieve stock-specific tweets by querying regexes made up of NASDAQ ticker symbols, e.g."}, {"id": 49, "string": "\"\\$GOOG\\b\" for Google Inc.. We preprocess tweet texts using the NLTK package (Bird et al., 2009 ) with the particular Twitter paper, the problem is solved by keeping the notational consistency with our recurrent model and using its time step t to index trading days."}, {"id": 50, "string": "Details will be provided in Section 4."}, {"id": 51, "string": "We use d here to make the formulation easier to follow."}, {"id": 52, "string": "5 https://finance.yahoo.com/industries 6 Our dataset is available at https://github.com/ yumoxu/stocknet-dataset."}, {"id": 53, "string": "mode, including for tokenization and treatment of hyperlinks, hashtags and the \"@\" identifier."}, {"id": 54, "string": "To alleviate sparsity, we further filter samples by ensuring there is at least one tweet for each corpus in the lag."}, {"id": 55, "string": "We extract historical prices for the 88 selected stocks to build the historical price dataset from Yahoo Finance."}, {"id": 56, "string": "7 4 Model Overview Figure 1 : Illustration of the generative process from observed market information to stock movements."}, {"id": 57, "string": "We use solid lines to denote the generation process and dashed lines to denote the variational approximation to the intractable posterior."}, {"id": 58, "string": "We provide an overview of data alignment, model factorization and model components."}, {"id": 59, "string": "As explained in Section 1, we assume that predicting the movement on trading day d can benefit from predicting the movements on its former trading days."}, {"id": 60, "string": "However, due to the general principle of sample independence, building connections directly across samples with temporally-close target dates is problematic for model training."}, {"id": 61, "string": "As an alternative, we notice that within a sample with a target trading day d there are likely to be other trading days than d in its lag that can simulate the prediction targets close to d. Motivated by this observation and multi-task learning (Caruana, 1998) , we make movement predictions not only for d, but also other trading days existing in the lag."}, {"id": 62, "string": "For instance, as shown in Figure 2 , for a sample targeting 07/08/2012 and a 5-day lag, 03/08/2012 and 06/08/2012 are eligible trading days in the lag and we also make predictions for them using the market information in this sample."}, {"id": 63, "string": "The relations between these predictions can thus be captured within the scope of a sample."}, {"id": 64, "string": "As shown in the instance above, not every single date in a lag is an eligible trading day, e.g."}, {"id": 65, "string": "weekends and holidays."}, {"id": 66, "string": "To better organize and use the input, we regard the trading day, instead of the calendar day used in existing research, as the basic unit for building samples."}, {"id": 67, "string": "To this end, we first find all the T eligible trading days referred in a sample, in other words, existing in the time interval [d \u2212 \u2206d + 1, d]."}, {"id": 68, "string": "For clarity, in the scope of one sample, we index these trading days with t \u2208 [1, T ], 8 and each of them maps to an actual (absolute) trading day d t ."}, {"id": 69, "string": "We then propose trading-day alignment: we reorganize our inputs, including the tweet corpora and historical prices, by aligning them to these T trading days."}, {"id": 70, "string": "Specifically, on the tth trading day, we recognize market signals from the corpus M t in [d t\u22121 , d t ) and the historical prices p t on d t\u22121 , for predicting the movement y t on d t ."}, {"id": 71, "string": "We provide an aligned sample for illustration in Figure 2 ."}, {"id": 72, "string": "As a result, every single unit in a sample is a trading day, and we can predict a sequence of movements y = [y 1 , ."}, {"id": 73, "string": "."}, {"id": 74, "string": "."}, {"id": 75, "string": ", y T ]."}, {"id": 76, "string": "The main target is y T while the remainder y * = [y 1 , ."}, {"id": 77, "string": "."}, {"id": 78, "string": "."}, {"id": 79, "string": ", y T \u22121 ] serves as the temporal auxiliary target."}, {"id": 80, "string": "We use these in addition to the main target to improve prediction accuracy (Section 5.3)."}, {"id": 81, "string": "We model the generative process shown in Figure 1."}, {"id": 82, "string": "We encode observed market information as a random variable X = [x 1 ; ."}, {"id": 83, "string": "."}, {"id": 84, "string": "."}, {"id": 85, "string": "; x T ], from which we generate the latent driven factor Z = [z 1 ; ."}, {"id": 86, "string": "."}, {"id": 87, "string": "."}, {"id": 88, "string": "; z T ] for our prediction task."}, {"id": 89, "string": "For the aforementioned multi-task learning purpose, we aim at modeling the conditional probability distribution p \u03b8 (y|X) = Z p \u03b8 (y, Z|X) instead of p \u03b8 (y T |X)."}, {"id": 90, "string": "We write the following factorization for generation, p \u03b8 (y, Z|X) = p \u03b8 (y T |X, Z) p \u03b8 (z T |z <T , X) (2) T \u22121 t=1 p \u03b8 (y t |x \u2264t , z t ) p \u03b8 (z t |z <t , x \u2264t , y t ) where for a given indexed matrix of T vectors [v 1 ; ."}, {"id": 91, "string": "."}, {"id": 92, "string": "."}, {"id": 93, "string": "; v T ], we denote by v <t and v \u2264t the subma- trix [v 1 ; ."}, {"id": 94, "string": "."}, {"id": 95, "string": "."}, {"id": 96, "string": "; v t\u22121 ] and the submatrix [v 1 ; ."}, {"id": 97, "string": "."}, {"id": 98, "string": "."}, {"id": 99, "string": "; v t ], respectively."}, {"id": 100, "string": "Since y * is known in generation, we use the posterior p \u03b8 (z t |z <t , x \u2264t , y t ) , t < T to incorporate market signals more accurately and only use the prior p \u03b8 (z T |z <T , X) when generating z T ."}, {"id": 101, "string": "Besides, when t < T , y t is independent of z <t while our main prediction target, y T is made dependent on z <T through a temporal attention mechanism (Section 5.3)."}, {"id": 102, "string": "We show StockNet modeling the above generative process in Figure 2 ."}, {"id": 103, "string": "In a nutshell, StockNet Figure 2 : The architecture of StockNet."}, {"id": 104, "string": "We use the main target of 07/08/2012 and the lag size of 5 for illustration."}, {"id": 105, "string": "Since 04/08/2012 and 05/08/2012 are not trading days (a weekend), trading-day alignment helps StockNet to organize message corpora and historical prices for the other three trading days in the lag."}, {"id": 106, "string": "We use dashed lines to denote auxiliary components."}, {"id": 107, "string": "Red points denoting temporal objectives are integrated with a temporal attention mechanism to acquire the final training objective."}, {"id": 108, "string": "z 1 z 2 z 3 h 2 h 3 02/08 Input Output h dec h enc \u00b5 log 2 z N (0, I) DKL \u21e5 N (\u00b5, 2 ) k N (0, I) \u21e4 \" comprises three primary components following a bottom-up fashion, 1."}, {"id": 109, "string": "Market Information Encoder (MIE) that encodes tweets and prices to X; 2."}, {"id": 110, "string": "Variational Movement Decoder (VMD) that infers Z with X, y and decodes stock movements y from X, Z; 3."}, {"id": 111, "string": "Attentive Temporal Auxiliary (ATA) that integrates temporal loss through an attention mechanism for model training."}, {"id": 112, "string": "Model Components We detail next the components of our model (MIE, VMD, ATA) and the way we estimate our model parameters."}, {"id": 113, "string": "Market Information Encoder MIE encodes information from social media and stock prices to enhance market information quality, and outputs the market information input X for VMD."}, {"id": 114, "string": "Each temporal input is defined as x t = [c t , p t ] (3) where c t and p t are the corpus embedding and the historical price vector, respectively."}, {"id": 115, "string": "The basic strategy of acquiring c t is to first feed messages into the Message Embedding Layer for their low-dimensional representations, then selectively gather them according to their quality."}, {"id": 116, "string": "To handle the circumstance that multiple stocks are discussed in one single message, in addition to text information, we incorporate the position information of stock symbols mentioned in messages as well."}, {"id": 117, "string": "Specifically, the layer consists of a forward GRU and a backward GRU for the preceding and following contexts of a stock symbol, s, respectively."}, {"id": 118, "string": "Formally, in the message corpus of the tth trading day, we denote the word sequence of the kth message, k \u2208 [1, K], as W where W = s, \u2208 [1, L], and its word embedding matrix as E = [e 1 ; e 2 ; ."}, {"id": 119, "string": "."}, {"id": 120, "string": "."}, {"id": 121, "string": "; e L ]."}, {"id": 122, "string": "We run the two GRUs as follows, \u2212 \u2192 h f = \u2212 \u2212\u2212 \u2192 GRU(e f , \u2212 \u2192 h f \u22121 ) (4) \u2190 \u2212 h b = \u2190 \u2212\u2212 \u2212 GRU(e b , \u2190 \u2212 h b+1 ) (5) m = ( \u2212 \u2192 h + \u2190 \u2212 h )/2 (6) where f \u2208 [1, ."}, {"id": 123, "string": "."}, {"id": 124, "string": "."}, {"id": 125, "string": ", ], b \u2208 [ , ."}, {"id": 126, "string": "."}, {"id": 127, "string": "."}, {"id": 128, "string": ", L]."}, {"id": 129, "string": "The stock symbol is regarded as the last unit in both the preceding and the following contexts where the hidden values, \u2212 \u2192 h l , \u2190 \u2212 h l , are averaged to acquire the message embedding m. Gathering all message embeddings for the tth trading day, we have a mes-sage embedding matrix M t \u2208 R dm\u00d7K ."}, {"id": 130, "string": "In practice, the layer takes as inputs a five-rank tensor for a mini-batch, and yields all M t in the batch with shared parameters."}, {"id": 131, "string": "Tweet quality varies drastically."}, {"id": 132, "string": "Inspired by the news-level attention (Hu et al., 2018) , we weight messages with their respective salience in collective intelligence measurement."}, {"id": 133, "string": "Specifically, we first project M t non-linearly to u t , the normalized attention weight over the corpus, u t = \u03b6(w u tanh(W m,u M t )) (7) where \u03b6(\u00b7) is the softmax function and W m,u \u2208 R dm\u00d7dm , w u \u2208 R dm\u00d71 are model parameters."}, {"id": 134, "string": "Then we compose messages accordingly to acquire the corpus embedding, c t = M t u t ."}, {"id": 135, "string": "(8) Since it is the price change that determines the stock movement rather than the absolute price value, instead of directly feeding the raw price vectorp t = p c t ,p h t ,p l t comprising of the adjusted closing, highest and lowest price on a trading day t, into the networks, we normalize it with its last adjusted closing price, p t =p t /p c t\u22121 \u2212 1."}, {"id": 136, "string": "We then concatenate c t with p t to form the final market information input x t for the decoder."}, {"id": 137, "string": "Variational Movement Decoder The purpose of VMD is to recurrently infer and decode the latent driven factor Z and the movement y from the encoded market information X."}, {"id": 138, "string": "Inference While latent driven factors help to depict the market status leading to stock movements, the posterior inference in the generative model shown in Eq."}, {"id": 139, "string": "(2) is intractable."}, {"id": 140, "string": "Following the spirit of the VAE, we use deep neural networks to fit latent distributions, i.e."}, {"id": 141, "string": "the prior p \u03b8 (z t |z <t , x \u2264t ) and the posterior p \u03b8 (z t |z <t , x \u2264t , y t ), and sidestep the intractability through neural approximation and reparameterization (Kingma and Welling, 2013; Rezende et al., 2014) ."}, {"id": 142, "string": "We first employ a variational approximator q \u03c6 (z t |z <t , x \u2264t , y t ) for the intractable posterior."}, {"id": 143, "string": "We observe the following factorization, q \u03c6 (Z|X, y) = T t=1 q \u03c6 (z t |z <t , x \u2264t , y t ) ."}, {"id": 144, "string": "(9) Neural approximation aims at minimizing the Kullback-Leibler divergence between the q \u03c6 (Z|X, y) and p \u03b8 (Z|X, y)."}, {"id": 145, "string": "Instead of optimizing it directly, we observe that the following equation naturally holds, log p \u03b8 (y|X) (10) =D KL [q \u03c6 (Z|X, y) p \u03b8 (Z|X, y)] +E q \u03c6 (Z|X,y) [log p \u03b8 (y|X, Z)] \u2212D KL [q \u03c6 (Z|X, y) p \u03b8 (Z|X)] where D KL [q p] is the Kullback-Leibler divergence between the distributions q and p. Therefore, we equivalently maximize the following variational recurrent lower bound by plugging Eq."}, {"id": 146, "string": "(2, 9) into Eq."}, {"id": 147, "string": "(10) , L (\u03b8, \u03c6; X, y) (11) = T t=1 E q \u03c6( zt|z<t,x \u2264t ,yt) log p \u03b8 (y t |x \u2264t , z \u2264t ) \u2212 D KL [q \u03c6 (z t |z <t , x \u2264t , y t ) p \u03b8 (z t |z <t , x \u2264t )] \u2264 log p \u03b8 (y|X) where the likelihood term Li et al."}, {"id": 148, "string": "(2017) also provide a lower bound for inferring directly-connected recurrent latent variables in text summarization."}, {"id": 149, "string": "In their work, priors are modeled with p \u03b8 (z t ) \u223c N (0, I), which, in fact, turns the KL term into a static regularization term encouraging sparsity."}, {"id": 150, "string": "In Eq."}, {"id": 151, "string": "(11), we provide a more theoretically rigorous lower bound where the KL term with p \u03b8 (z t |z <t , x \u2264t ) plays a dynamic role in inferring dependent latent variables for every different model input and latent history."}, {"id": 152, "string": "p \u03b8 (y t |x \u2264t , z \u2264t ) = p \u03b8 (y t |x \u2264t , z t ) , if t < T p \u03b8 (y T |X, Z) , if t = T. (12) Decoding As per time series, VMD adopts an RNN with a GRU cell to extract features and decode stock signals recurrently, h s t = GRU(x t , h s t\u22121 )."}, {"id": 153, "string": "(13) We let the approximator q \u03c6 (z t |z <t , x \u2264t , y t ) subject to a standard multivariate Gaussian distribution N (\u00b5, \u03b4 2 I)."}, {"id": 154, "string": "We calculate \u00b5 and \u03b4 as \u00b5 t = W \u03c6 z,\u00b5 h z t + b \u03c6 \u00b5 (14) log \u03b4 2 t = W \u03c6 z,\u03b4 h z t + b \u03c6 \u03b4 (15) and the shared hidden representation h z t as h z t = tanh(W \u03c6 z [z t\u22121 , x t , h s t , y t ] + b \u03c6 z ) (16) where W \u03c6 z,\u00b5 , W \u03c6 z,\u03b4 , W \u03c6 z are weight matrices and b \u03c6 \u00b5 , b \u03c6 \u03b4 , b \u03c6 z are biases."}, {"id": 155, "string": "Since Gaussian distribution belongs to the \"location-scale\" distribution family, we can further reparameterize z t as z t = \u00b5 t + \u03b4 t (17) where denotes an element-wise product."}, {"id": 156, "string": "The noise term \u223c N (0, I) naturally involves stochastic signals in our model."}, {"id": 157, "string": "Similarly, We let the prior p \u03b8 (z t |z <t , x \u2264t ) \u223c N (\u00b5 , \u03b4 2 I)."}, {"id": 158, "string": "Its calculation is the same as that of the posterior except the absence of y t and independent model parameters, \u00b5 t = W \u03b8 o,\u00b5 h z t + b \u03b8 \u00b5 (18) log \u03b4 2 t = W \u03b8 o,\u03b4 h z t + b \u03b8 \u03b4 (19) where h z t = tanh(W \u03b8 z [z t\u22121 , x t , h s t ] + b \u03b8 z )."}, {"id": 159, "string": "(20) Following Zhang et al."}, {"id": 160, "string": "(2016) , differently from the posterior, we set the prior z t = \u00b5 t during decoding."}, {"id": 161, "string": "Finally, we integrate deterministic features and the final prediction hypothesis is given as g t = tanh(W g [x t , h s t , z t ] + b g ) (21) y t = \u03b6(W y g t + b y ), t < T (22) where W g , W y are weight matrices and b g , b y are biases."}, {"id": 162, "string": "The softmax function \u03b6(\u00b7) outputs the confidence distribution over up and down."}, {"id": 163, "string": "As introduced in Section 4, the decoding of the main target y T depends on z <T and thus lies at the interface between VMD and ATA."}, {"id": 164, "string": "We will elaborate on it in the next section."}, {"id": 165, "string": "Attentive Temporal Auxiliary With the acquisition of a sequence of auxiliary predictions\u1ef8 * = [\u1ef9 1 ; ."}, {"id": 166, "string": "."}, {"id": 167, "string": "."}, {"id": 168, "string": ";\u1ef9 T \u22121 ], we incorporate two-folded auxiliary effects into the main prediction and the training objective flexibly by first introducing a shared temporal attention mechanism."}, {"id": 169, "string": "Since each hypothesis of a temporal auxiliary contributes unequally to the main prediction and model training, as shown in Figure 3 , temporal attention calculates their weights in these two contributions by employing two scoring components: an information score and a dependency score."}, {"id": 170, "string": "Specifically, v i = w i tanh(W g,i G * ) (23) v d = g T tanh(W g,d G * ) (24) v * = \u03b6(v i v d ) (25) where W g,i , W g,d \u2208 R dg\u00d7dg , w i \u2208 R dg\u00d71 are model parameters."}, {"id": 171, "string": "The integrated representations G * = [g 1 ; ."}, {"id": 172, "string": "."}, {"id": 173, "string": "."}, {"id": 174, "string": "; g T \u22121 ] and g T are reused as the final representations of temporal market information."}, {"id": 175, "string": "The information score v i evaluates historical trading days as per their own information quality, while the dependency score v d captures their dependencies with our main target."}, {"id": 176, "string": "We integrate the two and acquire the final normalized attention weight v * \u2208 R 1\u00d7(T \u22121) by feeding their elementwise product into the softmax function."}, {"id": 177, "string": "As a result, the main prediction can benefit from temporally-close hypotheses have been made and we decode our main hypothesis\u1ef9 T as y T = \u03b6(W T [\u1ef8 * v * , g T ] + b T ) (26) where W T is a weight matrix and b T is a bias."}, {"id": 178, "string": "As to the model objective, we use the Monte Carlo method to approximate the expectation term in Eq."}, {"id": 179, "string": "(11) and typically only one sample is used for gradient computation."}, {"id": 180, "string": "To incorporate varied temporal importance at the objective level, we first break down the approximated L into a series of temporal objectives f \u2208 R T \u00d71 where f t comprises a likelihood term and a KL term for a trading day t, f t = log p \u03b8 (y t |x \u2264t , z \u2264t ) (27) \u2212 \u03bbD KL [q \u03c6 (z t |z <t , x \u2264t , y t ) p \u03b8 (z t |z <t , x \u2264t )] where we adopt the KL term annealing trick (Bowman et al., 2016; Semeniuta et al., 2017) and add a linearly-increasing KL term weight \u03bb \u2208 (0, 1] to gradually release the KL regularization effect in the training procedure."}, {"id": 181, "string": "Then we reuse v * to build the final temporal weight vector v \u2208 R 1\u00d7T , v = [\u03b1v * , 1] (28) where 1 is for the main prediction and we adopt the auxiliary weight \u03b1 \u2208 [0, 1] to control the overall auxiliary effects on the model training."}, {"id": 182, "string": "\u03b1 is tuned on the development set and its effects will be discussed at length in Section 6.5."}, {"id": 183, "string": "Finally, we write the training objective F by recomposition, F (\u03b8, \u03c6; X, y) = 1 N N n v (n) f (n) (29) where our model can learn to generalize with the selective attendance of temporal auxiliary."}, {"id": 184, "string": "We take the derivative of F with respect to all the model parameters {\u03b8, \u03c6} through backpropagation for the update."}, {"id": 185, "string": "Experiments In this section, we detail our experimental setup and results."}, {"id": 186, "string": "Training Setup We use a 5-day lag window for sample construction and 32 shuffled samples in a batch."}, {"id": 187, "string": "9 The maximal token number contained in a message and the maximal message number on a trading day are empirically set to 30 and 40, respectively, with the excess clipped."}, {"id": 188, "string": "Since all tweets in the batched samples are simultaneously fed into the model, we set the word embedding size to 50 instead of larger sizes to control memory costs and make model training feasible on one single GPU (11GB memory)."}, {"id": 189, "string": "We set the hidden size of Message Embedding Layer to 100 and that of VMD to 150."}, {"id": 190, "string": "All weight matrices in the model are initialized with the fan-in trick and biases are initialized with zero."}, {"id": 191, "string": "We train the model with an Adam optimizer (Kingma and Ba, 2014) with the initial learning rate of 0.001."}, {"id": 192, "string": "Following Bowman et al."}, {"id": 193, "string": "(2016), we use the input dropout rate of 0.3 to regularize latent variables."}, {"id": 194, "string": "Tensorflow (Abadi et al., 2016) is used to construct the computational graph of StockNet and hyper-parameters are tweaked on the development set."}, {"id": 195, "string": "Evaluation Metrics Following previous work for stock prediction (Xie et al., 2013; Ding et al., 2015) , we adopt the standard measure of accuracy and Matthews Correlation Coefficient (MCC) as evaluation metrics."}, {"id": 196, "string": "MCC avoids bias due to data skew."}, {"id": 197, "string": "Given the confusion matrix tp fn fp tn containing the number of samples classified as true positive, false positive, true negative and false negative, MCC is calculated as MCC = tp \u00d7 tn \u2212 fp \u00d7 fn (tp + fp)(tp + fn)(tn + fp)(tn + fn) ."}, {"id": 198, "string": "(30) Baselines and Proposed Models We construct the following five baselines in different genres, 10 \u2022 RAND: a naive predictor making random guess in up or down."}, {"id": 199, "string": "\u2022 ARIMA: Autoregressive Integrated Moving Average, an advanced technical analysis method using only price signals (Brown, 2004) ."}, {"id": 200, "string": "\u2022 RANDFOREST: a discriminative Random Forest classifier using Word2vec text representations (Pagolu et al., 2016) ."}, {"id": 201, "string": "\u2022 TSLDA: a generative topic model jointly learning topics and sentiments (Nguyen and Shirai, 2015) ."}, {"id": 202, "string": "\u2022 HAN: a state-of-the-art discriminative deep neural network with hierarchical attention (Hu et al., 2018) ."}, {"id": 203, "string": "To make a detailed analysis of all the primary components in StockNet, in addition to HEDGE-FUNDANALYST, the fully-equipped StockNet, we also construct the following four variations, \u2022 TECHNICALANALYST: the generative StockNet using only historical prices."}, {"id": 204, "string": "(Brown, 2004) 51.39 -0.020588 FUNDAMENTALANALYST 58.23 0.071704 RANDFOREST (Pagolu et al., 2016) 53.08 0.012929 INDEPENDENTANALYST 57.54 0.036610 TSLDA (Nguyen and Shirai, 2015) 54.07 0.065382 DISCRIMINATIVEANALYST 56.15 0.056493 HAN (Hu et al., 2018) 57.64 0.051800 HEDGEFUNDANALYST 58.23 0.080796  \u2022 DISCRIMINATIVEANALYST: the discriminative StockNet directly optimizing the likelihood objective."}, {"id": 205, "string": "Following Zhang et al."}, {"id": 206, "string": "(2016) , we set z t = \u00b5 t to take out the effects of the KL term."}, {"id": 207, "string": "Results Since stock prediction is a challenging task and a minor improvement usually leads to large potential profits, the accuracy of 56% is generally reported as a satisfying result for binary stock movement prediction (Nguyen and Shirai, 2015) ."}, {"id": 208, "string": "We show the performance of the baselines and our proposed models in Table 1 ."}, {"id": 209, "string": "TLSDA is the best baseline in MCC while HAN is the best baseline in accuracy."}, {"id": 210, "string": "Our model, HEDGEFUNDAN-ALYST achieves the best performance of 58.23 in accuracy and 0.080796 in MCC, outperforming TLSDA and HAN with 4.16, 0.59 in accuracy, and 0.015414, 0.028996 in MCC, respectively."}, {"id": 211, "string": "Though slightly better than random guess, classic technical analysis, e.g."}, {"id": 212, "string": "ARIMA, does not yield satisfying results."}, {"id": 213, "string": "Similar in using only historical prices, TECHNICALANALYST shows an obvious advantage in this task compared ARIMA."}, {"id": 214, "string": "We believe there are two major reasons: (1) TECHNICAL-ANALYST learns from training data and incorporates more flexible non-linearity; (2) our test set contains a large number of stocks while ARIMA is more sensitive to peculiar sequence stationarity."}, {"id": 215, "string": "It is worth noting that FUNDAMENTALANA-LYST gains exceptionally competitive results with only 0.009092 less in MCC than HEDGEFUNDAN-ALYST."}, {"id": 216, "string": "The performance of FUNDAMENTALANALYST and TECHNICALANALYST confirm the positive effects from tweets and historical prices in stock movement prediction, respectively."}, {"id": 217, "string": "As an effective ensemble of the two market information, HEDGE-FUNDANALYST gains even better performance."}, {"id": 218, "string": "Compared with DISCRIMINATIVEANALYST, the performance improvements of HEDGEFUNDANA-LYST are not from enlarging the networks, demonstrating that modeling underlying market status explicitly with latent driven factors indeed benefits stock movement prediction."}, {"id": 219, "string": "The comparison with INDEPENDENTANALYST also shows the effectiveness of capturing temporal dependencies between predictions with the temporal auxiliary."}, {"id": 220, "string": "However, the effects of the temporal auxiliary are more complex and will be analyzed further in the next section."}, {"id": 221, "string": "Effects of Temporal Auxiliary We provide a detailed discuss of how the temporal auxiliary affects model performance."}, {"id": 222, "string": "As introduced in Eq."}, {"id": 223, "string": "(28), the temporal auxiliary weight \u03b1 controls the overall effects of the objective-level temporal auxiliary to our model."}, {"id": 224, "string": "Figure 4 presents how the performance of HEDGEFUNDANALYST and DISCRIMINATIVEANALYST fluctuates with \u03b1."}, {"id": 225, "string": "As shown in Figure 4 , enhanced by the temporal auxiliary, HEDGEFUNDANALYST approaches the best performance at 0.5, and DISCRIMINATIVEANALYST achieves its maximum at 0.7."}, {"id": 226, "string": "In fact, objectivelevel auxiliary can be regarded as a denoising regularizer: for a sample with a specific movement as the main target, the market source in the lag can be heterogeneous, e.g."}, {"id": 227, "string": "affected by bad news, tweets on earlier days are negative but turn to positive due to timely crises management."}, {"id": 228, "string": "Without temporal auxiliary tasks, the model tries to identify positive signals on earlier days only for the main target of rise movement, which is likely to result in pure noise."}, {"id": 229, "string": "In such cases, temporal auxiliary tasks help to filter market sources in the lag as per their respective aligned auxiliary movements."}, {"id": 230, "string": "Besides, from the perspective of training variational models, the temporal auxiliary helps HEDGEFUNDANALYST to encode more useful information into the latent driven factor Z, which is consistent with recent research in VAEs (Semeniuta et al., 2017) ."}, {"id": 231, "string": "Compared with HEDGEFUND-ANALYST that contains a KL term performing dynamic regularization, DISCRIMINATIVEANALYST requires stronger regularization effects coming with a bigger \u03b1 to achieve its best performance."}, {"id": 232, "string": "Since y * also involves in generating y T through the temporal attention, tweaking \u03b1 acts as a tradeoff between focusing on the main target and generalizing by denoising."}, {"id": 233, "string": "Therefore, as shown in Figure 4 , our models do not linearly benefit from incorporating temporal auxiliary."}, {"id": 234, "string": "In fact, the two models follow a similar pattern in terms of performance change: the curves first drop down with the increase of \u03b1, except the MCC curve for DIS-CRIMINATIVEANALYST rising up temporarily at 0.3."}, {"id": 235, "string": "After that, the curves ascend abruptly to their maximums, then keep descending till \u03b1 = 1."}, {"id": 236, "string": "Though the start phase of increasing \u03b1 even leads to worse performance, when auxiliary effects are properly introduced, the two models finally gain better results than those with no involvement of auxiliary effects, e.g."}, {"id": 237, "string": "INDEPENDENTANALYST."}, {"id": 238, "string": "Conclusion We demonstrated the effectiveness of deep generative approaches for stock movement prediction from social media data by introducing StockNet, a neural network architecture for this task."}, {"id": 239, "string": "We tested our model on a new comprehensive dataset and showed it performs better than strong baselines, including implementation of previous work."}, {"id": 240, "string": "Our comprehensive dataset is publicly available at https://github.com/ yumoxu/stocknet-dataset."}], "headers": [{"section": "Introduction", "n": "1", "start": 0, "end": 30}, {"section": "Problem Formulation", "n": "2", "start": 31, "end": 38}, {"section": "Data Collection", "n": "3", "start": 39, "end": 109}, {"section": "Model Components", "n": "5", "start": 110, "end": 112}, {"section": "Market Information Encoder", "n": "5.1", "start": 113, "end": 136}, {"section": "Variational Movement Decoder", "n": "5.2", "start": 137, "end": 164}, {"section": "Attentive Temporal Auxiliary", "n": "5.3", "start": 165, "end": 183}, {"section": "Experiments", "n": "6", "start": 184, "end": 185}, {"section": "Training Setup", "n": "6.1", "start": 186, "end": 194}, {"section": "Evaluation Metrics", "n": "6.2", "start": 195, "end": 197}, {"section": "Baselines and Proposed Models", "n": "6.3", "start": 198, "end": 206}, {"section": "Results", "n": "6.4", "start": 207, "end": 220}, {"section": "Effects of Temporal Auxiliary", "n": "6.5", "start": 221, "end": 236}, {"section": "Conclusion", "n": "7", "start": 237, "end": 240}], "figures": [{"filename": "../figure/image/1300-Figure1-1.png", "caption": "Figure 1: Illustration of the generative process from observed market information to stock movements. We use solid lines to denote the generation process and dashed lines to denote the variational approximation to the intractable posterior.", "page": 2, "bbox": {"x1": 148.79999999999998, "x2": 219.84, "y1": 204.48, "y2": 279.36}}, {"filename": "../figure/image/1300-Figure3-1.png", "caption": "Figure 3: The temporal attention in our model. Squares are the non-linear projections of gt and points are scores or normalized weights.", "page": 5, "bbox": {"x1": 324.96, "x2": 509.28, "y1": 68.16, "y2": 205.44}}, {"filename": "../figure/image/1300-Table1-1.png", "caption": "Table 1: Performance of baselines and StockNet variations in accuracy and MCC.", "page": 7, "bbox": {"x1": 72.0, "x2": 531.36, "y1": 62.879999999999995, "y2": 145.92}}, {"filename": "../figure/image/1300-Figure4-1.png", "caption": "Figure 4: (a) Performance of HEDGEFUNDANALYST with varied \u03b1, see Eq. (28). (b) Performance of DISCRIMINATIVEANALYST with varied \u03b1.", "page": 7, "bbox": {"x1": 74.39999999999999, "x2": 521.76, "y1": 180.48, "y2": 287.03999999999996}}, {"filename": "../figure/image/1300-Figure2-1.png", "caption": "Figure 2: The architecture of StockNet. We use the main target of 07/08/2012 and the lag size of 5 for illustration. Since 04/08/2012 and 05/08/2012 are not trading days (a weekend), trading-day alignment helps StockNet to organize message corpora and historical prices for the other three trading days in the lag. We use dashed lines to denote auxiliary components. Red points denoting temporal objectives are integrated with a temporal attention mechanism to acquire the final training objective.", "page": 3, "bbox": {"x1": 77.75999999999999, "x2": 521.28, "y1": 66.72, "y2": 271.68}}]}