{"title": "NAVER Machine Translation System for WAT 2015", "abstract": "In this paper, we describe NAVER machine translation system for English to Japanese and Korean to Japanese tasks at WAT 2015. We combine the traditional SMT and neural MT in both tasks.", "text": [{"id": 0, "string": "Introduction This paper explains the NAVER machine translation system for the 2nd Workshop on Asian Translation (WAT 2015) (Nakazawa et al., 2015) ."}, {"id": 1, "string": "We participate in two tasks; English to Japanese (En-Ja) and Korean to Japanese (Ko-Ja)."}, {"id": 2, "string": "Our system is a combined system of traditional statistical machine translation (SMT) and neural machine translation (NMT)."}, {"id": 3, "string": "We adopt the tree-tostring syntax-based model as En-Ja SMT baseline, while we adopt the phrase-based model as Ko-Ja."}, {"id": 4, "string": "We propose improved SMT systems for each task and an NMT model based on the architecture using recurrent neural network (RNN) (Cho et al., 2014; Sutskever et al., 2014) ."}, {"id": 5, "string": "We give detailed explanations of each SMT system in section 2 and section 3."}, {"id": 6, "string": "We describe our NMT model in section 4."}, {"id": 7, "string": "2 English to Japanese Training data We used 1 million sentence pairs that are contained in train-1.txt of ASPEC-JE corpus for training the translation rule tables and NMT models."}, {"id": 8, "string": "We also used 3 million Japanese sentences that are contained in train-1.txt, train-2.txt,train-3.txt of ASPEC-JE corpus for training the 5-gram language model."}, {"id": 9, "string": "We also used 1,790 sentence pairs of dev.txt for tuning the weights of each feature of SMT linear model and as validation data of neural network."}, {"id": 10, "string": "We filtered out the sentences that have 100 or more tokens from training data."}, {"id": 11, "string": "Language Analyzer We used Moses tokenizer and Berkeley constituency parser 1 for tokenizing and parsing an English sentence."}, {"id": 12, "string": "We used our own Japanese tokenizer and part-of-speech tagger for tokenizing and tagging a Japanese sentence."}, {"id": 13, "string": "After running the tokenizer and the tagger, we make a token from concatenation of a word and its part-of-speech."}, {"id": 14, "string": "Tree-to-string Syntax-based SMT To determining the baseline model, we first performed comparative experiments with the phrasebased, hierarchical phrase-based and syntax-based models."}, {"id": 15, "string": "As a result, we chose the tree-to-string syntax-based model."}, {"id": 16, "string": "The SMT models that consider source syntax such as tree-to-string and forest-to-string brought out better performance than the phrase-based and hierarchical phrase-based models in the WAT 2014 En-Ja task."}, {"id": 17, "string": "The tree-to-string model was proposed by Huang (2006) and Liu (2006) ."}, {"id": 18, "string": "It utilizes the constituency tree of source language to extract translation rules and decode a target sentence."}, {"id": 19, "string": "The translation rules are extracted from a source-parsed and word-aligned corpus in the training step."}, {"id": 20, "string": "We use synchronous context free grammar (SCFG) rules."}, {"id": 21, "string": "In addition, we used a rule augmentation method which is known as syntax-augmented machine translation (Zollmann and Venugopal, 2006) ."}, {"id": 22, "string": "Because the tree-to-string SMT makes some constraints on extracting rules by considering syntactic tree structures, it usually extracts fewer rules than hierarchical phrase-based SMT (HPBSMT) (Chiang, 2005) ."}, {"id": 23, "string": "Thus it is required to augment tree-to-string translation rules."}, {"id": 24, "string": "The rule augmentation method allows the training system to extract more rules by modifying parse trees."}, {"id": 25, "string": "Given a parse tree, we produce additional nodes by combining any pairs of neighboring nodes, not only children nodes, e.g."}, {"id": 26, "string": "NP+VP."}, {"id": 27, "string": "We limit the maximum span of each rule to 40 tokens in the rule extraction process."}, {"id": 28, "string": "The tree-to-string decoder use a chart parsing algorithm with cube pruning proposed by Chiang (2005) ."}, {"id": 29, "string": "Our En-Ja SMT system was developed by using the open source SMT engines; Moses and Giza++."}, {"id": 30, "string": "Its other specifications are as follows: \u2022 Grow-diag-final-and word alignment heuristic \u2022 Good-Turing discounting for smoothing probabilities \u2022 Minimum Error Rate Training (MERT) for tuning feature weights \u2022 Cube-pruning-pop-limit = 3000 Handling Out-of-Vocabulary In order to handle out-of-vocabulary (OOV) words, we use two techniques; hyphen word split and spell error correction."}, {"id": 31, "string": "The former is to split a word with hyphen (-) to two separate tokens before running the language analyzer."}, {"id": 32, "string": "The latter is to automatically detect and correct spell errors in an input sentence."}, {"id": 33, "string": "We give a detailed description of spell error correction in section 2.4.1."}, {"id": 34, "string": "English Spell Correction It is not easy to translate a word including errata, because the erroneous word has only a slim chance of occurrence in the training data."}, {"id": 35, "string": "We discovered a lot of spell errors among OOV words that appear in English scientific text."}, {"id": 36, "string": "We introduce English spell correction for reducing OOV words in input sentences."}, {"id": 37, "string": "We developed our spell corrector by using Aspell 2 ."}, {"id": 38, "string": "For detecting a spell error, we skip words that have only capitals, numbers or symbols, because they are likely to be abbreviations or mathematic expressions."}, {"id": 39, "string": "Then we regard words detected by Aspell as spell error words."}, {"id": 40, "string": "For correcting spell error, we use only top-3 suggestion words from Aspell."}, {"id": 41, "string": "We find that a large gap between an original word and its suggestion word makes wrong correction."}, {"id": 42, "string": "To avoid excessive correction, we introduce a gap thresholding technique, that ignores the suggestion word that has 3 or longer edit distance and selects one that has 3 Korean to Japanese Training data We used 1 million sentence pairs that are contained in JPO corpus for training phrase tables and NMT models."}, {"id": 43, "string": "We also used Japanese part of the corpus for training the 5-gram language model."}, {"id": 44, "string": "We also used 2,000 sentence pairs of dev.txt for tuning the weights of each feature of SMT linear model and as validation data of neural network."}, {"id": 45, "string": "We did not filter out any sentences."}, {"id": 46, "string": "Language Analyzer We used MeCab-ko 3 for tokenizing a Korean sentence."}, {"id": 47, "string": "We used Juman 4 for tokenizing a Japanese sentence."}, {"id": 48, "string": "We did not perform part-of-speech tagging for both languages."}, {"id": 49, "string": "Phrase-based SMT As in the En-Ja task, we first performed comparative experiments with the phrase-based and hierarchical phrase-based models, and then adopt the phrase-based model as our baseline model."}, {"id": 50, "string": "For the Ko-Ja task, we develop two phrasebased systems; word-level and character-level."}, {"id": 51, "string": "We use word-level tokenization for the word-based system."}, {"id": 52, "string": "We found that setting the distortion limit to zero yields better translation in aspect of both BLEU and human evaluation."}, {"id": 53, "string": "We use the 5-gram language model."}, {"id": 54, "string": "We use character-level tokenization for character-based system."}, {"id": 55, "string": "We use the 10gram language model and set the maximum phrase length to 10 in the phrase pair extraction process."}, {"id": 56, "string": "We found that the character-level system does not suffer from tokenization error and out-ofvocabulary issue."}, {"id": 57, "string": "The JPO corpus contains many technical terms and loanwords like chemical compound names, which are more inaccurately tokenized and allow a lot of out-of-vocabulary tokens to be generated."}, {"id": 58, "string": "Since Korean and Japanese share similar transliteration rules for loanwords, the character-level system can learn translation of unseen technical words."}, {"id": 59, "string": "It generally produces better translations than a table-based transliteration."}, {"id": 60, "string": "Moreover, we tested jamo-level tokenization 5 for Korean text, however, the preliminary test did not produce effective results."}, {"id": 61, "string": "We also investigated a parentheses imbalance problem."}, {"id": 62, "string": "We solved the problem by filtering out parentheses-imbalanced translations from the nbest results."}, {"id": 63, "string": "We found that the post-processing step can improve the BLEU score with low order language models, but cannot do with high order language models."}, {"id": 64, "string": "We do not use the step for final submission."}, {"id": 65, "string": "To boosting the performance, we combine the word-level phrase-based model (Word PB) and the character-level phrase-based model (Char PB)."}, {"id": 66, "string": "If there are one or more OOV words in an input sentence, our translator choose the Char PB model, otherwise, the Word PB model."}, {"id": 67, "string": "Our Ko-Ja SMT system was developed by using the open source SMT engines; Moses and Giza++."}, {"id": 68, "string": "Its other specifications are as follows: \u2022 Grow-diag-final-and word alignment heuristic \u2022 Good-Turing discounting for smoothing probabilities \u2022 Minimum Error Rate Training (MERT) for tuning feature weights Neural Machine Translation Neural machine translation (NMT) is a new approach to machine translation that has shown promising results compared to the existing approaches such as phrase-based statistical machine translation (Cho et al., 2014; Sutskever et al., 2014; Bahdanau et al., 2015 )."}, {"id": 69, "string": "An NMT system is a single neural network that reads a source sentence and generates its translation."}, {"id": 70, "string": "Using the bilingual corpus, the whole neural network is jointly trained to maximize the conditional probability of a correct translation given a source sentence."}, {"id": 71, "string": "NMT has several advantages over the existing statistical machine translation systems such as the phrase-based system."}, {"id": 72, "string": "First, NMT uses minimal domain knowledge."}, {"id": 73, "string": "Second, the NMT system is jointly trained to maximize the translation performance, unlike the existing phrase-based system which consists of many separately trained features."}, {"id": 74, "string": "Third, the NMT system removes the need to store explicit phrase tables and language models."}, {"id": 75, "string": "Lastly, the decoder of an NMT system is easy to implement."}, {"id": 76, "string": "Despite these advantages and promising results, NMT has a limitation in handling a larger target vocabulary, as the complexity of training and decoding increases proportionally to the number of target words."}, {"id": 77, "string": "In this paper, we propose a new approach to avoid the large target vocabulary problem by preprocessing the target word sequences, encoding them as a longer character sequence drawn from a small character vocabulary."}, {"id": 78, "string": "The proposed approach removes the need to replace rare words with the unknown word symbol."}, {"id": 79, "string": "Our approach is simpler than other methods recently proposed to address the same issue (Luong et al., 2015; Jean et al., 2015) ."}, {"id": 80, "string": "Model In this paper, we use our in-house software of NMT that uses an attention mechanism, as recently proposed by Bahdanau et al."}, {"id": 81, "string": "(2015) ."}, {"id": 82, "string": "The encoder of NMT is a bi-directional recurrent neural network such that h t = [ h t ; h t ] (1) h t = f GRU (W s we x t , h t+1 ) (2) h t = f GRU (W s we x t , h t\u22121 ) (3) where h t is a hidden state of the encoder, x t is a one-hot encoded vector indicating one of the words in the source vocabulary, W s we is a weight matrix for the word embedding of the source language, and f GRU is a gated recurrent unit (GRU) (Cho et al., 2014) ."}, {"id": 83, "string": "At each time, the decoder of NMT computes the context vector c t as a convex combination of the hidden states (h 1 ,."}, {"id": 84, "string": "."}, {"id": 85, "string": "."}, {"id": 86, "string": ",h T ) with the alignment weights \u03b1 1 ,."}, {"id": 87, "string": "."}, {"id": 88, "string": "."}, {"id": 89, "string": ",\u03b1 T : c t = T i=1 \u03b1 ti h i (4) \u03b1 ti = exp(e tj ) T j=1 exp(e tj ) (5) e ti = f F F N N (z t\u22121 , h i , y t\u22121 ) (6) where f F F N N is a feedforward neural network with a single hidden layer, z t\u22121 is a previous hidden state of the decoder, and y t\u22121 is a previous generated target word (one-hot encoded vector)."}, {"id": 90, "string": "A new hidden state z t of the decoder which uses GRU is computed based on z t\u22121 , y t\u22121 , and c t : z t = f GRU (y t\u22121 , z t\u22121 , c t ) (7) The probability of the next target word y t is then computed by (8) p(y t |y <t , x) = y T t f sof tmax {W z y z t + W zy z t + W cy c t + W yy (W t we y t\u22121 ) + b y } z t = f ReLU (W zz z t ) (9) where f sof tmax is a softmax function, f ReLU is a rectified linear unit (ReLU), W t we is a weight matrix for the word embedding of the target language, and b y is a target word bias."}, {"id": 91, "string": "Settings We constructed the source word vocabulary with the most common words in the source language corpora."}, {"id": 92, "string": "For the target character vocabulary, we used a BI (begin/inside) representation (e.g., \u7d50/B, \u679c/I), because it gave better accuracy in preliminary experiment."}, {"id": 93, "string": "The sizes of the source vocabularies for English and Korean were 245K and 60K, respectively, for the En-Ja and Ko-Ja tasks."}, {"id": 94, "string": "The sizes of the target character vocabularies for Japanese were 6K and 5K, respectively, for the En-Ja and Ko-Ja tasks."}, {"id": 95, "string": "We chose the dimensionality of the source word embedding and the target character embedding to be 200, and chose the size of the recurrent units to be 1,000."}, {"id": 96, "string": "Each model was optimized using stochastic gradient descent (SGD)."}, {"id": 97, "string": "We did not use dropout."}, {"id": 98, "string": "Training was early-stopped to maximize the performance on the development set measured by BLEU."}, {"id": 99, "string": "Experimental Results All scores of this section are reported in experiments on the official test data; test.txt of the ASPEC-JE corpus."}, {"id": 100, "string": "Table 1 shows the evaluation results of our En-Ja traditional SMT system."}, {"id": 101, "string": "The first row in the table indicates the baseline of the tree-to-string systaxbased model."}, {"id": 102, "string": "The second row shows the system that reflects the tree modification described in section 2.3."}, {"id": 103, "string": "The augmentation method drastically increased both the number of rules and the BLEU score."}, {"id": 104, "string": "Our OOV handling methods described in  The decoding time of the rule-augmented treeto-string SMT is about 1.3 seconds per a sentence in our 12-core machine."}, {"id": 105, "string": "Even though it is not a terrible problem, we are required to improve the decoding speed by pruning the rule table or using the incremental decoding method (Huang and Mi, 2010) ."}, {"id": 106, "string": "Table 2 shows the evaluation results of our Ko-Ja traditional SMT system."}, {"id": 107, "string": "We obtained the best result in the combination of two phrase-based SMT systems."}, {"id": 108, "string": "Table 3 shows effects of our NMT model."}, {"id": 109, "string": "\"Human\" indicates the pairwise crowdsourcing evaluation scores provided by WAT 2015 organizers."}, {"id": 110, "string": "In the table, \"T2S/PBMT only\" is the final T2S/PBMT systems shown in section 5.1 and section 5.2."}, {"id": 111, "string": "\"NMT only\" is the system using only RNN encoder-decoder without any traditional SMT methods."}, {"id": 112, "string": "The last row is the combined system that reranks T2S/PBMT n-best translations by NMT."}, {"id": 113, "string": "Our T2S/PBMT system outputs 100,000-best translations in En-Ja and 10,000best translations in Ko-Ja."}, {"id": 114, "string": "The final output is 1best translation selected by considering only NMT score."}, {"id": 115, "string": "En-Ja SMT Ko-Ja SMT NMT NMT outperforms the traditional SMT in En-Ja, while it does not in Ko-Ja."}, {"id": 116, "string": "This result means that NMT produces a strong effect in the language pair with long linguistic distance."}, {"id": 117, "string": "Moreover, the reranking system achieved a great synergy of T2S/PBMT and NMT in both task, even  if \"NMT only\" is not effective in Ko-Ja."}, {"id": 118, "string": "From the human evaluation, we can be clear that our NMT model produces successful results."}, {"id": 119, "string": "Conclusion This paper described NAVER machine translation system for En-Ja and Ko-Ja tasks at WAT 2015."}, {"id": 120, "string": "We developed both the traditional SMT and NMT systems and integrated NMT into the traditional SMT in both tasks by reranking n-best translations of the traditional SMT."}, {"id": 121, "string": "Our evaluation results showed that a combination of the NMT and traditional SMT systems outperformed two independent systems."}, {"id": 122, "string": "For the future work, we try to improve the space and time efficiency of both the tree-to-string SMT and the NMT model."}, {"id": 123, "string": "We also plan to develop and evaluate the NMT system in other language pairs."}], "headers": [{"section": "Introduction", "n": "1", "start": 0, "end": 6}, {"section": "Training data", "n": "2.1", "start": 7, "end": 10}, {"section": "Language Analyzer", "n": "2.2", "start": 11, "end": 13}, {"section": "Tree-to-string Syntax-based SMT", "n": "2.3", "start": 14, "end": 30}, {"section": "Handling Out-of-Vocabulary", "n": "2.4", "start": 31, "end": 33}, {"section": "English Spell Correction", "n": "2.4.1", "start": 34, "end": 41}, {"section": "Training data", "n": "3.1", "start": 42, "end": 45}, {"section": "Language Analyzer", "n": "3.2", "start": 46, "end": 48}, {"section": "Phrase-based SMT", "n": "3.3", "start": 49, "end": 67}, {"section": "Neural Machine Translation", "n": "4", "start": 68, "end": 79}, {"section": "Model", "n": "4.1", "start": 80, "end": 90}, {"section": "Settings", "n": "4.2", "start": 91, "end": 98}, {"section": "Experimental Results", "n": "5", "start": 99, "end": 114}, {"section": "NMT", "n": "5.3", "start": 115, "end": 118}, {"section": "Conclusion", "n": "6", "start": 119, "end": 123}], "figures": [{"filename": "../figure/image/1220-Table3-1.png", "caption": "Table 3: Effect of NMT.", "page": 4, "bbox": {"x1": 137.76, "x2": 460.32, "y1": 62.879999999999995, "y2": 132.0}}, {"filename": "../figure/image/1220-Table2-1.png", "caption": "Table 2: Ko-Ja SMT.", "page": 3, "bbox": {"x1": 310.56, "x2": 522.24, "y1": 166.56, "y2": 223.2}}, {"filename": "../figure/image/1220-Table1-1.png", "caption": "Table 1: En-Ja SMT.", "page": 3, "bbox": {"x1": 312.0, "x2": 521.28, "y1": 62.879999999999995, "y2": 132.0}}]}