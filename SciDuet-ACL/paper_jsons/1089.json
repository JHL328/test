{"title": "Categorizing and Inferring the Relationship between the Text and Image of Twitter Posts", "abstract": "Text in social media posts is frequently accompanied by images in order to provide content, supply context, or to express feelings. This paper studies how the meaning of the entire tweet is composed through the relationship between its textual content and its image. We build and release a data set of image tweets annotated with four classes which express whether the text or the image provides additional information to the other modality. We show that by combining the text and image information, we can build a machine learning approach that accurately distinguishes between the relationship types. Further, we derive insights into how these relationships are materialized through text and image content analysis and how they are impacted by user demographic traits. These methods can be used in several downstream applications including pre-training image tagging models, collecting distantly supervised data for image captioning, and can be directly used in end-user applications to optimize screen estate.", "text": [{"id": 0, "string": "Introduction Social media sites have traditionally been centered around publishing textual content."}, {"id": 1, "string": "Recently, posting images on social media has become a very popular way of expressing content and feelings especially due to the wide availability of mobile devices and connectivity."}, {"id": 2, "string": "Images are currently present in a significant fraction of tweets and tweets with images get double the engagement of those without (Buffer, 2016) ."}, {"id": 3, "string": "Thus, in addition to text, images have become key components of tweets."}, {"id": 4, "string": "However, little is known about how textual content is related to the images with which they appear."}, {"id": 5, "string": "For example, concepts or feelings mentioned in text could be illustrated or strengthened by images, text can point to the content of an image or  can just provide commentary on the image content."}, {"id": 6, "string": "Formalizing and understanding the relationship between the two modalities -text and images -is useful in several areas: a) for NLP and computer vision research, where image and text data from tweets are used to developing data sets and methods for image captioning (Mitchell et al., 2012) or object recognition (Mahajan et al., 2018) ; b) for social scientists and psychologists trying to understand social media use; c) in browsers or apps where images that may not contain additional content in addition to the text would be replaced by a placeholder and displayed if the end-user desires to in order to op-timize screen space (see Figure 2 )."}, {"id": 7, "string": "Figure 1 illustrates four different ways in which the text and image of the same tweet can be related: \u2022 Figures 1(a,b) show how the image can add to the semantics of the tweet, by either providing more information than the text (Figure 1a ) or by providing the context for understanding the text (Figure 1b ); \u2022 In Figures 1(c,d) , the image only illustrates what is expressed through text, without providing any additional information."}, {"id": 8, "string": "Hence, in both of these cases, the text alone is sufficient to understanding the tweet's key message; \u2022 Figures 1(a,c) show examples of tweets where there is a semantic overlap between the content of the text and image: bike and sign in Figure 1a and tacos in Figure 1c ; \u2022 In Figures 1(b,d) , the textual content is not represented in the image, with the text being either a comment on the image's content (Figure 1b) or the image illustrating a feeling related to the text's content."}, {"id": 9, "string": "In this paper, we present a comprehensive analysis that focuses on the types of relationships between the text and image in a tweet."}, {"id": 10, "string": "Our contributions include: \u2022 Defining the types of relationships between the text and the image of a social media post; \u2022 Building a data set of tweets annotated with text -image relationship type; 1 \u2022 Machine learning methods that use both text and image content to predict the relationship between the two modalities; \u2022 An analysis into the author's demographic traits that are related to usage preference of textimage relationship types; \u2022 An analysis of the textual features which characterize each relationship type."}, {"id": 11, "string": "Related Work Task."}, {"id": 12, "string": "The relationship between a text and its associated image was researched in a few prior studies."}, {"id": 13, "string": "For general web pages, Marsh and Domas White (2003) propose a taxonomy of 49 relationship grouped in three major categories based on how similar is the image to the text ranging from little relation to going beyond the text, which forms the basis of one of our relationship dimen-sions."}, {"id": 14, "string": "Martinec and Salway (2005) aim to categorize text-image relationships in scientific articles from two perspectives: the relative importance of one modality compared to the other and the logico-semantic overlap."}, {"id": 15, "string": "Alikhani and Stone (2018) argue that understanding multimodal textimage presentation requires studying the coherence relations that organize the content."}, {"id": 16, "string": "Even when a single relationship is used, such as captioning, it can be expressed in multiple forms such as telic, atelic or stative ."}, {"id": 17, "string": "Wang et al."}, {"id": 18, "string": "(2014) use the intuition that text and images from microposts can be associated or not or depend on one another and use this intuition in a topic model that learns topics and image tags jointly."}, {"id": 19, "string": "Jas and Parikh (2015) study the concept of image specificity through how similar to each other are multiple descriptions of that image."}, {"id": 20, "string": "However, none of these studies propose any predictive methods for text-image relationship types."}, {"id": 21, "string": "annotate and train models on a recipe data set (Yagcioglu et al., 2018) for the relationships between instructional text and images around the following dimensions: temporal, logical and incidental detail."}, {"id": 22, "string": "Chen et al."}, {"id": 23, "string": "(2013) study text-image relationships using social media data focusing on the distinction between images that are overall visually relevant or non-relevant to the textual content."}, {"id": 24, "string": "They build models using the text and image content that predict the relationship type (Chen et al., 2015) ."}, {"id": 25, "string": "We build on this research and define an annotation scheme that focuses on each of the two modalities separately and look at both their semantic overlap and contribution to the meaning of the whole tweet."}, {"id": 26, "string": "Applications."}, {"id": 27, "string": "Several applications require to be able to automatically predict the semantic textimage relationship in the data."}, {"id": 28, "string": "Models for automatically generating image descriptions (Feng and Lapata, 2010; Ordonez et al., 2011; Mitchell et al., 2012; Vinyals et al., 2015; Lu et al., 2017) or predicting tags (Mahajan et al., 2018) are built using large training data sets of noisy imagetext pairs from sources such as tweets."}, {"id": 29, "string": "Multimodal named entity disambiguation leverages visual context vectors from social media images to aid named entity disambiguation (Moon et al., 2018) ."}, {"id": 30, "string": "Multimodal topic labeling focuses on generating candidate labels (text or images) for a given topic and ranks them by relevance (Sorodoc et al., 2017) ."}, {"id": 31, "string": "Several resources of images paired with descriptive captions are available, which can be used to build similarity metrics and joint semantic spaces for text and images (Young et al., 2014) ."}, {"id": 32, "string": "However, all these assume that the text an image represent similar concepts which, as we show in this paper, is not true in Twitter."}, {"id": 33, "string": "Being able to classify this relationship can be useful for all above-mentioned applications."}, {"id": 34, "string": "Categorizing Text-Image Relationships We define the types of semantic relationships that can exist between the content of the text and the image by splitting them into two tasks for simplicity."}, {"id": 35, "string": "The first task is centered on the role of the text to the tweet's semantics, while the second focuses on the image's role."}, {"id": 36, "string": "The first task -referred to as the text task in the rest of the paper -focuses on identifying if there is semantic overlap between the context of the text and the image."}, {"id": 37, "string": "This task is the defined using the following guidelines: 1."}, {"id": 38, "string": "Some or all of the content words in the text are represented in the image (Text is represented) 2."}, {"id": 39, "string": "None of the content words in the text are represented in the image (Text is not represented): \u2022 None of the content words are represented in the image, or \u2022 The text is only a comment about the content of the image, or \u2022 The text expresses a feeling or emotion about the content of the image, or \u2022 The text only makes a reference to something shown in the image, or \u2022 The text is unrelated to the image Examples for this task can be seen in Figure 1 by comparing Figures 1(a ,c) (Text is represented) with Figures 1(b,d) (Text is not represented)."}, {"id": 40, "string": "The second task -referred to as the image task in the rest of the paper -focuses on the role of the image to the semantics of the tweet and aims to identify if the image's content contributes with additional information to the meaning of the tweet beyond the text, as judged by an independent third party."}, {"id": 41, "string": "This task is defined and annotated using the following guidelines: 1."}, {"id": 42, "string": "Image has additional content that represents the meaning of the text and the image (Image adds): \u2022 Image contains other text that adds additional meaning to the text, or \u2022 Image depicts something that adds information to the text or \u2022 Image contains other entities that are referenced by the text."}, {"id": 43, "string": "2."}, {"id": 44, "string": "Image does not add additional content that represents the meaning of text+image (Image does not add)."}, {"id": 45, "string": "Examples for the image task can be seen in Combining the labels of the two binary tasks described above gives rise to four types of text-image relationships (Image+Text Task)."}, {"id": 46, "string": "All of the four relationship types are exemplified in Figure 1 ."}, {"id": 47, "string": "Data Set To study the relationship between the text and image in the same social media post, we define a new annotation schema and collect a new annotated corpus."}, {"id": 48, "string": "To the best of our knowledge, no such corpus exists in prior research."}, {"id": 49, "string": "Data Sampling We use Twitter as the source of our data, as this source contains a high level of expression of thoughts, opinions and emotions (Java et al., 2007; Kouloumpis et al., 2011) ."}, {"id": 50, "string": "It represents a platform for observing written interactions and conversations between users (Ritter et al., 2010) ."}, {"id": 51, "string": "The tweets were deliberately randomly sampled tweets from a list of users for which several of their socio-demographic traits are known, introduced in past research ."}, {"id": 52, "string": "This will enable us to explore if the frequency of posting tweets with a certain text-image relationship is different across socio-demographic groups."}, {"id": 53, "string": "We downloaded as many tweets as we could from these users using the Twitter API (up to 3,200 tweets/user per API limits)."}, {"id": 54, "string": "We decided to annotate only tweets from within the same time range (2016) in order to reduce the influence of potential platform usage changes with time."}, {"id": 55, "string": "We filter out tweets that are not written in English using the langid.py tool (Lui and Baldwin, 2012) ."}, {"id": 56, "string": "In total, 2,263 users (out of the initial 4,132) have posted tweets with at least one image in the year 2016 and were included in our analysis."}, {"id": 57, "string": "Our final data set contains 4,471 tweets."}, {"id": 58, "string": "Demographic Variables The Twitter users from the data set we sampled have self-reported the following demographic variables through a survey: gender, age, education level and annual income."}, {"id": 59, "string": "All users solicited for data collection were from the United States in order to limit cultural variation."}, {"id": 60, "string": "\u2022 Gender was considered binary 2 and coded with Female -1 and Male -0."}, {"id": 61, "string": "All other variables are treated as ordinal variables."}, {"id": 62, "string": "\u2022 Age is represented as a integer value in the 13-90 year old interval."}, {"id": 63, "string": "\u2022 Education level is coded as an ordinal variable with 6 values representing the highest degree obtained, with the lowest being 'No high school degree' (coded as 1) and the highest being 'Advanced Degree (e.g., PhD)' (coded as 6)."}, {"id": 64, "string": "\u2022 Income level is coded as on ordinal variable with 8 values representing the annual income of the person, ranging from '< $20,000' to '> $200,000')."}, {"id": 65, "string": "For a full description of the user recruitment and quality control processes, we refer the interested reader to ."}, {"id": 66, "string": "Annotation We have collected annotations for text-image pairs from 4,471 tweets using the Figure Eight platform (formerly CrowdFlower)."}, {"id": 67, "string": "We annotate all tweets containing both text and image using two independent annotation tasks in order to simplify the task and not to prime annotators use the outcome of one task as a indicator for the outcome of the other."}, {"id": 68, "string": "For quality control, 10% of annotations were test questions annotated by the authors."}, {"id": 69, "string": "Annotators had to maintain a minimum accuracy on test questions of 85% for the image task and 75% for the text task for their annotations to be valid."}, {"id": 70, "string": "Inter-annotator agreement is measured using Krippendorf's Alpha."}, {"id": 71, "string": "The overall Krippendorfs Alpha is 0.71 for the image task, which is in the upper part of the substantial agreement band (Artstein and Poesio, 2008) ."}, {"id": 72, "string": "We collect 3 judgments and use majority vote to obtain the final label to further remove noise."}, {"id": 73, "string": "For the text task, we collected and aggregated 5 judgments as the Krippendorf's Alpha is 0.46, which is considered moderate agreement (Artstein and Poesio, 2008) ."}, {"id": 74, "string": "The latter task was more difficult due to requiring specific world knowledge (e.g."}, {"id": 75, "string": "a singer mentioned in a text also present in an image) or contained information encoded in hashtags or usernames which the annotators sometimes overlooked."}, {"id": 76, "string": "The aggregated judgments for each task were combined to obtain the four class labels."}, {"id": 77, "string": "Methods Our goal is to develop methods that are capable of automatically classifying the text-image relationship in tweets."}, {"id": 78, "string": "We experiment with several methods which use information of four different types: demographics of the user posting the tweet, metadata from the tweet, the text of the tweet or the image of the tweet; plus a combination of them."}, {"id": 79, "string": "The methods we use are described in this section."}, {"id": 80, "string": "User Demographics User demographic features are the survey-based demographic information we have available for all users that posted the annotated tweets."}, {"id": 81, "string": "The use of these traits is based on the intuition that different demographic groups have different posting preferences (Pennacchiotti and Popescu, 2011; ."}, {"id": 82, "string": "We use this approach for comparison reasons only, as in practical use cases we would normally not have access to the author's demographic traits."}, {"id": 83, "string": "We code the gender, age, education level and income level of the user as features and use them in a logistic regression classifier to classify the textimage relationship."}, {"id": 84, "string": "Tweet Metadata We experiment with using the tweet metadata as features."}, {"id": 85, "string": "These code if a tweet is a reply, tweet, like or neither."}, {"id": 86, "string": "We also add as features the tweet like count, the number of followers, friends and posts of the post's author and include them all in a logistic regression classifier."}, {"id": 87, "string": "These features are all available at tweet publishing time and we build a model using them to establish a more solid baseline for content based approaches."}, {"id": 88, "string": "Text-based Methods We use the textual content of the tweet alone to build models for predicting the text-image relationship."}, {"id": 89, "string": "We expect that certain textual cues will be specific to relationships even without considering the image content."}, {"id": 90, "string": "For example, tweets ending in an ellipsis or short comments will likely be predictive of the text not being represented in the image."}, {"id": 91, "string": "Surface Features."}, {"id": 92, "string": "We first use a range of surface features which capture more of the shallow stylistic content of the tweet."}, {"id": 93, "string": "We extract number of tokens, uppercase tokens, exclamations, questions, ellipsis, hashtags, @ mentions, quotes and URLs from the tweet and use them as features in a logistic regression classifier."}, {"id": 94, "string": "Bag of Words."}, {"id": 95, "string": "The most common approach for building a text-based model is using bag-ofwords features."}, {"id": 96, "string": "Here, we extract unigram and bigram features and use them in a logistic regression classifier with elastic net regularization (Zou and Hastie, 2005) ."}, {"id": 97, "string": "LSTM."}, {"id": 98, "string": "Finally, based on recent results in text classification, we also experiment with a neural network approach which uses a Long-Short Term Memory (LSTM) (Hochreiter and Schmidhuber, 1997) network."}, {"id": 99, "string": "The LSTM network processes the tweet sequentially, where each word is represented by its embedding (E = 200) followed by a dense hidden layer (D = 64) and by a a ReLU activation function and dropout (0.4) The model is trained by minimizing cross entropy using the Adam optimizer (Kingma and Ba, 2014)."}, {"id": 100, "string": "The network uses in-domain Twitter GloVe embeddings pre-trained on 2 billion tweets (Pennington et al., 2014) ."}, {"id": 101, "string": "Image-based Methods We use the content of the tweet image alone to build models for predicting the text-image relationship."}, {"id": 102, "string": "Similar to text, we expect that certain image content will be predictive of text-image relationships even without considering the text content."}, {"id": 103, "string": "For example, images of people may be more likely to have in the text the names of those persons."}, {"id": 104, "string": "To analyze image content, we make use of large pre-trained neural networks for the task of object recognition on the ImageNet data set."}, {"id": 105, "string": "ImageNet (Deng et al., 2009 ) is a visual database developed for object recognition research and consists of 1000 object types."}, {"id": 106, "string": "In particular, we use the popular pre-trained InceptionNet model (Szegedy et al., 2015) , which achieved the best performance at the ImageNet Large Scale Visual Recognition Challenge 2014 to build the following two imagebased models."}, {"id": 107, "string": "ImageNet Classes."}, {"id": 108, "string": "First, we represent each image in a tweet with the probability distribution over the 1,000 ImageNet classes obtained from Inception-Net."}, {"id": 109, "string": "Then, we pass those features to a logistic regression classifier which is trained on our task."}, {"id": 110, "string": "In this setup, the network parameters remain fixed, while only the ImageNet class weights are learned in the logistic regression classifier."}, {"id": 111, "string": "Tuned InceptionNet."}, {"id": 112, "string": "Additionally, we tailored the InceptionNet network to directly predict our tasks by using the multinomial logistic loss with softmax as the final layer for our task to replace the 1,000 ImageNet classes."}, {"id": 113, "string": "Then, we loaded the pretrained network from (Szegedy et al., 2015) and fine-tuned the final fully-connected layer with the modified loss layers."}, {"id": 114, "string": "We perform this in order to directly predict our task, while also overcoming the necessity of re-extracting the entire model weights from our restricted set of images."}, {"id": 115, "string": "The two approaches to classification using image content based on pre-trained model on Im-ageNet have been used successfully in past research (Cinar et al., 2015) ."}, {"id": 116, "string": "Joint Text-Image Methods Finally, we combine the textual and image information in a single model to classify the text-image relationship type, as we expect both types of content and their interaction to be useful to the task."}, {"id": 117, "string": "Ensemble."}, {"id": 118, "string": "A simple method for combining the information from both modalities is to build an ensemble classifier."}, {"id": 119, "string": "This is done with a logistic regression model with two features: the Bag of Words text model's predicted class probability and the Tuned InceptionNet model's predicted class probability."}, {"id": 120, "string": "The parameters of the model are tuned by cross validation on the training data and similar splits as the individual models."}, {"id": 121, "string": "LSTM + InceptionNet."}, {"id": 122, "string": "We also build a joint approach by concatenating the features from the final layers of our LSTM and InceptionNet models and passing them through a fully-connected (FC) feed forward neural network with one hidden layer (64 nodes)."}, {"id": 123, "string": "The final output is our text-image relationship type."}, {"id": 124, "string": "We use the Adam optimizer to fine tune this network."}, {"id": 125, "string": "The LSTM model has the same parameters as in the text-only approach, while the InceptionNet model is initialized with the pre-trained model on the ImageNet data set."}, {"id": 126, "string": "Predicting Text-Image Relationship We split our data into a 80% train (3,576 tweets) and 20% test (895 tweets) stratified sample for all of our experiments."}, {"id": 127, "string": "Parameters were tuned using 10-fold cross-validation with the training set, and results are reported on the test set."}, {"id": 128, "string": "Table 1 presents the weighted F1-scores for the text task, the image task and the image+text task with all the methods described in Section 5."}, {"id": 129, "string": "The weighted F1 score is the weighted average of the class-level F1 scores, where the weight is the number of items in each class."}, {"id": 130, "string": "The majority baseline always predicts the most frequent class in each task, namely: Image does not add for the image task, Text is not represented for the text task and Image does not add & Text is not represented for the Image + Text task."}, {"id": 131, "string": "The models using user demographics and tweet metadata show minor improvements over the majority class baseline for both tasks."}, {"id": 132, "string": "When the two tasks are combined, both feature types offer only a slight increase over the baseline."}, {"id": 133, "string": "This shows that user factors mildly impact the frequency with which relationship types are used, which will be explored further in the analysis section."}, {"id": 134, "string": "The models that use tweet text as features show consistent improvements over the baseline for all three tasks."}, {"id": 135, "string": "The two models that use the tweet's topical content (Bag of Words and LSTM) obtain higher predictive performance over the surface features."}, {"id": 136, "string": "Both content based models obtain relatively similar performance, with the LSTM performing better on the image task."}, {"id": 137, "string": "The models which use information extracted from the image alone also consistently outperform the baseline on all three tasks."}, {"id": 138, "string": "Re-tuning the neural network performs substantially better than building a model directly from the ImageNet classes on the image task and narrowly outperforms the other method on the text task."}, {"id": 139, "string": "This is somewhat expected, as the retuning is performed on this domain specific task."}, {"id": 140, "string": "When comparing text and image based models across tasks, we observe that using image features obtains substantially better performance on the image task, while the text models obtain bet-ter performance on the text task."}, {"id": 141, "string": "This is somewhat natural, as the focus of each annotation task is on one modality and methods relying on content from that modality are more predictive alone as to what ultimately represents the text-image relationship type."}, {"id": 142, "string": "Our naive ensemble approach does not yield substantially better results than the best performing methods using a single modality."}, {"id": 143, "string": "However, by jointly modelling both modalities, we are able to obtain improvements -especially on the image task."}, {"id": 144, "string": "This shows that both types of information and their interaction are important to this task."}, {"id": 145, "string": "Methods that exploit more heavily the interaction and semantic similarity between the text and the image are left for future work."}, {"id": 146, "string": "We also observe that the predictive methods we described are better at classifying the image task."}, {"id": 147, "string": "The analysis section below will allow us to uncover more about what type of content characterizes each relationship type."}, {"id": 148, "string": "Analysis In this section, we aim to gain a better understanding of the type of content specific of the four textimage relationship types and about user type preferences in their usage."}, {"id": 149, "string": "User Analysis Socio-demographic traits of the authors of posts are known to be correlated with several social media behaviors including text (Rao et al., 2010; Pennacchiotti and Popescu, 2011; Schwartz et al., 2013; Volkova et al., 2014; Lampos et al., 2014; Preo\u0163iuc-Pietro et al., 2015a ,b, 2016 and images (Alowibdi et al., 2013; You et al., 2014; Farseev et al., 2015; Skowron et al., 2016; Liu et al., 2016; Guntuku et al., 2017; Samani et al., 2018; Guntuku et al., 2019) ."}, {"id": 150, "string": "We hypothesize that socio-demographic traits also play a role in the types of text-image relationships employed on Twitter."}, {"id": 151, "string": "To measure this, we use partial Pearson correlation where the dependent variables are one of four socio-demographic traits described in Section 4.2."}, {"id": 152, "string": "The independent variables indicate the average times with which the user employed a certain relationship type."}, {"id": 153, "string": "We code this using six different variables: two representing the two broader tasks -the percentage of tweets where image adds information and the percentage of tweets where the text is represented in the image -and four encoding each combination between the two tasks."}, {"id": 154, "string": "In addition, for all analyses we consider gender and age as basic human traits and control for data skew by introducing both variables as controls in partial correlation, as done in prior work (Schwartz et al., 2013; Holgate et al., 2018) ."}, {"id": 155, "string": "When studying age and gender, we only use the other trait as the control."}, {"id": 156, "string": "Because we are running several statistical tests at once (24) without predefined hypotheses, we use Bonferroni correction to counteract the problem of multiple comparisons."}, {"id": 157, "string": "The results are presented in Table 2 ."}, {"id": 158, "string": "We observe that age is the only user demographic trait that is significantly correlated to text-image relationship preference after controlling for multiple comparisons and other demographic traits."}, {"id": 159, "string": "The text-image relationship where the text is represented in the image, at least partially, is positively correlated with age (r = 0.117)."}, {"id": 160, "string": "Further analyzing the four individual text-image relationship types reveals that older users especially prefer tweets where there is a semantic overlap between the concepts present in the text and the image, but the image contributes with additional information to the meaning of the tweet."}, {"id": 161, "string": "This is arguably the most conventional usage of images, where they illustrate the text and provide more details than the text could."}, {"id": 162, "string": "Younger users prefer most tweets where the image adds information to the meaning of the tweet, but this has no semantic overlap with the text."}, {"id": 163, "string": "These are usually tweets where the text represents merely a comment or a feeling expressed with the image providing the context."}, {"id": 164, "string": "This represents a more image-centric approach to the meaning of the tweet that is specific to younger users."}, {"id": 165, "string": "These correlations are controlled for gender."}, {"id": 166, "string": "Education was also correlated with images where the text was represented in the image (r = 0.076, p < .01, Bonferroni corrected), but this correlation did not meet the significance criteria when controlled for age to which education is moderately correlated (r = 0.302)."}, {"id": 167, "string": "This demonstrates the importance of controlling for such factors in this type of analysis."}, {"id": 168, "string": "No effects were found with respect to gender or income."}, {"id": 169, "string": "Table 2 : Pearson correlation between user demographic traits and usage of the different text-image relationship types."}, {"id": 170, "string": "All correlations in bold are significant at p < .01, two-tailed t-test, Bonferroni corrected for multiple comparisons."}, {"id": 171, "string": "Results for gender are controlled for age and vice versa."}, {"id": 172, "string": "Results for education and income are controlled for age and gender."}, {"id": 173, "string": "Tweet Metadata Analysis We adapt a similar approach to uncover potential relationships between the text-image relationship expressed in the tweet and tweet metadata features described in Section 5.2."}, {"id": 174, "string": "However, after controlling for multiple comparisons, we are left with no significant correlations at p < 0.01 level."}, {"id": 175, "string": "Hence, we refrain from presenting and discussing any results using this feature group as significant."}, {"id": 176, "string": "Text Analysis Finally, we aim to identify the text and image features that characterize the four types of text-image relationship."}, {"id": 177, "string": "We use univariate Pearson correlation where the independent variable is each feature's normalized value in a tweet and the dependent variables are two binary indicators for the text and image tasks respectively."}, {"id": 178, "string": "When performed using text features, this technique was coined Differential Language Analysis (Schwartz et al., 2013 (Schwartz et al., , 2017 ."}, {"id": 179, "string": "The results when using unigrams as features are presented in Figure 3 , 4 and 5."}, {"id": 180, "string": "Results for the image task (Figure 3) show that the image adds to the meaning of the tweet if words such as this, it, why, needs or want are used."}, {"id": 181, "string": "These words can appear in texts with the role of referencing or pointing to an entity which is only present in the image."}, {"id": 182, "string": "Conversely, the image does not add to the meaning of the tweet when words indicative of objects that are also described in the image are present (cat, baby, eyes or face), thus resulting in the image not adding to the meaning of the tweet."}, {"id": 183, "string": "A special case are tweets with birthday wishes, where a person is mentioned in text and also displayed in an image."}, {"id": 184, "string": "Finally, the tbt keyword and hashtag is a popular social media trend where users post nostalgic pictures of their past accompanied by their textual description."}, {"id": 185, "string": "The comparison between the two outcomes of the text task is presented in Figure 4 ."}, {"id": 186, "string": "When the text and image semantically overlap, we observe words indicative of actions (i've), possessions (your) or qualitative statements (congrats, loved, excited, tried), usually about objects or persons also present in the image."}, {"id": 187, "string": "We also observe a few nouns (cats, hillary) indicating frequent content that is also depicted in images (NB: the tweets were collected in 2016 when the U.S. presiden-tial elections took place)."}, {"id": 188, "string": "Analyzing this outcome jointly with the text task, we uncover a prominent theme consisting of words describing first person actions (congrats, thank, i've, saw, tell) present when the image provides facets not covered by text (Figure 5d )."}, {"id": 189, "string": "Several keywords from text (cat, game, winter) show types of content which are present in both image and text, but the image is merely an illustrating these concepts without adding additional information (Figure 5a) ."}, {"id": 190, "string": "In contrast, the text is not represented in the image when it contains words specific of comments (when, lmao), questions (do, was), references (this) or ellipsis ('...'), all often referencing the content of the image as identified through data inspection."}, {"id": 191, "string": "References to self, objects and personal states (i, me) and feelings (miss) are also expressed in text about items or things not appearing the image from the same tweet."}, {"id": 192, "string": "Further exploring this result though the image task outcome, we see that the latter category of feelings about persons of objects ( Figure 5a ) -miss, happy, lit, like) are specific of when the image does not add additional information."}, {"id": 193, "string": "Through manual inspection of these images, they often display a meme (as in Figure 1d ) or unrelated expressions to the text's content."}, {"id": 194, "string": "The image adds information when the text is not represented (Figure 5c ) if the latter includes personal feelings, (me, i, i'm, want), comments (lol, lmao) and references (this, it), usually related to the image content as identified through an analysis of the data."}, {"id": 195, "string": "Conclusions We defined and analyzed quantitatively and qualitatively the semantic relationships between the text and the image of the same tweet using a novel annotated data set."}, {"id": 196, "string": "The frequency of use is influenced by the age of the poster, with younger users employing images with a more prominent role in the tweet, rather than just being redundant to the text or as a means of illustrating it."}, {"id": 197, "string": "We studied the correlation between the content in the text and relation with the image, highlighting a differentiation between relationship types, even if only using the text of the tweet alone."}, {"id": 198, "string": "We developed models that use both text and image features to classify the text-image relationship, with especially high performance (F1 = 0.81) in identifying if the image is redundant, which is immediately useful for downstream applications that maximize screen es-tate for users."}, {"id": 199, "string": "Future work will look deeper into using the similarity between the content of the text and image (Leong and Mihalcea, 2011), as the text task results showed room for improvements."}, {"id": 200, "string": "We envision that our data, task and classifiers will be useful as a preprocessing step in collecting data for training large scale models for image captioning (Feng and Lapata, 2010) or tagging (Mahajan et al., 2018) or for improving recommendations (Chen et al., 2016) by filtering out tweets where the text and image have no semantic overlap or can enable new tasks such as identifying tweets that contain creative descriptions for images."}], "headers": [{"section": "Introduction", "n": "1", "start": 0, "end": 10}, {"section": "Related Work", "n": "2", "start": 11, "end": 33}, {"section": "Categorizing Text-Image Relationships", "n": "3", "start": 34, "end": 45}, {"section": "Data Set", "n": "4", "start": 46, "end": 48}, {"section": "Data Sampling", "n": "4.1", "start": 49, "end": 57}, {"section": "Demographic Variables", "n": "4.2", "start": 58, "end": 65}, {"section": "Annotation", "n": "4.3", "start": 66, "end": 76}, {"section": "Methods", "n": "5", "start": 77, "end": 79}, {"section": "User Demographics", "n": "5.1", "start": 80, "end": 83}, {"section": "Tweet Metadata", "n": "5.2", "start": 84, "end": 87}, {"section": "Text-based Methods", "n": "5.3", "start": 88, "end": 100}, {"section": "Image-based Methods", "n": "5.4", "start": 101, "end": 115}, {"section": "Joint Text-Image Methods", "n": "5.5", "start": 116, "end": 125}, {"section": "Predicting Text-Image Relationship", "n": "6", "start": 126, "end": 145}, {"section": "Analysis", "n": "7", "start": 146, "end": 148}, {"section": "User Analysis", "n": "7.1", "start": 149, "end": 172}, {"section": "Tweet Metadata Analysis", "n": "7.2", "start": 173, "end": 175}, {"section": "Text Analysis", "n": "7.3", "start": 176, "end": 194}, {"section": "Conclusions", "n": "8", "start": 195, "end": 200}], "figures": [{"filename": "../figure/image/1089-Figure2-1.png", "caption": "Figure 2: Example of application using the image task classifier. Automatically collapsing images that do not add content beyond text optimizes screen real estate and allows users to view more tweets in their feed view. The end-user could open hidden images individually.", "page": 2, "bbox": {"x1": 126.72, "x2": 450.71999999999997, "y1": 62.879999999999995, "y2": 259.2}}, {"filename": "../figure/image/1089-Table1-1.png", "caption": "Table 1: Experimental results in predicting text-image relationship with different methods and grouped by modalities used in prediction. Results are presented in weighted F1 score.", "page": 5, "bbox": {"x1": 308.64, "x2": 524.16, "y1": 62.879999999999995, "y2": 209.28}}, {"filename": "../figure/image/1089-Table2-1.png", "caption": "Table 2: Pearson correlation between user demographic traits and usage of the different text-image relationship types. All correlations in bold are significant at p < .01, two-tailed t-test, Bonferroni corrected for multiple comparisons. Results for gender are controlled for age and vice versa. Results for education and income are controlled for age and gender.", "page": 7, "bbox": {"x1": 72.96, "x2": 289.44, "y1": 62.879999999999995, "y2": 180.95999999999998}}, {"filename": "../figure/image/1089-Figure4-1.png", "caption": "Figure 4: Words specific of each of the two classes from the text task when compared to the other.", "page": 7, "bbox": {"x1": 318.71999999999997, "x2": 514.0799999999999, "y1": 202.56, "y2": 271.68}}, {"filename": "../figure/image/1089-Figure5-1.png", "caption": "Figure 5: Words that are specific of each of the four classes compared to all other three classes. Font size is proportional to the Pearson correlation between each relationship type and word frequency. Color is proportional to the word frequency (see legend above the figures for reference).", "page": 7, "bbox": {"x1": 316.8, "x2": 516.0, "y1": 309.59999999999997, "y2": 480.47999999999996}}, {"filename": "../figure/image/1089-Figure3-1.png", "caption": "Figure 3: Words specific of each of the two classes from the image task when compared to the other.", "page": 7, "bbox": {"x1": 318.71999999999997, "x2": 514.0799999999999, "y1": 60.96, "y2": 164.64}}, {"filename": "../figure/image/1089-Figure1-1.png", "caption": "Figure 1: Examples of the four types of text-image relationship from this study.", "page": 0, "bbox": {"x1": 317.76, "x2": 515.04, "y1": 223.2, "y2": 516.48}}]}