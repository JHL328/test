{"title": "Sequence-to-Action: End-to-End Semantic Graph Generation for Semantic Parsing", "abstract": "This paper proposes a neural semantic parsing approach -Sequence-to-Action, which models semantic parsing as an endto-end semantic graph generation process. Our method simultaneously leverages the advantages from two recent promising directions of semantic parsing. Firstly, our model uses a semantic graph to represent the meaning of a sentence, which has a tight-coupling with knowledge bases. Secondly, by leveraging the powerful representation learning and prediction ability of neural network models, we propose a RNN model which can effectively map sentences to action sequences for semantic graph generation. Experiments show that our method achieves state-of-the-art performance on OVERNIGHT dataset and gets competitive performance on GEO and ATIS datasets.", "text": [{"id": 0, "string": "Introduction Semantic parsing aims to map natural language sentences to logical forms (Zelle and Mooney, 1996; Zettlemoyer and Collins, 2005; Wong and Mooney, 2007; Lu et al., 2008; Kwiatkowski et al., 2013) ."}, {"id": 1, "string": "For example, the sentence \"Which states border Texas?\""}, {"id": 2, "string": "will be mapped to answer (A, (state (A), next to (A, stateid ( texas ))))."}, {"id": 3, "string": "A semantic parser needs two functions, one for structure prediction and the other for semantic grounding."}, {"id": 4, "string": "Traditional semantic parsers are usually based on compositional grammar, such as CCG Collins, 2005, 2007) , DCS (Liang et al., 2011) , etc."}, {"id": 5, "string": "These parsers compose structure using manually designed grammars, use lexicons for semantic grounding, and exploit fea-  tures for candidate logical forms ranking."}, {"id": 6, "string": "Unfortunately, it is challenging to design grammars and learn accurate lexicons, especially in wideopen domains."}, {"id": 7, "string": "Moreover, it is often hard to design effective features, and its learning process is not end-to-end."}, {"id": 8, "string": "To resolve the above problems, two promising lines of work have been proposed: Semantic graph-based methods and Seq2Seq methods."}, {"id": 9, "string": "Semantic graph-based methods (Reddy et al., 2014 (Reddy et al., , 2016 Bast and Haussmann, 2015; Yih et al., 2015) represent the meaning of a sentence as a semantic graph (i.e., a sub-graph of a knowledge base, see example in Figure 1 ) and treat semantic parsing as a semantic graph matching/generation process."}, {"id": 10, "string": "Compared with logical forms, semantic graphs have a tight-coupling with knowledge bases (Yih et al., 2015) , and share many commonalities with syntactic structures (Reddy et al., 2014) ."}, {"id": 11, "string": "Therefore both the structure and semantic constraints from knowledge bases can be easily exploited during parsing (Yih et al., 2015) ."}, {"id": 12, "string": "The main challenge of semantic graph-based parsing is how to effectively construct the semantic graph of a sentence."}, {"id": 13, "string": "Currently, semantic graphs are either constructed by matching with patterns (Bast and Haussmann, 2015) , transforming from dependency tree (Reddy et al., 2014 (Reddy et al., , 2016 , or via a staged heuristic search algorithm (Yih et al., 2015) ."}, {"id": 14, "string": "These methods are all based on manuallydesigned, heuristic construction processes, making them hard to handle open/complex situations."}, {"id": 15, "string": "In recent years, RNN models have achieved success in sequence-to-sequence problems due to its strong ability on both representation learning and prediction, e.g., in machine translation ."}, {"id": 16, "string": "A lot of Seq2Seq models have also been employed for semantic parsing (Xiao et al., 2016; Dong and Lapata, 2016; Jia and Liang, 2016) , where a sentence is parsed by translating it to linearized logical form using RNN models."}, {"id": 17, "string": "There is no need for high-quality lexicons, manually-built grammars, and hand-crafted features."}, {"id": 18, "string": "These models are trained end-to-end, and can leverage attention mechanism Luong et al., 2015) to learn soft alignments between sentences and logical forms."}, {"id": 19, "string": "In this paper, we propose a new neural semantic parsing framework -Sequence-to-Action, which can simultaneously leverage the advantages of semantic graph representation and the strong prediction ability of Seq2Seq models."}, {"id": 20, "string": "Specifically, we model semantic parsing as an end-to-end semantic graph generation process."}, {"id": 21, "string": "For example in Figure  1 , our model will parse the sentence \"Which states border Texas\" by generating a sequence of actions [add variable:A, add type:state, ...]."}, {"id": 22, "string": "To achieve the above goal, we first design an action set which can encode the generation process of semantic graph (including node actions such as add variable, add entity, add type, edge actions such as add edge, and operation actions such as argmin, argmax, count, sum, etc.)."}, {"id": 23, "string": "And then we design a RNN model which can generate the action sequence for constructing the semantic graph of a sentence."}, {"id": 24, "string": "Finally we further enhance parsing by incorporating both structure and semantic constraints during decoding."}, {"id": 25, "string": "Compared with the manually-designed, heuristic generation algorithms used in traditional semantic graph-based methods, our sequence-toaction method generates semantic graphs using a RNN model, which is learned end-to-end from training data."}, {"id": 26, "string": "Such a learnable, end-to-end generation makes our approach more effective and can fit to different situations."}, {"id": 27, "string": "Compared with the previous Seq2Seq semantic parsing methods, our sequence-to-action model predicts a sequence of semantic graph generation actions, rather than linearized logical forms."}, {"id": 28, "string": "We find that the action sequence encoding can better capture structure and semantic information, and is more compact."}, {"id": 29, "string": "And the parsing can be enhanced by exploiting structure and semantic constraints."}, {"id": 30, "string": "For example, in GEO dataset, the action add edge:next to must subject to the semantic constraint that its arguments must be of type state and state, and the structure constraint that the edge next to must connect two nodes to form a valid graph."}, {"id": 31, "string": "We evaluate our approach on three standard datasets: GEO (Zelle and Mooney, 1996) , ATIS (He and Young, 2005) and OVERNIGHT (Wang et al., 2015b) ."}, {"id": 32, "string": "The results show that our method achieves state-of-the-art performance on OVERNIGHT dataset and gets competitive performance on GEO and ATIS datasets."}, {"id": 33, "string": "The main contributions of this paper are summarized as follows: \u2022 We propose a new semantic parsing framework -Sequence-to-Action, which models semantic parsing as an end-to-end semantic graph generation process."}, {"id": 34, "string": "This new framework can synthesize the advantages of semantic graph representation and the prediction ability of Seq2Seq models."}, {"id": 35, "string": "\u2022 We design a sequence-to-action model, including an action set encoding for semantic graph generation and a Seq2Seq RNN model for action sequence prediction."}, {"id": 36, "string": "We further enhance the parsing by exploiting structure and semantic constraints during decoding."}, {"id": 37, "string": "Experiments validate the effectiveness of our method."}, {"id": 38, "string": "2 Sequence-to-Action Model for End-to-End Semantic Graph Generation Given a sentence X = x 1 , ..., x |X| , our sequenceto-action model generates a sequence of actions Y = y 1 , ..., y |Y | for constructing the correct semantic graph."}, {"id": 39, "string": "Figure 2 shows an example."}, {"id": 40, "string": "The conditional probability P (Y |X) used in our Figure 2 : An example of a sentence paired with its semantic graph, together with the action sequence for semantic graph generation."}, {"id": 41, "string": "model is decomposed as follows: P (Y |X) = |Y | t=1 P (y t |y <t , X) (1) where y <t = y 1 , ..., y t\u22121 ."}, {"id": 42, "string": "To achieve the above goal, we need: 1) an action set which can encode semantic graph generation process; 2) an encoder which encodes natural language input X into a vector representation, and a decoder which generates y 1 , ..., y |Y | conditioned on the encoding vector."}, {"id": 43, "string": "In following we describe them in detail."}, {"id": 44, "string": "Actions for Semantic Graph Generation Generally, a semantic graph consists of nodes (including variables, entities, types) and edges (semantic relations), with some universal operations (e.g., argmax, argmin, count, sum, and not)."}, {"id": 45, "string": "To generate a semantic graph, we define six types of actions as follows: Add Variable Node: This kind of actions denotes adding a variable node to semantic graph."}, {"id": 46, "string": "In most cases a variable node is a return node (e.g., which, what), but can also be an intermediate variable node."}, {"id": 47, "string": "We represent this kind of action as add variable:A, where A is the identifier of the variable node."}, {"id": 48, "string": "Add Entity Node: This kind of actions denotes adding an entity node (e.g., Texas, New York) and is represented as add entity node:texas."}, {"id": 49, "string": "An entity node corresponds to an entity in knowledge bases."}, {"id": 50, "string": "Add Type Node: This kind of actions denotes adding a type node (e.g., state, city)."}, {"id": 51, "string": "We represent them as add type node:state."}, {"id": 52, "string": "Add Edge: This kind of actions denotes adding an edge between two nodes."}, {"id": 53, "string": "An edge is a binary relation in knowledge bases."}, {"id": 54, "string": "This kind of actions is represented as add edge:next to."}, {"id": 55, "string": "Operation Action: This kind of actions denotes adding an operation."}, {"id": 56, "string": "An operation can be argmax, argmin, count, sum, not, et al."}, {"id": 57, "string": "Because each operation has a scope, we define two actions for an operation, one is operation start action, represented as start operation:most, and the other is operation end action, represented as end operation:most."}, {"id": 58, "string": "The subgraph within the start and end operation actions is its scope."}, {"id": 59, "string": "Argument Action: Some above actions need argument information."}, {"id": 60, "string": "For example, which nodes the add edge:next to action should connect to."}, {"id": 61, "string": "In this paper, we design argument actions for add type, add edge and operation actions, and the argument actions should be put directly after its main action."}, {"id": 62, "string": "For add type actions, we put an argument action to indicate which node this type node should constrain."}, {"id": 63, "string": "The argument can be a variable node or an entity node."}, {"id": 64, "string": "An argument action for a type node is represented as arg:A."}, {"id": 65, "string": "For add edge action, we use two argument actions: arg1 node and arg2 node, and they are represented as arg1 node:A and arg2 node:B."}, {"id": 66, "string": "We design argument actions for different operations."}, {"id": 67, "string": "For operation:sum, there are three arguments: arg-for, arg-in and arg-return."}, {"id": 68, "string": "For operation:count, they are arg-for and arg-return."}, {"id": 69, "string": "There are two arg-for arguments for operation:most."}, {"id": 70, "string": "We can see that each action encodes both structure and semantic information, which makes it easy to capture more information for parsing and can be tightly coupled with knowledge base."}, {"id": 71, "string": "Furthermore, we find that action sequence encoding is more compact than linearized logical form (See Section 4.4 for more details)."}, {"id": 72, "string": "Figure 3 : Our attention-based Sequence-to-Action RNN model, with a controller for incorporating constraints."}, {"id": 73, "string": "Neural Sequence-to-Action Model Based on the above action encoding mechanism, this section describes our encoder-decoder model for mapping sentence to action sequence."}, {"id": 74, "string": "Specifically, similar to the RNN model in Jia and Liang (2016) , this paper employs the attentionbased sequence-to-sequence RNN model."}, {"id": 75, "string": "Figure  3 presents the overall structure."}, {"id": 76, "string": "Encoder: The encoder converts the input sequence x 1 , ..., x m to a sequence of contextsensitive vectors b 1 , ..., b m using a bidirectional RNN ."}, {"id": 77, "string": "Firstly each word x i is mapped to its embedding vector, then these vectors are fed into a forward RNN and a backward RNN."}, {"id": 78, "string": "The sequence of hidden states h 1 , ..., h m are generated by recurrently applying the recurrence: h i = LST M (\u03c6 (x) (x i ), h i\u22121 )."}, {"id": 79, "string": "(2) The recurrence takes the form of LSTM (Hochreiter and Schmidhuber, 1997)."}, {"id": 80, "string": "Finally, for each input position i, we define its context-sensitive embedding as b i = [h F i , h B i ] ."}, {"id": 81, "string": "Decoder: This paper uses the classical attentionbased decoder , which generates action sequence y 1 , ..., y n , one action at a time."}, {"id": 82, "string": "At each time step j, it writes y j based on the current hidden state s j , then updates the hidden state to s j+1 based on s j and y j ."}, {"id": 83, "string": "The decoder is formally defined by the following equations: s 1 = tanh(W (s) [h F m , h B 1 ]) (3) e ji = s T j W (a) b i (4) a ji = exp(e ji ) m i =1 exp(e ji ) (5) c j = m i=1 a ji b i (6) P (y j = w|x, y 1:j\u22121 ) \u221d exp(U w [s j , c j ]) (7) s j+1 = LST M ([\u03c6 (y) (y j ), c j ], s j ) (8) where the normalized attention scores a ji defines the probability distribution over input words, indicating the attention probability on input word i at time j; e ji is un-normalized attention score."}, {"id": 84, "string": "To incorporate constraints during decoding, an extra controller component is added and its details will be described in Section 3.3."}, {"id": 85, "string": "Action Embedding."}, {"id": 86, "string": "The above decoder needs the embedding of each action."}, {"id": 87, "string": "As described above, each action has two parts, one for structure (e.g., add edge), and the other for semantic (e.g., next to)."}, {"id": 88, "string": "As a result, actions may share the same structure or semantic part, e.g., add edge:next to and add edge:loc have the same structure part, and add node:A and arg node:A have the same semantic part."}, {"id": 89, "string": "To make parameters more compact, we first embed the structure part and the semantic part independently, then concatenate them to get the final embedding."}, {"id": 90, "string": "For in- 3 Constrained Semantic Parsing using Sequence-to-Action Model stance, \u03c6 (y) (add edge:next to ) = [ \u03c6 (y) strut ( add edge ), \u03c6 In this section, we describe how to build a neural semantic parser using sequence-to-action model."}, {"id": 91, "string": "We first describe the training and the inference of our model, and then introduce how to incorporate structure and semantic constraints during decoding."}, {"id": 92, "string": "Training Parameter Estimation."}, {"id": 93, "string": "The parameters of our model include RNN parameters W (s) , W (a) , U w , word embeddings \u03c6 (x) , and action embeddings \u03c6 (y) ."}, {"id": 94, "string": "We estimate these parameters from training data."}, {"id": 95, "string": "Given a training example with a sentence X and its action sequence Y , we maximize the likelihood of the generated sequence of actions given X."}, {"id": 96, "string": "The objective function is: n i=1 log P (Y i |X i ) (9) Standard stochastic gradient descent algorithm is employed to update parameters."}, {"id": 97, "string": "Logical Form to Action Sequence."}, {"id": 98, "string": "Currently, most datasets of semantic parsing are labeled with logical forms."}, {"id": 99, "string": "In order to train our model, we convert logical forms to action sequences using semantic graph as an intermediate representation (See Figure 4 for an overview)."}, {"id": 100, "string": "Concretely, we transform logical forms into semantic graphs using a depth-first-search algorithm from root, and then generate the action sequence using the same order."}, {"id": 101, "string": "Specifically, entities, variables and types are nodes; relations are edges."}, {"id": 102, "string": "Conversely we can convert action sequence to logical form similarly."}, {"id": 103, "string": "Based on the above algorithm, action sequences can be transformed into logical forms in a deterministic way, and the same for logical forms to action sequences."}, {"id": 104, "string": "Mechanisms for Handling Entities."}, {"id": 105, "string": "Entities play an important role in semantic parsing (Yih et al., 2015) ."}, {"id": 106, "string": "In Dong and Lapata (2016) , entities are replaced with their types and unique IDs."}, {"id": 107, "string": "In Jia and Liang (2016) , entities are generated via attention-based copying mechanism helped with a lexicon."}, {"id": 108, "string": "This paper implements both mechanisms and compares them in experiments."}, {"id": 109, "string": "Inference Given a new sentence X, we predict action sequence by: Y * = argmax Y P (Y |X) (10) where Y represents action sequence, and P (Y |X) is computed using Formula (1)."}, {"id": 110, "string": "Beam search is used for best action sequence decoding."}, {"id": 111, "string": "Semantic graph and logical form can be derived from Y * as described in above."}, {"id": 112, "string": "Incorporating Constraints in Decoding For decoding, we generate action sequentially."}, {"id": 113, "string": "It is obviously that the next action has a strong correlation with the partial semantic graph generated to current, and illegal actions can be filtered using structure and semantic constraints."}, {"id": 114, "string": "Specifically, we incorporate constraints in decoding using a controller."}, {"id": 115, "string": "This procedure has two steps: 1) the controller constructs partial semantic graph using the actions generated to current; 2) the controller checks whether a new generated action can meet Figure 5 : A demonstration of illegal action filtering using constraints."}, {"id": 116, "string": "The graph in color is the constructed semantic graph to current."}, {"id": 117, "string": "all structure/semantic constraints using the partial semantic graph."}, {"id": 118, "string": "Structure Constraints."}, {"id": 119, "string": "The structure constraints ensure action sequence will form a connected acyclic graph."}, {"id": 120, "string": "For example, there must be two argument nodes for an edge, and the two argument nodes should be different (The third candidate next action in Figure 5 violates this constraint)."}, {"id": 121, "string": "This kind of constraints are domain-independent."}, {"id": 122, "string": "The controller encodes structure constraints as a set of rules."}, {"id": 123, "string": "Semantic Constraints."}, {"id": 124, "string": "The semantic constraints ensure the constructed graph must follow the schema of knowledge bases."}, {"id": 125, "string": "Specifically, we model two types of semantic constraints."}, {"id": 126, "string": "One is selectional preference constraints where the argument types of a relation should follow knowledge base schemas."}, {"id": 127, "string": "For example, in GEO dataset, relation next to's arg1 and arg2 should both be a state."}, {"id": 128, "string": "The second is type conflict constraints, i.e., an entity/variable node's type must be consistent, i.e., a node cannot be both of type city and state."}, {"id": 129, "string": "Semantic constraints are domain-specific and are automatically extracted from knowledge base schemas."}, {"id": 130, "string": "The controller encodes semantic constraints as a set of rules."}, {"id": 131, "string": "Experiments In this section, we assess the performance of our method and compare it with previous methods."}, {"id": 132, "string": "Datasets We conduct experiments on three standard datasets: GEO, ATIS and OVERNIGHT."}, {"id": 133, "string": "GEO contains natural language questions about US geography paired with corresponding Prolog database queries."}, {"id": 134, "string": "Following Zettlemoyer and Collins (2005) , we use the standard 600/280 instance splits for training/test."}, {"id": 135, "string": "ATIS contains natural language questions of a flight database, with each question is annotated with a lambda calculus query."}, {"id": 136, "string": "Following Zettlemoyer and Collins (2007) , we use the standard 4473/448 instance splits for training/test."}, {"id": 137, "string": "OVERNIGHT contains natural language paraphrases paired with logical forms across eight domains."}, {"id": 138, "string": "We evaluate on the standard train/test splits as Wang et al."}, {"id": 139, "string": "(2015b) ."}, {"id": 140, "string": "Experimental Settings Following the experimental setup of Jia and Liang (2016) : we use 200 hidden units and 100dimensional word vectors for sentence encoding."}, {"id": 141, "string": "The dimensions of action embedding are tuned on validation datasets for each corpus."}, {"id": 142, "string": "We initialize all parameters by uniformly sampling within the interval [-0.1, 0.1]."}, {"id": 143, "string": "We train our model for a total of 30 epochs with an initial learning rate of 0.1, and halve the learning rate every 5 epochs after epoch 15."}, {"id": 144, "string": "We replace word vectors for words occurring only once with an universal word vector."}, {"id": 145, "string": "The beam size is set as 5."}, {"id": 146, "string": "Our model is implemented in Theano (Bergstra et al., 2010) , and the codes and settings are released on Github: https://github.com/dongpobeyond/Seq2Act."}, {"id": 147, "string": "We evaluate different systems using the standard accuracy metric, and the accuracies on different datasets are obtained as same as Jia and Liang (2016) ."}, {"id": 148, "string": "Overall Results We compare our method with state-of-the-art systems on all three datasets."}, {"id": 149, "string": "Because all systems using the same training/test splits, we directly use the reported best performances from their original papers for fair comparison."}, {"id": 150, "string": "For our method, we train our model with three settings: the first one is the basic sequence-toaction model without constraints -Seq2Act; the second one adds structure constraints in decoding -Seq2Act (+C1); the third one is the full model which adds both structure and semantic GEO ATIS Previous Work Zettlemoyer and Collins (2005) Kwiatkowksi et al."}, {"id": 151, "string": "(2010) 88.9 - Kwiatkowski et al."}, {"id": 152, "string": "(2011) 88.6 82.8 Liang et al."}, {"id": 153, "string": "(2011)* (+lexicon) 91.1 -Poon (2013) -83.5 Zhao et al."}, {"id": 154, "string": "(2015) 88.9 84.2 Rabinovich et al."}, {"id": 155, "string": "(2017) 87.1 85.9 Seq2Seq Models Jia and Liang (2016) 85.0 76.3 Jia and Liang (2016) constraints -Seq2Act (+C1+C2)."}, {"id": 156, "string": "Semantic constraints (C2) are stricter than structure constraints (C1)."}, {"id": 157, "string": "Therefore we set that C1 should be first met for C2 to be met."}, {"id": 158, "string": "So in our experiments we add constraints incrementally."}, {"id": 159, "string": "The overall results are shown in Table 1 -2."}, {"id": 160, "string": "From the overall results, we can see that: 1) By synthetizing the advantages of semantic graph representation and the prediction ability of Seq2Seq model, our method achieves stateof-the-art performance on OVERNIGHT dataset, and gets competitive performance on GEO and ATIS dataset."}, {"id": 161, "string": "In fact, on GEO our full model (Seq2Act+C1+C2) also gets the best test accuracy of 88.9 if under the same settings, which only falls behind Liang et al."}, {"id": 162, "string": "(2011) * which uses extra handcrafted lexicons and Jia and Liang (2016) * which uses extra augmented training data."}, {"id": 163, "string": "On ATIS our full model gets the second best test accuracy of 85.5, which only falls behind Rabinovich et al."}, {"id": 164, "string": "(2017) which uses a supervised attention strategy."}, {"id": 165, "string": "On OVERNIGHT, our full model gets state-of-theart accuracy of 79.0, which even outperforms Jia and Liang (2016) * with extra augmented training data."}, {"id": 166, "string": "2) Compared with the linearized logical form representation used in previous Seq2Seq baselines, our action sequence encoding is more effective for semantic parsing."}, {"id": 167, "string": "On all three datasets, (2016) OVERNGIHT, the Seq2Act model gets a test accuracy of 78.0, better than the best Seq2Seq baseline gets 77.5."}, {"id": 168, "string": "We argue that this is because our action sequence encoding is more compact and can capture more information."}, {"id": 169, "string": "3) Structure constraints can enhance semantic parsing by ensuring the validity of graph using the generated action sequence."}, {"id": 170, "string": "In all three datasets, Seq2Act (+C1) outperforms the basic Seq2Act model."}, {"id": 171, "string": "This is because a part of illegal actions will be filtered during decoding."}, {"id": 172, "string": "4) By leveraging knowledge base schemas during decoding, semantic constraints are effective for semantic parsing."}, {"id": 173, "string": "Compared to Seq2Act and Seq2Act (+C1), the Seq2Act (+C1+C2) gets the best performance on all three datasets."}, {"id": 174, "string": "This is because semantic constraints can further filter semantic illegal actions using selectional preference and consistency between types."}, {"id": 175, "string": "Detailed Analysis Effect of Entity Handling Mechanisms."}, {"id": 176, "string": "This paper implements two entity handling mechanisms -Replacing (Dong and Lapata, 2016) which identifies entities and then replaces them with their types and IDs, and attention-based Copying (Jia and Liang, 2016) ."}, {"id": 177, "string": "To compare the above two mechanisms, we train and test with our full model and the results are shown in Table 3 ."}, {"id": 178, "string": "We can see that, Replacing mechanism outperforms Copying in all three datasets."}, {"id": 179, "string": "This is because Replacing is done   in preprocessing, while attention-based Copying is done during parsing and needs additional copy mechanism."}, {"id": 180, "string": "Linearized Logical Form vs. Action Sequence."}, {"id": 181, "string": "Table 4 shows the average length of linearized logical forms used in previous Seq2Seq models and the action sequences of our model on all three datasets."}, {"id": 182, "string": "As we can see, action sequence encoding is more compact than linearized logical form encoding: action sequence is shorter on all three datasets, 35.5%, 9.2% and 28.5% reduction in length respectively."}, {"id": 183, "string": "The main advantage of a shorter/compact encoding is that it will reduce the influence of long distance dependency problem."}, {"id": 184, "string": "Error Analysis We perform error analysis on results and find there are mainly two types of errors."}, {"id": 185, "string": "Unseen/Informal Sentence Structure."}, {"id": 186, "string": "Some test sentences have unseen syntactic structures."}, {"id": 187, "string": "For example, the first case in Table 5 has an unseen Gold Parse: answer(A, count (B, (const (C, stateid(iowa) ), next to(C, B), state (B)), A)) Predicted Parse: answer (A, count(B, state(B), A)) Under-Mapping Sentence: Please show me first class flights from indianapolis to memphis one way leaving before 10am Gold Parse: (lambda x (and (flight x) (oneway x) (class type x first:cl) (< (departure time x) 1000:ti) (from x indianapolis:ci) (to x memphis:ci))) Predicted Parse: (lambda x (and (flight x) (oneway x) (< (departure time x) 1000:ti) (from x indianapolis:ci) (to x memphis:ci))) Table 5 : Some examples for error analysis."}, {"id": 188, "string": "Each example includes the sentence for parsing, with gold parse and predicted parse from our model."}, {"id": 189, "string": "and informal structure, where entity word \"Iowa\" and relation word \"borders\" appear ahead of the question words \"how many\"."}, {"id": 190, "string": "For this problem, we can employ sentence rewriting or paraphrasing techniques (Chen et al., 2016; Dong et al., 2017) to transform unseen sentence structures into normal ones."}, {"id": 191, "string": "Under-Mapping."}, {"id": 192, "string": "As Dong and Lapata (2016) discussed, the attention model does not take the alignment history into consideration, makes some words are ignored during parsing."}, {"id": 193, "string": "For example in the second case in Table 5 , \"first class\" is ignored during the decoding process."}, {"id": 194, "string": "This problem can be further solved using explicit word coverage models used in neural machine translation (Tu et al., 2016; Cohn et al., 2016) Related Work Semantic parsing has received significant attention for a long time (Kate and Mooney, 2006; Clarke et al., 2010; Krishnamurthy and Mitchell, 2012; Berant and Liang, 2014; Quirk et al., 2015; Artzi et al., 2015; ."}, {"id": 195, "string": "Traditional methods are mostly based on the principle of compositional semantics, which first trigger predicates using lexicons and then compose them using grammars."}, {"id": 196, "string": "The prominent grammars include SCFG (Wong and Mooney, 2007; Li et al., 2015) , CCG (Zettlemoyer and Collins, 2005; Kwiatkowski et al., 2011; Cai and Yates, 2013) , DCS (Liang et al., 2011; Berant et al., 2013) , etc."}, {"id": 197, "string": "As discussed above, the main drawback of grammar-based methods is that they rely on high-quality lexicons, manually-built grammars, and hand-crafted features."}, {"id": 198, "string": "In recent years, one promising direction of semantic parsing is to use semantic graph as representation."}, {"id": 199, "string": "Thus semantic parsing is modeled as a semantic graph generation process."}, {"id": 200, "string": "Ge and Mooney (2009) build semantic graph by trans-forming syntactic tree."}, {"id": 201, "string": "Bast and Haussmann (2015) identify the structure of a semantic query using three pre-defined patterns."}, {"id": 202, "string": "Reddy et al."}, {"id": 203, "string": "(2014 Reddy et al."}, {"id": 204, "string": "( , 2016 use Freebase-based semantic graph representation, and convert sentences to semantic graphs using CCG or dependency tree."}, {"id": 205, "string": "Yih et al."}, {"id": 206, "string": "(2015) generate semantic graphs using a staged heuristic search algorithm."}, {"id": 207, "string": "These methods are all based on manually-designed, heuristic generation process, which may suffer from syntactic parse errors (Ge and Mooney, 2009; Reddy et al., 2014 Reddy et al., , 2016 , structure mismatch (Chen et al., 2016) , and are hard to deal with complex sentences (Yih et al., 2015) ."}, {"id": 208, "string": "One other direction is to employ neural Seq2Seq models, which models semantic parsing as an end-to-end, sentence to logical form machine translation problem."}, {"id": 209, "string": "Dong and Lapata (2016) , Jia and Liang (2016) and Xiao et al."}, {"id": 210, "string": "(2016) transform word sequence to linearized logical forms."}, {"id": 211, "string": "One main drawback of these methods is that it is hard to capture and exploit structure and semantic constraints using linearized logical forms."}, {"id": 212, "string": "Dong and Lapata (2016) propose a Seq2Tree model to capture the hierarchical structure of logical forms."}, {"id": 213, "string": "It has been shown that structure and semantic constraints are effective for enhancing semantic parsing."}, {"id": 214, "string": "Krishnamurthy et al."}, {"id": 215, "string": "(2017) use type constraints to filter illegal tokens."}, {"id": 216, "string": "Liang et al."}, {"id": 217, "string": "(2017) adopt a Lisp interpreter with pre-defined functions to produce valid tokens."}, {"id": 218, "string": "Iyyer et al."}, {"id": 219, "string": "(2017) adopt type constraints to generate valid actions."}, {"id": 220, "string": "Inspired by these approaches, we also incorporate both structure and semantic constraints in our neural sequence-to-action model."}, {"id": 221, "string": "Transition-based approaches are important in both dependency parsing (Nivre, 2008; Henderson et al., 2013) and AMR parsing (Wang et al., 2015a) ."}, {"id": 222, "string": "In semantic parsing, our method has a tight-coupling with knowledge bases, and con-straints can be exploited for more accurate decoding."}, {"id": 223, "string": "We believe this can also be used to enhance previous transition based methods and may also be used in other parsing tasks, e.g., AMR parsing."}, {"id": 224, "string": "Conclusions This paper proposes Sequence-to-Action, a method which models semantic parsing as an end-to-end semantic graph generation process."}, {"id": 225, "string": "By leveraging the advantages of semantic graph representation and exploiting the representation learning and prediction ability of Seq2Seq models, our method achieved significant performance improvements on three datasets."}, {"id": 226, "string": "Furthermore, structure and semantic constraints can be easily incorporated in decoding to enhance semantic parsing."}, {"id": 227, "string": "For future work, to solve the problem of the lack of training data, we want to design weakly supervised learning algorithm using denotations (QA pairs) as supervision."}, {"id": 228, "string": "Furthermore, we want to collect labeled data by designing an interactive UI for annotation assist like (Yih et al., 2016) , which uses semantic graphs to annotate the meaning of sentences, since semantic graph is more natural and can be easily annotated without the need of expert knowledge."}], "headers": [{"section": "Introduction", "n": "1", "start": 0, "end": 43}, {"section": "Actions for Semantic Graph Generation", "n": "2.1", "start": 44, "end": 72}, {"section": "Neural Sequence-to-Action Model", "n": "2.2", "start": 73, "end": 91}, {"section": "Training", "n": "3.1", "start": 92, "end": 108}, {"section": "Inference", "n": "3.2", "start": 109, "end": 111}, {"section": "Incorporating Constraints in Decoding", "n": "3.3", "start": 112, "end": 129}, {"section": "Experiments", "n": "4", "start": 130, "end": 131}, {"section": "Datasets", "n": "4.1", "start": 132, "end": 139}, {"section": "Experimental Settings", "n": "4.2", "start": 140, "end": 147}, {"section": "Overall Results", "n": "4.3", "start": 148, "end": 174}, {"section": "Detailed Analysis", "n": "4.4", "start": 175, "end": 183}, {"section": "Error Analysis", "n": "4.5", "start": 184, "end": 193}, {"section": "Related Work", "n": "5", "start": 194, "end": 223}, {"section": "Conclusions", "n": "6", "start": 224, "end": 228}], "figures": [{"filename": "../figure/image/1286-Figure1-1.png", "caption": "Figure 1: Overview of our method, with a demonstration example.", "page": 0, "bbox": {"x1": 308.64, "x2": 532.3199999999999, "y1": 224.64, "y2": 384.96}}, {"filename": "../figure/image/1286-Table1-1.png", "caption": "Table 1: Test accuracies on GEO and ATIS datasets, where * indicates systems with extraresources are used.", "page": 5, "bbox": {"x1": 306.71999999999997, "x2": 527.04, "y1": 63.839999999999996, "y2": 324.0}}, {"filename": "../figure/image/1286-Table4-1.png", "caption": "Table 4: Average length of logical forms and action sequences on three datasets. On OVERNIGHT, we average across all eight domains.", "page": 6, "bbox": {"x1": 306.71999999999997, "x2": 536.16, "y1": 366.71999999999997, "y2": 424.32}}, {"filename": "../figure/image/1286-Table2-1.png", "caption": "Table 2: Test accuracies on OVERNIGHT dataset, which includes eight domains: Social, Blocks, Basketball, Restaurants, Calendar, Housing, Publications, and Recipes.", "page": 6, "bbox": {"x1": 84.0, "x2": 513.12, "y1": 62.879999999999995, "y2": 214.07999999999998}}, {"filename": "../figure/image/1286-Table3-1.png", "caption": "Table 3: Test accuracies of Seq2Act (+C1+C2) on GEO, ATIS, and OVERNIGHT of two entity handling mechanisms.", "page": 6, "bbox": {"x1": 327.84, "x2": 505.44, "y1": 259.68, "y2": 317.28}}, {"filename": "../figure/image/1286-Figure2-1.png", "caption": "Figure 2: An example of a sentence paired with its semantic graph, together with the action sequence for semantic graph generation.", "page": 2, "bbox": {"x1": 73.44, "x2": 296.15999999999997, "y1": 64.8, "y2": 323.03999999999996}}, {"filename": "../figure/image/1286-Table5-1.png", "caption": "Table 5: Some examples for error analysis. Each example includes the sentence for parsing, with gold parse and predicted parse from our model.", "page": 7, "bbox": {"x1": 72.96, "x2": 524.16, "y1": 62.879999999999995, "y2": 174.23999999999998}}, {"filename": "../figure/image/1286-Figure3-1.png", "caption": "Figure 3: Our attention-based Sequence-to-Action RNN model, with a controller for incorporating constraints.", "page": 3, "bbox": {"x1": 73.44, "x2": 297.12, "y1": 63.839999999999996, "y2": 140.16}}, {"filename": "../figure/image/1286-Figure4-1.png", "caption": "Figure 4: The procedure of converting between logical form and action sequence.", "page": 4, "bbox": {"x1": 72.0, "x2": 298.08, "y1": 63.839999999999996, "y2": 112.32}}, {"filename": "../figure/image/1286-Figure5-1.png", "caption": "Figure 5: A demonstration of illegal action filtering using constraints. The graph in color is the constructed semantic graph to current.", "page": 4, "bbox": {"x1": 308.64, "x2": 532.3199999999999, "y1": 63.839999999999996, "y2": 260.15999999999997}}]}