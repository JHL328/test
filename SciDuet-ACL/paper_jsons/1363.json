{"title": "Weakly Supervised Semantic Parsing with Abstract Examples", "abstract": "Training semantic parsers from weak supervision (denotations) rather than strong supervision (programs) complicates training in two ways. First, a large search space of potential programs needs to be explored at training time to find a correct program. Second, spurious programs that accidentally lead to a correct denotation add noise to training. In this work we propose that in closed worlds with clear semantic types, one can substantially alleviate these problems by utilizing an abstract representation, where tokens in both the language utterance and program are lifted to an abstract form. We show that these abstractions can be defined with a handful of lexical rules and that they result in sharing between different examples that alleviates the difficulties in training. To test our approach, we develop the first semantic parser for CNLVR, a challenging visual reasoning dataset, where the search space is large and overcoming spuriousness is critical, because denotations are either TRUE or FALSE, and thus random programs are likely to lead to a correct denotation. Our method substantially improves performance, and reaches 82.5% accuracy, a 14.7% absolute accuracy improvement compared to the best reported accuracy so far.", "text": [{"id": 0, "string": "Introduction The goal of semantic parsing is to map language utterances to executable programs."}, {"id": 1, "string": "Early work on statistical learning of semantic parsers utilized * Authors equally contributed to this work."}, {"id": 2, "string": "IsSmall(x)), Not(IsTouchingWall(x, Side.Any)))))) Figure 1: Overview of our visual reasoning setup for the CN-LVR dataset."}, {"id": 3, "string": "Given an image rendered from a KB k and an utterance x, our goal is to parse x to a program z that results in the correct denotation y."}, {"id": 4, "string": "Our training data includes (x, k, y) triplets."}, {"id": 5, "string": "supervised learning, where training examples included pairs of language utterances and programs (Zelle and Mooney, 1996; Kate et al., 2005; Collins, 2005, 2007) ."}, {"id": 6, "string": "However, collecting such training examples at scale has quickly turned out to be difficult, because expert annotators who are familiar with formal languages are required."}, {"id": 7, "string": "This has led to a body of work on weaklysupervised semantic parsing (Clarke et al., 2010; Liang et al., 2011; Krishnamurthy and Mitchell, 2012; Kwiatkowski et al., 2013; Berant et al., 2013; Cai and Yates, 2013; ."}, {"id": 8, "string": "In this setup, training examples correspond to utterance-denotation pairs, where a denotation is the result of executing a program against the environment (see Fig."}, {"id": 9, "string": "1 )."}, {"id": 10, "string": "Naturally, collecting denotations is much easier, because it can be performed by non-experts."}, {"id": 11, "string": "Training semantic parsers from denotations rather than programs complicates training in two ways: (a) Search: The algorithm must learn to search through the huge space of programs at training time, in order to find the correct program."}, {"id": 12, "string": "This is a difficult search problem due to the combinatorial nature of the search space."}, {"id": 13, "string": "(b) Spurious-ness: Incorrect programs can lead to correct denotations, and thus the learner can go astray based on these programs."}, {"id": 14, "string": "Of the two mentioned problems, spuriousness has attracted relatively less attention (Pasupat and Liang, 2016; Guu et al., 2017) ."}, {"id": 15, "string": "Recently, the Cornell Natural Language for Visual Reasoning corpus (CNLVR) was released (Suhr et al., 2017) , and has presented an opportunity to better investigate the problem of spuriousness."}, {"id": 16, "string": "In this task, an image with boxes that contains objects of various shapes, colors and sizes is shown."}, {"id": 17, "string": "Each image is paired with a complex natural language statement, and the goal is to determine whether the statement is true or false (Fig."}, {"id": 18, "string": "1) ."}, {"id": 19, "string": "The task comes in two flavors, where in one the input is the image (pixels), and in the other it is the knowledge-base (KB) from which the image was synthesized."}, {"id": 20, "string": "Given the KB, it is easy to view CNLVR as a semantic parsing problem: our goal is to translate language utterances into programs that will be executed against the KB to determine their correctness (Johnson et al., 2017b; Hu et al., 2017) ."}, {"id": 21, "string": "Because there are only two return values, it is easy to generate programs that execute to the right denotation, and thus spuriousness is a major problem compared to previous datasets."}, {"id": 22, "string": "In this paper, we present the first semantic parser for CNLVR."}, {"id": 23, "string": "Semantic parsing can be coarsely divided into a lexical task (i.e., mapping words and phrases to program constants), and a structural task (i.e., mapping language composition to program composition operators)."}, {"id": 24, "string": "Our core insight is that in closed worlds with clear semantic types, like spatial and visual reasoning, we can manually construct a small lexicon that clusters language tokens and program constants, and create a partially abstract representation for utterances and programs (Table 1) in which the lexical problem is substantially reduced."}, {"id": 25, "string": "This scenario is ubiquitous in many semantic parsing applications such as calendar, restaurant reservation systems, housing applications, etc: the formal language has a compact semantic schema and a well-defined typing system, and there are canonical ways to express many program constants."}, {"id": 26, "string": "We show that with abstract representations we can share information across examples and better tackle the search and spuriousness challenges."}, {"id": 27, "string": "By pulling together different examples that share the same abstract representation, we can identify programs that obtain high reward across multiple examples, thus reducing the problem of spuriousness."}, {"id": 28, "string": "This can also be done at search time, by augmenting the search state with partial programs that have been shown to be useful in earlier iterations."}, {"id": 29, "string": "Moreover, we can annotate a small number of abstract utterance-program pairs, and automatically generate training examples, that will be used to warm-start our model to an initialization point in which search is able to find correct programs."}, {"id": 30, "string": "We develop a formal language for visual reasoning, inspired by Johnson et al."}, {"id": 31, "string": "(2017b) , and train a semantic parser over that language from weak supervision, showing that abstract examples substantially improve parser accuracy."}, {"id": 32, "string": "Our parser obtains an accuracy of 82.5%, a 14.7% absolute accuracy improvement compared to stateof-the-art."}, {"id": 33, "string": "All our code is publicly available at https://github.com/udiNaveh/ nlvr_tau_nlp_final_proj."}, {"id": 34, "string": "Setup Problem Statement Given a training set of N examples {(x i , k i , y i )} N i=1 , where x i is an utterance, k i is a KB describing objects in an image and y i \u2208 {TRUE, FALSE} denotes whether the utterance is true or false in the KB, our goal is to learn a semantic parser that maps a new utterance x to a program z such that when z is executed against the corresponding KB k, it yields the correct denotation y (see Fig."}, {"id": 35, "string": "1 )."}, {"id": 36, "string": "Programming language The original KBs in CNLVR describe an image as a set of objects, where each object has a color, shape, size and location in absolute coordinates."}, {"id": 37, "string": "We define a programming language over the KB that is more amenable to spatial reasoning, inspired by work on the CLEVR dataset (Johnson et al., 2017b) ."}, {"id": 38, "string": "This programming language provides access to functions that allow us to check the size, shape, and color of an object, to check whether it is touching a wall, to obtain sets of items that are above and below a certain set of items, etc."}, {"id": 39, "string": "1 More formally, a program is a sequence of tokens describing a possibly recursive sequence of function applications in prefix notation."}, {"id": 40, "string": "Each token is either a function with fixed arity (all functions have either one or two arguments), a constant, a variable or a \u03bb term used to define Boolean functions."}, {"id": 41, "string": "Functions, constants and variables have one of the following x: \"There are exactly 3 yellow squares touching the wall.\""}, {"id": 42, "string": "z: Equal(3, Count(Filter(ALL ITEMS, \u03bbx."}, {"id": 43, "string": "And (And (IsYellow(x), IsSquare(x), IsTouchingWall(x)))))) x: \"There are C-QuantMod C-Num C-Color C-Shape touching the wall.\""}, {"id": 44, "string": "z: C-QuantMod(C-Num, Count(Filter(ALL ITEMS, \u03bbx."}, {"id": 45, "string": "And (And (IsC-Color(x), IsC-Shape(x), IsTouchingWall(x)))))) Table 1 : An example for an utterance-program pair (x, z) and its abstract counterpart (x,z) x: \"There is a small yellow item not touching any wall.\""}, {"id": 46, "string": "z: Exist(Filter(ALL ITEMS, \u03bbx.And(And(IsYellow(x), IsSmall(x)), Not(IsTouchingWall(x, Side.Any))))) x: \"One tower has a yellow base.\""}, {"id": 47, "string": "z: GreaterEqual(1, Count(Filter(ALL ITEMS, \u03bbx.And(IsYellow(x), IsBottom(x))))) Table 2 : Examples for utterance-program pairs."}, {"id": 48, "string": "Commas and parenthesis provided for readability only."}, {"id": 49, "string": "atomic types: Int, Bool, Item, Size, Shape, Color, Side (sides of a box in the image); or a composite type Set(?"}, {"id": 50, "string": "), and Func(?,?)."}, {"id": 51, "string": "Valid programs have a return type Bool."}, {"id": 52, "string": "Tables 1 and 2 provide examples for utterances and their correct programs."}, {"id": 53, "string": "The supplementary material provides a full description of all program tokens, their arguments and return types."}, {"id": 54, "string": "Unlike CLEVR, CNLVR requires substantial set-theoretic reasoning (utterances refer to various aspects of sets of items in one of the three boxes in the image), which required extending the language described by Johnson et al."}, {"id": 55, "string": "(2017b) to include set operators and lambda abstraction."}, {"id": 56, "string": "We manually sampled 100 training examples from the training data and estimate that roughly 95% of the utterances in the training data can be expressed with this programming language."}, {"id": 57, "string": "Model We base our model on the semantic parser of Guu et al."}, {"id": 58, "string": "(2017) ."}, {"id": 59, "string": "In their work, they used an encoderdecoder architecture (Sutskever et al., 2014) to define a distribution p \u03b8 (z | x)."}, {"id": 60, "string": "The utterance x is encoded using a bi-directional LSTM (Hochreiter and Schmidhuber, 1997 ) that creates a contextualized representation h i for every utterance token x i , and the decoder is a feed-forward network combined with an attention mechanism over the encoder outputs (Bahdanau et al., 2015) ."}, {"id": 61, "string": "The feedforward decoder takes as input the last K tokens that were decoded."}, {"id": 62, "string": "More formally the probability of a program is the product of the probability of its tokens given the history: p \u03b8 (z | x) = t p \u03b8 (z t | x, z 1:t\u22121 ), and the probability of a decoded token is computed as follows."}, {"id": 63, "string": "First, a Bi-LSTM encoder converts the input sequence of utterance embeddings into a sequence of forward and backward states h {F,B} 1 , ."}, {"id": 64, "string": "."}, {"id": 65, "string": "."}, {"id": 66, "string": ", h {F,B} |x| ."}, {"id": 67, "string": "The utterance representation x isx = [h F |x| ; h B 1 ]."}, {"id": 68, "string": "Then decoding produces the program token-by-token: q t = ReLU(W q [x;v; z t\u2212K\u22121:t\u22121 ]), \u03b1 t,i \u221d exp(q t W \u03b1 h i ) , c t = i \u03b1 t,i h i , p \u03b8 (z t | x, z 1:t\u22121 ) \u221d exp(\u03c6 zt W s [q t ; c t ]), where \u03c6 z is an embedding for program token z, v is a bag-of-words vector for the tokens in x, z i:j = (z i , ."}, {"id": 69, "string": "."}, {"id": 70, "string": "."}, {"id": 71, "string": ", z j ) is a history vector of size K, the matrices W q , W \u03b1 , W s are learned parameters (along with the LSTM parameters and embedding matrices), and ';' denotes concatenation."}, {"id": 72, "string": "Search: Searching through the large space of programs is a fundamental challenge in semantic parsing."}, {"id": 73, "string": "To combat this challenge we apply several techniques."}, {"id": 74, "string": "First, we use beam search at decoding time and when training from weak supervision (see Sec."}, {"id": 75, "string": "4), similar to prior work Guu et al., 2017) ."}, {"id": 76, "string": "At each decoding step we maintain a beam B of program prefixes of length n, expand them exhaustively to programs of length n+1 and keep the top-|B| program prefixes with highest model probability."}, {"id": 77, "string": "Second, we utilize the semantic typing system to only construct programs that are syntactically valid, and substantially prune the program search space (similar to type constraints in Krishnamurthy et al."}, {"id": 78, "string": "(2017) 2017) )."}, {"id": 79, "string": "We maintain a stack that keeps track of the expected semantic type at each decoding step."}, {"id": 80, "string": "The stack is initialized with the type Bool."}, {"id": 81, "string": "Then, at each decoding step, only tokens that return the semantic type at the top of the stack are allowed, the stack is popped, and if the decoded token is a function, the semantic types of its arguments are pushed to the stack."}, {"id": 82, "string": "This dramatically reduces the search space and guarantees that only syntactically valid programs will be produced."}, {"id": 83, "string": "Fig."}, {"id": 84, "string": "2 illustrates the state of the stack when decoding a program for an input utterance."}, {"id": 85, "string": "Given the constrains on valid programs, our model p \u03b8 (z | x) is defined as: t p \u03b8 (z t | x, z 1:t\u22121 ) \u00b7 1(z t | z 1:t\u22121 ) z p \u03b8 (z | x, z 1:t\u22121 ) \u00b7 1(z | z 1:t\u22121 ) , where 1(z t | z 1:t\u22121 ) indicates whether a certain program token is valid given the program prefix."}, {"id": 86, "string": "Discriminative re-ranking: The above model is a locally-normalized model that provides a distribution for every decoded token, and thus might suffer from the label bias problem (Andor et al., 2016; Lafferty et al., 2001) ."}, {"id": 87, "string": "Thus, we add a globally-normalized re-ranker p \u03c8 (z | x) that scores all |B| programs in the final beam produced by p \u03b8 (z | x)."}, {"id": 88, "string": "Our globally-normalized model is: p g \u03c8 (z | x) \u221d exp(s \u03c8 (x, z)), and is normalized over all programs in the beam."}, {"id": 89, "string": "The scoring function s \u03c8 (x, z) is a neural network with identical architecture to the locallynormalized model, except that (a) it feeds the decoder with the candidate program z and does not generate it."}, {"id": 90, "string": "(b) the last hidden state is inserted to a feed-forward network whose output is s \u03c8 (x, z)."}, {"id": 91, "string": "Our final ranking score is p \u03b8 (z|x)p g \u03c8 (z | x)."}, {"id": 92, "string": "Training We now describe our basic method for training from weak supervision, which we extend upon in Sec."}, {"id": 93, "string": "5 using abstract examples."}, {"id": 94, "string": "To use weak supervision, we treat the program z as a latent variable that is approximately marginalized."}, {"id": 95, "string": "To describe the objective, define R(z, k, y) \u2208 {0, 1} to be one if executing program z on KB k results in denotation y, and zero otherwise."}, {"id": 96, "string": "The objective is then to maximize p(y | x) given by: z\u2208Z p \u03b8 (z | x)p(y | z, k) = z\u2208Z p \u03b8 (z | x)R(z, k, y) \u2248 z\u2208B p \u03b8 (z | x)R(z, k, y) where Z is the space of all programs and B \u2282 Z are the programs found by beam search."}, {"id": 97, "string": "In most semantic parsers there will be relatively few z that generate the correct denotation y."}, {"id": 98, "string": "However, in CNLVR, y is binary, and so spuriousness is a central problem."}, {"id": 99, "string": "To alleviate it, we utilize a property of CNLVR: the same utterance appears 4 times with 4 different images."}, {"id": 100, "string": "2 If a program is spurious it is likely that it will yield the wrong denotation in one of those 4 images."}, {"id": 101, "string": "Thus, we can re-define each training example to be (x, {(k j , y j )} 4 j=1 ), where each utterance x is paired with 4 different KBs and the denotations of the utterance with respect to these KBs."}, {"id": 102, "string": "Then, we maximize p({y j } 4 j=1 | x, ) by maximizing the objective above, except that R(z, {k j , y j } 4 j=1 ) = 1 iff the denotation of z is correct for all four KBs."}, {"id": 103, "string": "This dramatically reduces the problem of spuriousness, as the chance of randomly obtaining a correct denotation goes down from 1 2 to 1 16 ."}, {"id": 104, "string": "This is reminiscent of Pasupat and Liang (2016) , where random permutations of Wikipedia tables were shown to crowdsourcing workers to eliminate spurious programs."}, {"id": 105, "string": "We train the discriminative ranker analogously by maximizing the probability of programs with correct denotation z\u2208B p g \u03c8 (z | x)R(z, k, y)."}, {"id": 106, "string": "This basic training method fails for CNLVR (see Sec."}, {"id": 107, "string": "6), due to the difficulties of search and spuriousness."}, {"id": 108, "string": "Thus, we turn to learning from abstract examples, which substantially reduce these problems."}, {"id": 109, "string": "Learning from Abstract Examples The main premise of this work is that in closed, well-typed domains such as visual reasoning, the main challenge is handling language compositionality, since questions may have a complex and nested structure."}, {"id": 110, "string": "Conversely, the problem of mapping lexical items to functions and constants in the programming language can be substantially alleviated by taking advantage of the compact KB schema and typing system, and utilizing a Utterance Program Cluster # \"yellow\" IsYellow C-Color 3 \"big\" IsBig C-Size 3 \"square\" IsSquare C-Shape 4 \"3\" 3 C-Num 2 \"exactly\" EqualInt C-QuantMod 5 \"top\" Side.Top C-Location 2 \"above\" GetAbove C-SpaceRel 6 Total: 25 Table 3 : Example mappings from utterance tokens to program tokens for the seven clusters used in the abstract representation."}, {"id": 111, "string": "The rightmost column counts the number of mapping in each cluster, resulting in a total of 25 mappings."}, {"id": 112, "string": "small lexicon that maps prevalent lexical items into typed program constants."}, {"id": 113, "string": "Thus, if we abstract away from the actual utterance into a partially abstract representation, we can combat the search and spuriousness challenges as we can generalize better across examples in small datasets."}, {"id": 114, "string": "Consider the utterances: 1."}, {"id": 115, "string": "\"There are exactly 3 yellow squares touching the wall.\""}, {"id": 116, "string": "2."}, {"id": 117, "string": "\"There are at least 2 blue circles touching the wall.\""}, {"id": 118, "string": "While the surface forms of these utterances are different, at an abstract level they are similar and it would be useful to leverage this similarity."}, {"id": 119, "string": "We therefore define an abstract representation for utterances and logical forms that is suitable for spatial reasoning."}, {"id": 120, "string": "We define seven abstract clusters (see Table 3 ) that correspond to the main semantic types in our domain."}, {"id": 121, "string": "Then, we associate each cluster with a small lexicon that contains language-program token pairs associated with this cluster."}, {"id": 122, "string": "These mappings represent the canonical ways in which program constants are expressed in natural language."}, {"id": 123, "string": "Table 3 shows the seven clusters we use, with an example for an utterance-program token pair from the cluster, and the number of mappings in each cluster."}, {"id": 124, "string": "In total, 25 mappings are used to define abstract representations."}, {"id": 125, "string": "As we show next, abstract examples can be used to improve the process of training semantic parsers."}, {"id": 126, "string": "Specifically, in sections 5.1-5.3, we use abstract examples in several ways, from generating new training data to improving search accuracy."}, {"id": 127, "string": "The combined effect of these approaches is quite dramatic, as our evaluation demonstrates."}, {"id": 128, "string": "High Coverage via Abstract Examples We begin by demonstrating that abstraction leads to rather effective coverage of the types of questions asked in a dataset."}, {"id": 129, "string": "Namely, that many ques-tions in the data correspond to a small set of abstract examples."}, {"id": 130, "string": "We created abstract representations for all 3,163 utterances in the training examples by mapping utterance tokens to their cluster label, and then counted how many distinct abstract utterances exist."}, {"id": 131, "string": "We found that as few as 200 abstract utterances cover roughly half of the training examples in the original training set."}, {"id": 132, "string": "The above suggests that knowing how to answer a small set of abstract questions may already yield a reasonable baseline."}, {"id": 133, "string": "To test this baseline, we constructured a \"rule-based\" parser as follows."}, {"id": 134, "string": "We manually annotated 106 abstract utterances with their corresponding abstract program (including alignment between abstract tokens in the utterance and program)."}, {"id": 135, "string": "For example, Table 1 shows the abstract utterance and program for the utterance \"There are exactly 3 yellow squares touching the wall\"."}, {"id": 136, "string": "Note that the utterance \"There are at least 2 blue circles touching the wall\" will be mapped to the same abstract utterance and program."}, {"id": 137, "string": "Given this set of manual annotations, our rulebased semantic parser operates as follows: Given an utterance x, create its abstract representationx."}, {"id": 138, "string": "If it exactly matches one of the manually annotated utterances, map it to its corresponding abstract programz."}, {"id": 139, "string": "Replace the abstract program tokens with real program tokens based on the alignment with the utterance tokens, and obtain a final program z. Ifx does not match return TRUE, the majority label."}, {"id": 140, "string": "The rule-based parser will fail for examples not covered by the manual annotation."}, {"id": 141, "string": "However, it already provides a reasonable baseline (see Table 4 )."}, {"id": 142, "string": "As shown next, manual annotations can also be used for generating new training data."}, {"id": 143, "string": "Data Augmentation While the rule-based semantic parser has high precision and gauges the amount of structural variance in the data, it cannot generalize beyond observed examples."}, {"id": 144, "string": "However, we can automatically generate non-abstract utterance-program pairs from the manually annotated abstract pairs and train a semantic parser with strong supervision that can potentially generalize better."}, {"id": 145, "string": "E.g., consider the utterance \"There are exactly 3 yellow squares touching the wall\", whose abstract representation is given in Table 1 ."}, {"id": 146, "string": "It is clear that we can use this abstract pair to generate a program for a new utterance \"There are exactly 3 blue squares touching the wall\"."}, {"id": 147, "string": "This program will be identical Algorithm 1 Decoding with an Abstract Cache 1: procedure DECODE(x, y, C, D) 2: // C is a map where the key is an abstract utterance and the value is a pair (Z,R) of a list of abstract programs Z and their average rewardsR."}, {"id": 148, "string": "D is an integer."}, {"id": 149, "string": "3:x \u2190 Abstract utterance of x 4: A \u2190 D programs in C[x] with top reward values 5: B1 \u2190 compute beam of programs of length 1 6: for t = 2 ."}, {"id": 150, "string": "."}, {"id": 151, "string": "."}, {"id": 152, "string": "T do // Decode with cache 7: Bt \u2190 construct beam from Bt\u22121 8: At = truncate(A, t) 9: Bt.add(de-abstract(At)) 10: for z \u2208 BT do //Update cache 11: Update rewards in C[x] using (z, R(z, y)) 12: return BT \u222a de-abstract(A)."}, {"id": 153, "string": "to the program of the first utterance, with IsBlue replacing IsYellow."}, {"id": 154, "string": "More generally, we can sample any abstract example and instantiate the abstract clusters that appear in it by sampling pairs of utterance-program tokens for each abstract cluster."}, {"id": 155, "string": "Formally, this is equivalent to a synchronous context-free grammar (Chiang, 2005) that has a rule for generating each manually-annotated abstract utteranceprogram pair, and rules for synchronously generating utterance and program tokens from the seven clusters."}, {"id": 156, "string": "We generated 6,158 (x, z) examples using this method and trained a standard sequence to sequence parser by maximizing log p \u03b8 (z|x) in the model above."}, {"id": 157, "string": "Although these are generated from a small set of 106 abstract utterances, they can be used to learn a model with higher coverage and accuracy compared to the rule-based parser, as our evaluation demonstrates."}, {"id": 158, "string": "3 The resulting parser can be used as a standalone semantic parser."}, {"id": 159, "string": "However, it can also be used as an initialization point for the weakly-supervised semantic parser."}, {"id": 160, "string": "As we observe in Sec."}, {"id": 161, "string": "6, this results in further improvement in accuracy."}, {"id": 162, "string": "Caching Abstract Examples We now describe a caching mechanism that uses abstract examples to combat search and spuriousness when training from weak supervision."}, {"id": 163, "string": "As shown in Sec."}, {"id": 164, "string": "5.1, many utterances are identical at the abstract level."}, {"id": 165, "string": "Thus, a natural idea is to keep track at training time of abstract utteranceprogram pairs that resulted in a correct denotation, and use this information to direct the search procedure."}, {"id": 166, "string": "Concretely, we construct a cache C that maps abstract utterances to all abstract programs that were decoded by the model, and tracks the average reward obtained for those programs."}, {"id": 167, "string": "For every utterance x, after obtaining the final beam of programs, we add to the cache all abstract utteranceprogram pairs (x,z), and update their average reward (Alg."}, {"id": 168, "string": "1, line 10)."}, {"id": 169, "string": "To construct an abstract example (x,z) from an utterance-program pair (x, z) in the beam, we perform the following procedure."}, {"id": 170, "string": "First, we createx by replacing utterance tokens with their cluster label, as in the rule-based semantic parser."}, {"id": 171, "string": "Then, we go over every program token in z, and replace it with an abstract cluster if the utterance contains a token that is mapped to this program token according to the mappings from Table 3 ."}, {"id": 172, "string": "This also provides an alignment from abstract program tokens to abstract utterance tokens that is necessary when utilizing the cache."}, {"id": 173, "string": "We propose two variants for taking advantage of the cache C. Both are shown in Algorithm 1."}, {"id": 174, "string": "1."}, {"id": 175, "string": "Full program retrieval (Alg."}, {"id": 176, "string": "1, line 12): Given utterance x, construct an abstract utterancex, retrieve the top D abstract programs A from the cache, compute the de-abstracted programs Z using alignments from program tokens to utterance tokens, and add the D programs to the final beam."}, {"id": 177, "string": "2."}, {"id": 178, "string": "Program prefix retrieval (Alg."}, {"id": 179, "string": "1, line 9): Here, we additionally consider prefixes of abstract programs to the beam, to further guide the search process."}, {"id": 180, "string": "At each step t, let B t be the beam of decoded programs at step t. For every abstract programz \u2208 A add the de-abstracted prefix z 1:t to B t and expand B t+1 accordingly."}, {"id": 181, "string": "This allows the parser to potentially construct new programs that are not in the cache already."}, {"id": 182, "string": "This approach combats both spuriousness and the search challenge, because we add promising program prefixes to the beam that might have fallen off of it earlier."}, {"id": 183, "string": "Fig."}, {"id": 184, "string": "3 visualizes the caching mechanism."}, {"id": 185, "string": "A high-level overview of our entire approach for utilizing abstract examples at training time for both data augmentation and model training is given in Fig."}, {"id": 186, "string": "4 ."}, {"id": 187, "string": "Experimental Evaluation Model and Training Parameters The Bi-LSTM state dimension is 30."}, {"id": 188, "string": "The decoder has one hidden layer of dimension 50, that takes the  last 4 decoded tokens as input as well as encoder states."}, {"id": 189, "string": "Token embeddings are of dimension 12, beam size is 40 and D = 10 programs are used in Algorithm 1."}, {"id": 190, "string": "Word embeddings are initialized from CBOW (Mikolov et al., 2013) trained on the training data, and are then optimized end-toend."}, {"id": 191, "string": "In the weakly-supervised parser we encourage exploration with meritocratic gradient updates with \u03b2 = 0.5 (Guu et al., 2017) ."}, {"id": 192, "string": "In the weaklysupervised parser we warm-start the parameters with the supervised parser, as mentioned above."}, {"id": 193, "string": "For optimization, Adam is used (Kingma and Ba, 2014) ), with learning rate of 0.001, and mini-batch size of 8."}, {"id": 194, "string": "Pre-processing Because the number of utterances is relatively small for training a neural model, we take the following steps to reduce sparsity."}, {"id": 195, "string": "We lowercase all utterance tokens, and also use their lemmatized form."}, {"id": 196, "string": "We also use spelling correction to replace words that contain typos."}, {"id": 197, "string": "After pre-processing we replace every word that occurs less than 5 times with an UNK symbol."}, {"id": 198, "string": "Evaluation We evaluate on the public development and test sets of CNLVR as well as on the hidden test set."}, {"id": 199, "string": "The standard evaluation metric is accuracy, i.e., how many examples are correctly classified."}, {"id": 200, "string": "In addition, we report consistency, which is the proportion of utterances for which the decoded program has the correct denotation for all 4 images/KBs."}, {"id": 201, "string": "It captures whether a model consistently produces a correct answer."}, {"id": 202, "string": "when taking the KB as input, which is a maximum entropy classifier (MAXENT)."}, {"id": 203, "string": "For our models, we evaluate the following variants of our approach: \u2022 RULE: The rule-based parser from Sec."}, {"id": 204, "string": "5.1."}, {"id": 205, "string": "\u2022 SUP."}, {"id": 206, "string": ": The supervised semantic parser trained on augmented data as in Sec."}, {"id": 207, "string": "5.2 (5, 598 examples for training and 560 for validation)."}, {"id": 208, "string": "\u2022 WEAKSUP."}, {"id": 209, "string": ": Our full weakly-supervised semantic parser that uses abstract examples."}, {"id": 210, "string": "\u2022 +DISC: We add a discriminative re-ranker (Sec."}, {"id": 211, "string": "3) for both SUP."}, {"id": 212, "string": "and WEAKSUP."}, {"id": 213, "string": "Table 4 describes our main results."}, {"id": 214, "string": "Our weakly-supervised semantic parser with re-ranking (W.+DISC) obtains 84.0 accuracy and 65.0 consistency on the public test set and 82.5 accuracy and 63.9 on the hidden one, improving accuracy by 14.7 points compared to state-of-theart."}, {"id": 215, "string": "The accuracy of the rule-based parser (RULE) is less than 2 points below MAXENT, showing that a semantic parsing approach is very suitable for this task."}, {"id": 216, "string": "The supervised parser obtains better performance (especially in consistency), and with re-ranking reaches 76.6 accuracy, showing that generalizing from generated examples is better than memorizing manually-defined patterns."}, {"id": 217, "string": "Our weakly-supervised parser significantly improves over SUP., reaching an accuracy of 81.7 before reranking, and 84.0 after re-ranking (on the public test set)."}, {"id": 218, "string": "Consistency results show an even crisper trend of improvement across the models."}, {"id": 219, "string": "Main results Analysis We analyze our results by running multiple ablations of our best model W.+DISC on the development set."}, {"id": 220, "string": "To examine the overall impact of our procedure, we trained a weakly-supervised parser from scratch without pre-training a supervised parser nor using a cache, which amounts to a re-implementation of the RANDOMER algorithm (Guu et al., 2017) ."}, {"id": 221, "string": "We find that the algorithm is  unable to bootstrap in this challenging setup and obtains very low performance."}, {"id": 222, "string": "Next, we examined the importance of abstract examples, by pretraining only on examples that were manually annotated (utterances that match the 106 abstract patterns), but with no data augmentation or use of a cache (\u2212ABSTRACTION)."}, {"id": 223, "string": "This results in performance that is similar to the MAJORITY baseline."}, {"id": 224, "string": "To further examine the importance of abstraction, we decoupled the two contributions, training once with a cache but without data augmentation for pre-training (\u2212DATAAUGMENTATION), and again with pre-training over the augmented data, but without the cache (\u2212BEAMCACHE)."}, {"id": 225, "string": "We found that the former improves by a few points over the MAXENT baseline, and the latter performs comparably to the supervised parser, that is, we are still unable to improve learning by training from denotations."}, {"id": 226, "string": "Lastly, we use a beam cache without line 9 in Alg."}, {"id": 227, "string": "1 (\u2212EVERYSTEPBEAMCACHE)."}, {"id": 228, "string": "This already results in good performance, substantially higher than SUP."}, {"id": 229, "string": "but is still 3.4 points worse than our best performing model on the development set."}, {"id": 230, "string": "Orthogonally, to analyze the importance of tying the reward of all four examples that share an utterance, we trained a model without this tying, where the reward is 1 iff the denotation is correct (ONEEXAMPLEREWARD)."}, {"id": 231, "string": "We find that spuriousness becomes a major issue and weaklysupervised learning fails."}, {"id": 232, "string": "Error Analysis We sampled 50 consistent and 50 inconsistent programs from the development set to analyze the weaknesses of our model."}, {"id": 233, "string": "By and large, errors correspond to utterances that are more complex syntactically and semantically."}, {"id": 234, "string": "In about half of the errors an object was described by two or more modifying clauses: \"there is a box with a yellow circle and three blue items\"; or nesting occurred: \"one of the gray boxes has exactly three objects one of which is a circle\"."}, {"id": 235, "string": "In these cases the model either ignored one of the conditions, resulting in a program equivalent to \"there is a box with three blue items\" for the first case, or applied composition operators wrongly, outputting an equivalent to \"one of the gray boxes has exactly three circles\" for the second case."}, {"id": 236, "string": "However, in some cases the parser succeeds on such examples and we found that 12% of the sampled utterances that were parsed correctly had a similar complex structure."}, {"id": 237, "string": "Other, less frequent reasons for failure were problems with cardinality interpretation, i.e."}, {"id": 238, "string": ",\"there are 2\" parsed as \"exactly 2\" instead of \"at least 2\"; applying conditions to items rather than sets, e.g., \"there are 2 boxes with a triangle closely touching a corner\" parsed as \"there are 2 triangles closely touching a corner\"; and utterances with questionable phrasing, e.g., \"there is a tower that has three the same blocks color\"."}, {"id": 239, "string": "Other insights are that the algorithm tended to give higher probability to the top ranked program when it is correct (average probability 0.18), compared to cases when it is incorrect (average probability 0.08), indicating that probabilities are correlated with confidence."}, {"id": 240, "string": "In addition, sentence length is not predictive for whether the model will succeed: average sentence length of an utterance is 10.9 when the model is correct, and 11.1 when it errs."}, {"id": 241, "string": "We also note that the model was successful with sentences that deal with spatial relations, but struggled with sentences that refer to the size of shapes."}, {"id": 242, "string": "This is due to the data distribution, which includes many examples of the former case and fewer examples of the latter."}, {"id": 243, "string": "Related Work Training semantic parsers from denotations has been one of the most popular training schemes for scaling semantic parsers since the beginning of the decade."}, {"id": 244, "string": "Early work focused on traditional log-linear models (Clarke et al., 2010; Liang et al., 2011; Kwiatkowski et al., 2013) , but recently denotations have been used to train neural semantic parsers Krishnamurthy et al., 2017; Rabinovich et al., 2017; Cheng et al., 2017) ."}, {"id": 245, "string": "Visual reasoning has attracted considerable attention, with datasets such as VQA (Antol et al., 2015) and CLEVR (Johnson et al., 2017a) ."}, {"id": 246, "string": "The advantage of CNLVR is that language utterances are both natural and compositional."}, {"id": 247, "string": "Treating vi-sual reasoning as an end-to-end semantic parsing problem has been previously done on CLEVR (Hu et al., 2017; Johnson et al., 2017b) ."}, {"id": 248, "string": "Our method for generating training data resembles data re-combination ideas in Jia and Liang (2016) , where examples are generated automatically by replacing entities with their categories."}, {"id": 249, "string": "While spuriousness is central to semantic parsing when denotations are not very informative, there has been relatively little work on explicitly tackling it."}, {"id": 250, "string": "Pasupat and Liang (2015) used manual rules to prune unlikely programs on the WIK-ITABLEQUESTIONS dataset, and then later utilized crowdsourcing (Pasupat and Liang, 2016) to eliminate spurious programs."}, {"id": 251, "string": "Guu et al."}, {"id": 252, "string": "(2017) proposed RANDOMER, a method for increasing exploration and handling spuriousness by adding randomness to beam search and a proposing a \"meritocratic\" weighting scheme for gradients."}, {"id": 253, "string": "In our work we found that random exploration during beam search did not improve results while meritocratic updates slightly improved performance."}, {"id": 254, "string": "Discussion In this work we presented the first semantic parser for the CNLVR dataset, taking structured representations as input."}, {"id": 255, "string": "Our main insight is that in closed, well-typed domains we can generate abstract examples that can help combat the difficulties of training a parser from delayed supervision."}, {"id": 256, "string": "First, we use abstract examples to semiautomatically generate utterance-program pairs that help warm-start our parameters, thereby reducing the difficult search challenge of finding correct programs with random parameters."}, {"id": 257, "string": "Second, we focus on an abstract representation of examples, which allows us to tackle spuriousness and alleviate search, by sharing information about promising programs between different examples."}, {"id": 258, "string": "Our approach dramatically improves performance on CNLVR, establishing a new state-of-the-art."}, {"id": 259, "string": "In this paper, we used a manually-built highprecision lexicon to construct abstract examples."}, {"id": 260, "string": "This is suitable for well-typed domains, which are ubiquitous in the virtual assistant use case."}, {"id": 261, "string": "In future work we plan to extend this work and automatically learn such a lexicon."}, {"id": 262, "string": "This can reduce manual effort and scale to larger domains where there is substantial variability on the language side."}], "headers": [{"section": "Introduction", "n": "1", "start": 0, "end": 32}, {"section": "Setup", "n": "2", "start": 33, "end": 56}, {"section": "Model", "n": "3", "start": 57, "end": 91}, {"section": "Training", "n": "4", "start": 92, "end": 108}, {"section": "Learning from Abstract Examples", "n": "5", "start": 109, "end": 127}, {"section": "High Coverage via Abstract Examples", "n": "5.1", "start": 128, "end": 142}, {"section": "Data Augmentation", "n": "5.2", "start": 143, "end": 161}, {"section": "Caching Abstract Examples", "n": "5.3", "start": 162, "end": 186}, {"section": "Experimental Evaluation", "n": "6", "start": 187, "end": 218}, {"section": "Analysis", "n": "6.1", "start": 219, "end": 242}, {"section": "Related Work", "n": "7", "start": 243, "end": 253}, {"section": "Discussion", "n": "8", "start": 254, "end": 262}], "figures": [{"filename": "../figure/image/1363-Figure4-1.png", "caption": "Figure 4: An overview of our approach for utilizing abstract examples for data augmentation and model training.", "page": 6, "bbox": {"x1": 81.6, "x2": 515.04, "y1": 316.8, "y2": 466.08}}, {"filename": "../figure/image/1363-Figure3-1.png", "caption": "Figure 3: A visualization of the caching mechanism. At each decoding step, prefixes of high-reward abstract programs are added to the beam from the cache.", "page": 6, "bbox": {"x1": 112.8, "x2": 484.32, "y1": 61.44, "y2": 271.2}}, {"filename": "../figure/image/1363-Table1-1.png", "caption": "Table 1: An example for an utterance-program pair (x, z) and its abstract counterpart (x\u0304, z\u0304)", "page": 2, "bbox": {"x1": 72.96, "x2": 526.0799999999999, "y1": 63.36, "y2": 92.64}}, {"filename": "../figure/image/1363-Table2-1.png", "caption": "Table 2: Examples for utterance-program pairs. Commas and parenthesis provided for readability only.", "page": 2, "bbox": {"x1": 72.96, "x2": 526.0799999999999, "y1": 126.24, "y2": 158.4}}, {"filename": "../figure/image/1363-Table4-1.png", "caption": "Table 4: Results on the development, public test (Test-P) and hidden test (Test-H) sets. For each model, we report both accuracy and consistency.", "page": 7, "bbox": {"x1": 81.6, "x2": 281.28, "y1": 61.44, "y2": 136.32}}, {"filename": "../figure/image/1363-Table5-1.png", "caption": "Table 5: Results of ablations of our main models on the development set. Explanation for the nature of the models is in the body of the paper.", "page": 7, "bbox": {"x1": 327.84, "x2": 505.44, "y1": 61.44, "y2": 144.0}}, {"filename": "../figure/image/1363-Figure2-1.png", "caption": "Figure 2: An example for the state of the type stack s while decoding a program z for an utterance x.", "page": 3, "bbox": {"x1": 98.39999999999999, "x2": 499.2, "y1": 65.75999999999999, "y2": 110.39999999999999}}, {"filename": "../figure/image/1363-Table3-1.png", "caption": "Table 3: Example mappings from utterance tokens to program tokens for the seven clusters used in the abstract representation. The rightmost column counts the number of mapping in each cluster, resulting in a total of 25 mappings.", "page": 4, "bbox": {"x1": 100.8, "x2": 261.12, "y1": 61.44, "y2": 138.24}}]}