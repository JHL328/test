{"title": "Simple and Effective Multi-Paragraph Reading Comprehension", "abstract": "We introduce a method of adapting neural paragraph-level question answering models to the case where entire documents are given as input. Most current question answering models cannot scale to document or multi-document input, and naively applying these models to each paragraph independently often results in them being distracted by irrelevant text. We show that it is possible to significantly improve performance by using a modified training scheme that teaches the model to ignore non-answer containing paragraphs. Our method involves sampling multiple paragraphs from each document, and using an objective function that requires the model to produce globally correct output. We additionally identify and improve upon a number of other design decisions that arise when working with document-level data. Experiments on TriviaQA and SQuAD shows our method advances the state of the art, including a 10 point gain on TriviaQA.", "text": [{"id": 0, "string": "Introduction Teaching machines to answer arbitrary usergenerated questions is a long-term goal of natural language processing."}, {"id": 1, "string": "For a wide range of questions, existing information retrieval methods are capable of locating documents that are likely to contain the answer."}, {"id": 2, "string": "However, automatically extracting the answer from those texts remains an open challenge."}, {"id": 3, "string": "The recent success of neural models at answering questions given a related paragraph (Wang et al., 2017c; Tan et al., 2017) suggests they have the potential to be a key part of * Work completed while interning at the Allen Institute for Artificial Intelligence a solution to this problem."}, {"id": 4, "string": "Most neural models are unable to scale beyond short paragraphs, so typically this requires adapting a paragraph-level model to process document-level input."}, {"id": 5, "string": "There are two basic approaches to this task."}, {"id": 6, "string": "Pipelined approaches select a single paragraph from the input documents, which is then passed to the paragraph model to extract an answer (Joshi et al., 2017; Wang et al., 2017a) ."}, {"id": 7, "string": "Confidence based methods apply the model to multiple paragraphs and return the answer with the highest confidence (Chen et al., 2017a) ."}, {"id": 8, "string": "Confidence methods have the advantage of being robust to errors in the (usually less sophisticated) paragraph selection step, however they require a model that can produce accurate confidence scores for each paragraph."}, {"id": 9, "string": "As we shall show, naively trained models often struggle to meet this requirement."}, {"id": 10, "string": "In this paper we start by proposing an improved pipelined method which achieves state-of-the-art results."}, {"id": 11, "string": "Then we introduce a method for training models to produce accurate per-paragraph confidence scores, and we show how combining this method with multiple paragraph selection further increases performance."}, {"id": 12, "string": "Our pipelined method focuses on addressing the challenges that come with training on documentlevel data."}, {"id": 13, "string": "We use a linear classifier to select which paragraphs to train and test on."}, {"id": 14, "string": "Since annotating entire documents is expensive, data of this sort is typically distantly supervised, meaning only the answer text, not the answer spans, are known."}, {"id": 15, "string": "To handle the noise this creates, we use a summed objective function that marginalizes the model's output over all locations the answer text occurs."}, {"id": 16, "string": "We apply this approach with a model design that integrates some recent ideas in reading comprehension models, including selfattention (Cheng et al., 2016) and bi-directional attention (Seo et al., 2016) ."}, {"id": 17, "string": "Our confidence method extends this approach to better handle the multi-paragraph setting."}, {"id": 18, "string": "Previous approaches trained the model on questions paired with paragraphs that are known a priori to contain the answer."}, {"id": 19, "string": "This has several downsides: the model is not trained to produce low confidence scores for paragraphs that do not contain an answer, and the training objective does not require confidence scores to be comparable between paragraphs."}, {"id": 20, "string": "We resolve these problems by sampling paragraphs from the context documents, including paragraphs that do not contain an answer, to train on."}, {"id": 21, "string": "We then use a shared-normalization objective where paragraphs are processed independently, but the probability of an answer candidate is marginalized over all paragraphs sampled from the same document."}, {"id": 22, "string": "This requires the model to produce globally correct output even though each paragraph is processed independently."}, {"id": 23, "string": "We evaluate our work on TriviaQA (Joshi et al., 2017) in the wiki, web, and unfiltered setting."}, {"id": 24, "string": "Our model achieves a nearly 10 point lead over published prior work."}, {"id": 25, "string": "We additionally perform an ablation study on our pipelined method, and we show the effectiveness of our multi-paragraph methods on a modified version of SQuAD (Rajpurkar et al., 2016) where only the correct document, not the correct paragraph, is known."}, {"id": 26, "string": "Finally, we combine our model with a web search backend to build a demonstration end-to-end QA system 1 , and show it performs well on questions from the TREC question answering task (Voorhees et al., 1999) ."}, {"id": 27, "string": "We release our code 2 to facilitate future work."}, {"id": 28, "string": "Pipelined Method In this section we propose a pipelined QA system, where a single paragraph is selected and passed to a paragraph-level question answering model."}, {"id": 29, "string": "Paragraph Selection If there is a single source document, we select the paragraph with the smallest TF-IDF cosine distance with the question."}, {"id": 30, "string": "Document frequencies are computed using the individual paragraphs within the document."}, {"id": 31, "string": "If there are multiple input documents, we found it beneficial to use a linear classifier that uses the same TF-IDF score, whether the paragraph was the first in its document, how many tokens preceded it, and the number of question words it includes as features."}, {"id": 32, "string": "The classifier is trained on the distantly supervised objective of selecting paragraphs that contain at least one answer span."}, {"id": 33, "string": "On TriviaQA web, relative to truncating the document as done by prior work, this improves the chance of the selected text containing the correct answer from 83.1% to 85.1%."}, {"id": 34, "string": "Handling Noisy Labels Question: Which British general was killed at Khartoum in 1885?"}, {"id": 35, "string": "Answer: Gordon Context: In February 1885 Gordon returned to the Sudan to evacuate Egyptian forces."}, {"id": 36, "string": "Khartoum came under siege the next month and rebels broke into the city, killing Gordon and the other defenders."}, {"id": 37, "string": "The British public reacted to his death by acclaiming 'Gordon of Khartoum', a saint."}, {"id": 38, "string": "However, historians have suggested that Gordon..."}, {"id": 39, "string": "Figure 1 : Noisy supervision can cause many spans of text that contain the answer, but are not situated in a context that relates to the question (red), to distract the model from learning from more relevant spans (green)."}, {"id": 40, "string": "In a distantly supervised setup we label all text spans that match the answer text as being correct."}, {"id": 41, "string": "This can lead to training the model to select unwanted answer spans."}, {"id": 42, "string": "Figure 1 contains an example."}, {"id": 43, "string": "To handle this difficulty, we use a summed objective function similar to the one from Kadlec et al."}, {"id": 44, "string": "(2016) , that optimizes the negative loglikelihood of selecting any correct answer span."}, {"id": 45, "string": "The models we consider here work by independently predicting the start and end token of the answer span, so we take this approach for both predictions."}, {"id": 46, "string": "For example, the objective for predicting the answer start token becomes \u2212 log a\u2208A p a where A is the set of tokens that start an answer and p i is the answer-start probability predicted by the model for token i."}, {"id": 47, "string": "This objective has the advantage of being agnostic to how the model distributes probability mass across the possible answer spans, allowing the model to focus on only the most relevant spans."}, {"id": 48, "string": "Model We use a model with the following layers (shown in Figure 2 ): Embedding: We embed words using pretrained word vectors."}, {"id": 49, "string": "We concatenate these with character-derived word embeddings, which are produced by embedding characters using a learned embedding matrix and then applying a convolutional neural network and max-pooling."}, {"id": 50, "string": "Pre-Process: A shared bi-directional GRU (Cho et al., 2014) is used to process the question and passage embeddings."}, {"id": 51, "string": "Attention: The attention mechanism from the Bi-Directional Attention Flow (BiDAF) model (Seo et al., 2016) is used to build a queryaware context representation."}, {"id": 52, "string": "Let h i and q j be the vector for context word i and question word j, and n q and n c be the lengths of the question and context respectively."}, {"id": 53, "string": "We compute attention between context word i and question word j as: a ij = w 1 \u00b7 h i + w 2 \u00b7 q j + w 3 \u00b7 (h i q j ) where w 1 , w 2 , and w 3 are learned vectors and is element-wise multiplication."}, {"id": 54, "string": "We then compute an attended vector c i for each context token as: p ij = e a ij nq j=1 e a ij c i = nq j=1 q j p ij We also compute a query-to-context vector q c : m i = max 1\u2264j\u2264nq a ij p i = e m i nc i=1 e m i q c = nc i=1 h i p i The final vector for each token is built by concatenating h i , c i , h i c i , and q c c i ."}, {"id": 55, "string": "In our model we subsequently pass the result through a linear layer with ReLU activations."}, {"id": 56, "string": "Self-Attention: Next we use a layer of residual self-attention."}, {"id": 57, "string": "The input is passed through another bi-directional GRU."}, {"id": 58, "string": "Then we apply the same attention mechanism, only now between the passage and itself."}, {"id": 59, "string": "In this case we do not use query-tocontext attention and we set a ij = \u2212inf if i = j."}, {"id": 60, "string": "As before, we pass the concatenated output through a linear layer with ReLU activations."}, {"id": 61, "string": "The result is then summed with the original input."}, {"id": 62, "string": "Prediction: In the last layer of our model a bidirectional GRU is applied, followed by a linear layer to compute answer start scores for each token."}, {"id": 63, "string": "The hidden states are concatenated with the input and fed into a second bi-directional GRU and linear layer to predict answer end scores."}, {"id": 64, "string": "The softmax function is applied to the start and end scores to produce answer start and end probabilities."}, {"id": 65, "string": "Dropout: We apply variational dropout (Gal and Ghahramani, 2016) to the input to all the GRUs and the input to the attention mechanisms at a rate of 0.2."}, {"id": 66, "string": "Confidence Method We adapt this model to the multi-paragraph setting by using the un-normalized and un-exponentiated (i.e., before the softmax operator is applied) score given to each span as a measure of the model's confidence."}, {"id": 67, "string": "For the boundary-based models we use here, a span's score is the sum of the start and end score given to its start and end token."}, {"id": 68, "string": "At test time we run the model on each paragraph and select the answer span with the highest confidence."}, {"id": 69, "string": "This is the approach taken by Chen et al."}, {"id": 70, "string": "(2017a) ."}, {"id": 71, "string": "Our experiments in Section 5 show that these confidence scores can be very poor if the model is only trained on answer-containing paragraphs, as done by prior work."}, {"id": 72, "string": "Table 1 contains some qualitative examples of the errors that occur."}, {"id": 73, "string": "We hypothesize that there are two key sources of error."}, {"id": 74, "string": "First, for models trained with the softmax objective, the pre-softmax scores for all spans can be arbitrarily increased or decreased by a constant value without changing the resulting softmax probability distribution."}, {"id": 75, "string": "As a result, nothing prevents models from producing scores that are arbitrarily all larger or all smaller for one paragraph ...one 2001 study finding a quarter square kilometer (62 acres) of Ecuadorian rainforest supports more than 1,100 tree species The affected region was approximately 1,160,000 square miles (3,000,000 km2) of rainforest, compared to 734,000 square miles Who was Warsz?"}, {"id": 76, "string": "....In actuality, Warsz was a 12th/13th century nobleman who owned a village located at the modern.... One of the most famous people born in Warsaw was Maria Sklodowska -Curie, who achieved international... How much did the initial LM weight in kg?"}, {"id": 77, "string": "The initial LM model weighed approximately 33,300 pounds (15,000 kg), and..."}, {"id": 78, "string": "The module was 11.42 feet (3.48 m) tall, and weighed approximately 12,250 pounds (5,560 kg) Table 1 : Examples from SQuAD where a model was less confident in a correct extraction from one paragraph (left) than in an incorrect extraction from another (right)."}, {"id": 79, "string": "Even if the passage has no correct answer and does not contain any question words, the model assigns high confidence to phrases that match the category the question is asking about."}, {"id": 80, "string": "Because the confidence scores are not well-calibrated, this confidence is often higher than the confidence assigned to correct answer spans in different paragraphs, even when those correct spans have better contextual evidence."}, {"id": 81, "string": "than another."}, {"id": 82, "string": "Second, if the model only sees paragraphs that contain answers, it might become too confident in heuristics or patterns that are only effective when it is known a priori that an answer exists."}, {"id": 83, "string": "For example, the model might become too reliant on selecting answers that match semantic type the question is asking about, causing it be easily distracted by other entities of that type when they appear in irrelevant text."}, {"id": 84, "string": "This kind of error has also been observed when distractor sentences are added to the context (Jia and Liang, 2017) We experiment with four approaches to training models to produce comparable confidence scores, shown in the following subsections."}, {"id": 85, "string": "In all cases we will sample paragraphs that do not contain an answer as additional training points."}, {"id": 86, "string": "Shared-Normalization In this approach a modified objective function is used where span start and end scores are normalized across all paragraphs sampled from the same context."}, {"id": 87, "string": "This means that paragraphs from the same context use a shared normalization factor in the final softmax operations."}, {"id": 88, "string": "We train on this objective by including multiple paragraphs from the same context in each mini-batch."}, {"id": 89, "string": "The key idea is that this will force the model to produce scores that are comparable between paragraphs, even though it does not have access to information about what other paragraphs are being considered."}, {"id": 90, "string": "Merge As an alternative to the previous method, we experiment with concatenating all paragraphs sam-pled from the same context together during training."}, {"id": 91, "string": "A paragraph separator token with a learned embedding is added before each paragraph."}, {"id": 92, "string": "No-Answer Option We also experiment with allowing the model to select a special \"no-answer\" option for each paragraph."}, {"id": 93, "string": "First we re-write our objective as: \u2212 log e sa n i=1 e s i \u2212 log e g b n j=1 e g j = \u2212 log e sa+g b n i=1 n j=1 e s i +g j where s j and g j are the scores for the start and end bounds produced by the model for token j, and a and b are the correct start and end tokens."}, {"id": 94, "string": "We have the model compute another score, z, to represent the weight given to a \"no-answer\" possibility."}, {"id": 95, "string": "Our revised objective function becomes: \u2212 log (1 \u2212 \u03b4)e z + \u03b4e sa+g b e z + n i=1 n j=1 e s i +g j where \u03b4 is 1 if an answer exists and 0 otherwise."}, {"id": 96, "string": "If there are multiple answer spans we use the same objective, except the numerator includes the summation over all answer start and end tokens."}, {"id": 97, "string": "We compute z by adding an extra layer at the end of our model."}, {"id": 98, "string": "We build input vectors by taking the summed hidden states of the RNNs used to predict the start/end token scores weighed by the start/end probabilities, and using a learned attention vector on the output of the self-attention layer."}, {"id": 99, "string": "These vectors are fed into a two layer network with an 80 dimensional hidden layer and ReLU activations that produces z as its only output."}, {"id": 100, "string": "Sigmoid As a final baseline, we consider training models with the sigmoid loss objective function."}, {"id": 101, "string": "That is, we compute a start/end probability for each token by applying the sigmoid function to the start/end scores of each token."}, {"id": 102, "string": "A cross entropy loss is used on each individual probability."}, {"id": 103, "string": "The intuition is that, since the scores are being evaluated independently of one another, they are more likely to be comparable between different paragraphs."}, {"id": 104, "string": "Experimental Setup Datasets We evaluate our approach on four datasets: Triv-iaQA unfiltered (Joshi et al., 2017) , a dataset of questions from trivia databases paired with documents found by completing a web search of the questions; TriviaQA wiki, the same dataset but only including Wikipedia articles; TriviaQA web, a dataset derived from TriviaQA unfiltered by treating each question-document pair where the document contains the question answer as an individual training point; and SQuAD (Rajpurkar et al., 2016) , a collection of Wikipedia articles and crowdsourced questions."}, {"id": 105, "string": "Preprocessing We note that for TriviaQA web we do not subsample as was done by Joshi et al."}, {"id": 106, "string": "(2017) , instead training on the all 530k training examples."}, {"id": 107, "string": "We also observe that TriviaQA documents often contain many small paragraphs, so we restructure the documents by merging consecutive paragraphs together up to a target size."}, {"id": 108, "string": "We use a maximum paragraph size of 400 unless stated otherwise."}, {"id": 109, "string": "Paragraph separator tokens with learned embeddings are added between merged paragraphs to preserve formatting information."}, {"id": 110, "string": "We are also careful to mark all spans of text that would be considered an exact match by the official evaluation script, which includes some minor text pre-processing, as answer spans, not just spans that are an exact string match with the answer text."}, {"id": 111, "string": "Sampling Our confidence-based approaches are trained by sampling paragraphs from the context during training."}, {"id": 112, "string": "For SQuAD and TriviaQA web we take Model EM F1 baseline (Joshi et al., 2017) the top four paragraphs as judged by our paragraph ranking function (see Section 2.1)."}, {"id": 113, "string": "We sample two different paragraphs from those four each epoch to train on."}, {"id": 114, "string": "Since we observe that the higherranked paragraphs are more likely to contain the context needed to answer the question, we sample the highest ranked paragraph that contains an answer twice as often as the others."}, {"id": 115, "string": "For the merge and shared-norm approaches, we additionally require that at least one of the paragraphs contains an answer span, and both of those paragraphs are included in the same mini-batch."}, {"id": 116, "string": "For TriviaQA wiki we repeat the process but use the top 8 paragraphs, and for TriviaQA unfiltered we use the top 16, because much more context is given in these settings."}, {"id": 117, "string": "Implementation We train the model with the Adadelta optimizer (Zeiler, 2012) with a batch size 60 for Triv-iaQA and 45 for SQuAD."}, {"id": 118, "string": "At test time we select the most probable answer span of length less than or equal to 8 for TriviaQA and 17 for SQuAD."}, {"id": 119, "string": "The GloVe 300 dimensional word vectors released by Pennington et al."}, {"id": 120, "string": "(2014) are used for word embeddings."}, {"id": 121, "string": "On SQuAD, we use a dimensionality of size 100 for the GRUs and of size 200 for the linear layers employed after each attention mechanism."}, {"id": 122, "string": "We found for TriviaQA, likely because there is more data, using a larger dimensionality of 140 for each GRU and 280 for the linear layers is beneficial."}, {"id": 123, "string": "During training, we maintain an exponential moving average of the weights with a decay rate of 0.999."}, {"id": 124, "string": "We use the weight averages at test time."}, {"id": 125, "string": "We do not update the word vectors during training."}, {"id": 126, "string": "Results TriviaQA Web and TriviaQA Wiki First, we do an ablation study on TriviaQA web to show the effects of our proposed methods for our pipeline model."}, {"id": 127, "string": "We start with a baseline following the one used by Joshi et al."}, {"id": 128, "string": "(2017) ."}, {"id": 129, "string": "This   system uses BiDAF (Seo et al., 2016) as the paragraph model, and selects a random answer span from each paragraph each epoch to train on."}, {"id": 130, "string": "The first 400 tokens of each document are used during training, and the first 800 during testing."}, {"id": 131, "string": "When using the TF-IDF paragraph selection approach, we instead break the documents into paragraphs of size 400 when training and 800 when testing, and select the top-ranked paragraph to feed into the model."}, {"id": 132, "string": "As shown in Table 2 , our baseline outperforms the results reported by Joshi et al."}, {"id": 133, "string": "(2017) significantly, likely because we are not subsampling the data."}, {"id": 134, "string": "We find both TF-IDF ranking and the sum objective to be effective."}, {"id": 135, "string": "Using our refined model increases the gain by another 4 points."}, {"id": 136, "string": "Next we show the results of our confidencebased approaches."}, {"id": 137, "string": "For this comparison we split documents into paragraphs of at most 400 tokens, and rank them using TF-IDF cosine distance."}, {"id": 138, "string": "Then we measure the performance of our proposed approaches as the model is used to independently process an increasing number of these paragraphs, and the highest confidence answer is selected as the final output."}, {"id": 139, "string": "The results are shown in Figure 3 ."}, {"id": 140, "string": "On this dataset even the model trained without any of the proposed training methods (\"none\") im- Figure 4 : Results for our confidence methods on TriviaQA unfiltered."}, {"id": 141, "string": "The shared-norm approach is the strongest, while the baseline model starts to lose performance as more paragraphs are used."}, {"id": 142, "string": "proves as more paragraphs are used, showing it does a passable job at focusing on the correct paragraph."}, {"id": 143, "string": "The no-answer option training approach lead to a significant improvement, and the sharednorm and merge approaches are even better."}, {"id": 144, "string": "We use the shared-norm approach for evaluation on the TriviaQA test sets."}, {"id": 145, "string": "We found that increasing the paragraph size to 800 at test time, and to 600 during training, was slightly beneficial, allowing our model to reach 66.04 EM and 70.98 F1 on the dev set."}, {"id": 146, "string": "As shown in Table 3 , our model is firmly ahead of prior work on both the TriviaQA web and TriviaQA wiki test sets."}, {"id": 147, "string": "Since our submission, a few additional entries have been added to the public leader for this dataset 5 , although to the best of our knowledge these results have not yet been published."}, {"id": 148, "string": "TriviaQA Unfiltered Next we apply our confidence methods to Trivi-aQA unfiltered."}, {"id": 149, "string": "This dataset is of particular interest because the system is not told which document contains the answer, so it provides a plausible simulation of answering a question using a document Figure 5 : Results for our confidence methods on document-level SQuAD."}, {"id": 150, "string": "The shared-norm model is the only model that does not lose performance when exposed to large numbers of paragraphs."}, {"id": 151, "string": "retrieval system."}, {"id": 152, "string": "We show the same graph as before for this dataset in Figure 4 ."}, {"id": 153, "string": "Our methods have an even larger impact on this dataset, probably because there are many more relevant and irrelevant paragraphs for each question, making paragraph selection more important."}, {"id": 154, "string": "Note the naively trained model starts to lose performance as more paragraphs are used, showing that errors are being caused by the model being overly confident in incorrect extractions."}, {"id": 155, "string": "We achieve a score of 61.55 EM and 67.61 F1 on the dev set."}, {"id": 156, "string": "This advances the only prior result reported for this dataset, 50.6 EM and 57.3 F1 from Wang et al."}, {"id": 157, "string": "(2017b) , by 10 points."}, {"id": 158, "string": "SQuAD We additionally evaluate our model on SQuAD."}, {"id": 159, "string": "SQuAD questions were not built to be answered independently of their context paragraph, which makes it unclear how effective of an evaluation tool they can be for document-level question answering."}, {"id": 160, "string": "To assess this we manually label 500 random questions from the training set."}, {"id": 161, "string": "We categorize questions as: 1."}, {"id": 162, "string": "Context-independent, meaning it can be understood independently of the paragraph."}, {"id": 163, "string": "2."}, {"id": 164, "string": "Document-dependent, meaning it can be understood given the article's title."}, {"id": 165, "string": "For example, \"What individual is the school named after?\""}, {"id": 166, "string": "for the document \"Harvard University\"."}, {"id": 167, "string": "3."}, {"id": 168, "string": "Paragraph-dependent, meaning it can only be understood given its paragraph."}, {"id": 169, "string": "For example, \"What was the first step in the reforms?\"."}, {"id": 170, "string": "We find 67.4% of the questions to be contextindependent, 22.6% to be document-dependent, and the remaining 10% to be paragraphdependent."}, {"id": 171, "string": "There are many document-dependent questions because questions are frequently about the subject of the document."}, {"id": 172, "string": "Since a reasonably high fraction of the questions can be understood given the document they are from, and to isolate our analysis from the retrieval mechanism used, we choose to evaluate on the document-level."}, {"id": 173, "string": "We build documents by concatenating all the paragraphs in SQuAD from the same article together into a single document."}, {"id": 174, "string": "Given the correct paragraph (i.e., in the standard SQuAD setting) our model reaches 72.14 EM and 81.05 F1 and can complete 26 epochs of training in less than five hours."}, {"id": 175, "string": "Most of our variations to handle the multi-paragraph setting caused a minor (up to half a point) drop in performance, while the sigmoid version fell behind by a point and a half."}, {"id": 176, "string": "We graph the document-level performance in Figure 5 ."}, {"id": 177, "string": "For SQuAD, we find it crucial to employ one of the suggested confidence training techniques."}, {"id": 178, "string": "The base model starts to drop in performance once more than two paragraphs are used."}, {"id": 179, "string": "However, the shared-norm approach is able to reach a peak performance of 72.37 F1 and 64.08 EM given 15 paragraphs."}, {"id": 180, "string": "Given our estimate that 10% of the questions are ambiguous if the paragraph is unknown, our approach appears to have adapted to the document-level task very well."}, {"id": 181, "string": "Finally, we compare the shared-norm model with the document-level result reported by Chen et al."}, {"id": 182, "string": "(2017a) ."}, {"id": 183, "string": "We re-evaluate our model using the documents used by Chen et al."}, {"id": 184, "string": "(2017a) , which consist of the same Wikipedia articles SQuAD was built from, but downloaded at different dates."}, {"id": 185, "string": "The advantage of this dataset is that it does not allow the model to know a priori which paragraphs were filtered out during the construction of SQuAD."}, {"id": 186, "string": "The disadvantage is that some of the articles have been edited since the questions were written, so some questions may no longer be answerable."}, {"id": 187, "string": "Our model achieves 59.14 EM and 67.34 F1 on this dataset, which significantly outperforms the 49.7 EM reported by Chen et al."}, {"id": 188, "string": "(2017a) ."}, {"id": 189, "string": "Curated TREC We perform one final experiment that tests our model as part of an end-to-end question answering system."}, {"id": 190, "string": "For document retrieval, we re-implement the pipeline from Joshi et al."}, {"id": 191, "string": "(2017) ."}, {"id": 192, "string": "Given a question, we retrieve up to 10 web documents us-Model Accuracy S-Norm (ours) 53.31 YodaQA with Bing (Baudi\u0161, 2015) , 37.18 YodaQA (Baudi\u0161, 2015) , 34.26 DrQA + DS (Chen et al., 2017a) 25.7 Table 4 : Results on the Curated TREC corpus, Yo-daQA results extracted from its github page 7 ing a Bing web search of the question, and all Wikipedia articles about entities the entity linker TAGME (Ferragina and Scaiella, 2010) identifies in the question."}, {"id": 193, "string": "We then use our linear paragraph ranker to select the 16 most relevant paragraphs from all these documents, which are passed to our model to locate the final answer span."}, {"id": 194, "string": "We choose to use the shared-norm model trained on the TriviaQA unfiltered dataset since it is trained using multiple web documents as input."}, {"id": 195, "string": "We use the same heuristics as Joshi et al."}, {"id": 196, "string": "(2017) to filter out trivia or QA websites to ensure questions cannot be trivially answered using webpages that directly address the question."}, {"id": 197, "string": "A demo of the system is publicly available 8 ."}, {"id": 198, "string": "We find accuracy on the TriviaQA unfiltered questions remains almost unchanged (within half a percent exact match score) when using our document retrieval method instead of the given documents, showing our pipeline does a good job of producing evidence documents that are similar to the ones in the training data."}, {"id": 199, "string": "We test the system on questions from the TREC QA tasks (Voorhees et al., 1999) , in particular a curated set of questions from Baudi\u0161 (2015) , the same dataset used in Chen et al."}, {"id": 200, "string": "(2017a) ."}, {"id": 201, "string": "We apply our system to the 694 test questions without retraining on the train questions."}, {"id": 202, "string": "We compare against DrQA (Chen et al., 2017a) and YodaQA (Baudi\u0161, 2015) ."}, {"id": 203, "string": "It is important to note that these systems use different document corpora (Wikipedia for DrQA, and Wikipedia, several knowledge bases, and optionally Bing web search for YodaQA) and different training data (SQuAD and the TREC training questions for DrQA, and TREC only for YodaQA), so we cannot make assertions about the relative performance of individual components."}, {"id": 204, "string": "Nevertheless, it is instructive to show how the methods we experiment with in this work can advance an end-to-end QA system."}, {"id": 205, "string": "The results are listed in  racy mark."}, {"id": 206, "string": "This is a strong proof-of-concept that neural paragraph reading combined with existing document retrieval methods can advance the stateof-the-art on general question answering."}, {"id": 207, "string": "It also shows that, despite the noise, the data from Trivi-aQA is sufficient to train models that can be effective on out-of-domain QA tasks."}, {"id": 208, "string": "Discussion We found that models that have only been trained on answer-containing paragraphs can perform very poorly in the multi-paragraph setting."}, {"id": 209, "string": "The results were particularly bad for SQuAD; we think this is partly because the paragraphs are shorter, so the model had less exposure to irrelevant text."}, {"id": 210, "string": "The shared-norm approach consistently outperformed the other methods, especially on SQuAD and TriviaQA unfiltered, where many paragraphs were needed to reach peak performance."}, {"id": 211, "string": "Figures  3, 4 , and 5 show this technique has a minimal effect on the performance when only one paragraph is used, suggesting the model's per-paragraph performance is preserved."}, {"id": 212, "string": "Meanwhile, it can be seen the accuracy of the shared-norm model never drops as more paragraphs are added, showing it successfully resolves the problem of being distracted by irrelevant text."}, {"id": 213, "string": "The no-answer and merge approaches were moderately effective, we suspect because they at least expose the model to more irrelevant text."}, {"id": 214, "string": "However, these methods do not address the fundamental issue of requiring confidence scores to be comparable between independent applications of the model to different paragraphs, which is why we think they lagged behind."}, {"id": 215, "string": "The sigmoid objective function reduces the paragraph-level performance considerably, especially on the TriviaQA datasets."}, {"id": 216, "string": "We suspect this is because it is vulnerable to label noise, as discussed in Section 2.2."}, {"id": 217, "string": "Error Analysis We perform an error analysis by labeling 200 random TriviaQA web dev-set errors made by the shared-norm model."}, {"id": 218, "string": "We found 40.5% of the er-rors were caused because the document did not contain sufficient evidence to answer the question, and 17% were caused by the correct answer not being contained in the answer key."}, {"id": 219, "string": "The distribution of the remaining errors is shown in Table 5 ."}, {"id": 220, "string": "We found quite a few cases where a sentence contained the answer, but the model was unable to extract it due to complex syntactic structure or paraphrasing."}, {"id": 221, "string": "Two kinds of multi-sentence reading errors were also common: cases that required connecting multiple statements made in a single paragraph, and long-range coreference cases where a sentence's subject was named in a previous paragraph."}, {"id": 222, "string": "Finally, some questions required background knowledge, or required the model to extract answers that were only stated indirectly (e.g., examining a list to extract the nth element)."}, {"id": 223, "string": "Overall, these results suggest good avenues for improvement are to continue advancing the sentence and paragraph level reading comprehension abilities of the model, and adding a mechanism to handle document-level coreferences."}, {"id": 224, "string": "Related Work Reading Comprehension Datasets."}, {"id": 225, "string": "The state of the art in reading comprehension has been rapidly advanced by neural models, in no small part due to the introduction of many large datasets."}, {"id": 226, "string": "The first large scale datasets for training neural reading comprehension models used a Cloze-style task, where systems must predict a held out word from a piece of text (Hermann et al., 2015; Hill et al., 2015) ."}, {"id": 227, "string": "Additional datasets including SQuAD (Rajpurkar et al., 2016) , WikiReading (Hewlett et al., 2016) , MS Marco (Nguyen et al., 2016) and Triv-iaQA (Joshi et al., 2017) provided more realistic questions."}, {"id": 228, "string": "Another dataset of trivia questions, Quasar-T (Dhingra et al., 2017) , was introduced recently that uses ClueWeb09 (Callan et al., 2009) as its source for documents."}, {"id": 229, "string": "In this work we choose to focus on SQuAD because it is well studied, and TriviaQA because it is more challenging and features documents and multi-document contexts (Quasar T is similar, but was released after we started work on this project)."}, {"id": 230, "string": "Neural Reading Comprehension."}, {"id": 231, "string": "Neural reading comprehension systems typically use some form of attention (Wang and Jiang, 2016) , although alternative architectures exist (Chen et al., 2017a; Weissenborn et al., 2017b) ."}, {"id": 232, "string": "Our model follows this approach, but includes some recent advances such as variational dropout (Gal and Ghahramani, 2016) and bi-directional attention (Seo et al., 2016) ."}, {"id": 233, "string": "Self-attention has been used in several prior works (Cheng et al., 2016; Wang et al., 2017c; Pan et al., 2017) ."}, {"id": 234, "string": "Our approach to allowing a reading comprehension model to produce a per-paragraph no-answer score is related to the approach used in the BiDAF-T (Min et al., 2017) model to produce per-sentence classification scores, although we use an attentionbased method instead of max-pooling."}, {"id": 235, "string": "Open QA."}, {"id": 236, "string": "Open question answering has been the subject of much research, especially spurred by the TREC question answering track (Voorhees et al., 1999) ."}, {"id": 237, "string": "Knowledge bases can be used, such as in (Berant et al., 2013) , although the resulting systems are limited by the quality of the knowledge base."}, {"id": 238, "string": "Systems that try to answer questions using natural language resources such as YodaQA (Baudi\u0161, 2015) typically use pipelined methods to retrieve related text, build answer candidates, and pick a final output."}, {"id": 239, "string": "Neural Open QA."}, {"id": 240, "string": "Open question answering with neural models was considered by Chen et al."}, {"id": 241, "string": "(2017a) , where researchers trained a model on SQuAD and combined it with a retrieval engine for Wikipedia articles."}, {"id": 242, "string": "Our work differs because we focus on explicitly addressing the problem of applying the model to multiple paragraphs."}, {"id": 243, "string": "A pipelined approach to QA was recently proposed by Wang et al."}, {"id": 244, "string": "(2017a) , where a ranker model is used to select a paragraph for the reading comprehension model to process."}, {"id": 245, "string": "More recent work has considered evidence aggregation techniques (Wang et al., 2017b; Swayamdipta et al., 2017) ."}, {"id": 246, "string": "Our work shows paragraph-level models that produce well-calibrated confidence scores can effectively exploit large amounts of text without aggregation, although integrating aggregation techniques could further improve our results."}, {"id": 247, "string": "Conclusion We have shown that, when using a paragraph-level QA model across multiple paragraphs, our training method of sampling non-answer-containing paragraphs while using a shared-norm objective function can be very beneficial."}, {"id": 248, "string": "Combining this with our suggestions for paragraph selection, using the summed training objective, and our model design allows us to advance the state of the art on TriviaQA."}, {"id": 249, "string": "As shown by our demo, this work can be directly applied to building deep-learningpowered open question answering systems."}], "headers": [{"section": "Introduction", "n": "1", "start": 0, "end": 26}, {"section": "Pipelined Method", "n": "2", "start": 27, "end": 28}, {"section": "Paragraph Selection", "n": "2.1", "start": 29, "end": 33}, {"section": "Handling Noisy Labels", "n": "2.2", "start": 34, "end": 47}, {"section": "Model", "n": "2.3", "start": 48, "end": 65}, {"section": "Confidence Method", "n": "3", "start": 66, "end": 85}, {"section": "Shared-Normalization", "n": "3.1", "start": 86, "end": 89}, {"section": "Merge", "n": "3.2", "start": 90, "end": 91}, {"section": "No-Answer Option", "n": "3.3", "start": 92, "end": 99}, {"section": "Sigmoid", "n": "3.4", "start": 100, "end": 103}, {"section": "Datasets", "n": "4.1", "start": 104, "end": 104}, {"section": "Preprocessing", "n": "4.2", "start": 105, "end": 110}, {"section": "Sampling", "n": "4.3", "start": 111, "end": 116}, {"section": "Implementation", "n": "4.4", "start": 117, "end": 125}, {"section": "TriviaQA Web and TriviaQA Wiki", "n": "5.1", "start": 126, "end": 147}, {"section": "TriviaQA Unfiltered", "n": "5.2", "start": 148, "end": 157}, {"section": "SQuAD", "n": "5.3", "start": 158, "end": 188}, {"section": "Curated TREC", "n": "5.4", "start": 189, "end": 207}, {"section": "Discussion", "n": "5.5", "start": 208, "end": 216}, {"section": "Error Analysis", "n": "5.6", "start": 217, "end": 223}, {"section": "Related Work", "n": "6", "start": 224, "end": 245}, {"section": "Conclusion", "n": "7", "start": 246, "end": 249}], "figures": [{"filename": "../figure/image/996-Figure4-1.png", "caption": "Figure 4: Results for our confidence methods on TriviaQA unfiltered. The shared-norm approach is the strongest, while the baseline model starts to lose performance as more paragraphs are used.", "page": 5, "bbox": {"x1": 310.08, "x2": 522.24, "y1": 199.68, "y2": 316.32}}, {"filename": "../figure/image/996-Figure3-1.png", "caption": "Figure 3: Results on TriviaQA web when applying our models to multiple paragraphs from each document. Most of our training methods improve the model\u2019s ability to utilize more text.", "page": 5, "bbox": {"x1": 74.88, "x2": 287.03999999999996, "y1": 199.68, "y2": 316.32}}, {"filename": "../figure/image/996-Table3-1.png", "caption": "Table 3: Published TriviaQA results. Our approach advances the state of the art by about 10 points on these datasets4", "page": 5, "bbox": {"x1": 84.96, "x2": 510.24, "y1": 62.879999999999995, "y2": 145.92}}, {"filename": "../figure/image/996-Figure1-1.png", "caption": "Figure 1: Noisy supervision can cause many spans of text that contain the answer, but are not situated in a context that relates to the question (red), to distract the model from learning from more relevant spans (green).", "page": 1, "bbox": {"x1": 309.59999999999997, "x2": 522.24, "y1": 205.44, "y2": 321.12}}, {"filename": "../figure/image/996-Figure5-1.png", "caption": "Figure 5: Results for our confidence methods on document-level SQuAD. The shared-norm model is the only model that does not lose performance when exposed to large numbers of paragraphs.", "page": 6, "bbox": {"x1": 74.88, "x2": 287.03999999999996, "y1": 64.8, "y2": 179.51999999999998}}, {"filename": "../figure/image/996-Figure2-1.png", "caption": "Figure 2: High level outline of our model.", "page": 2, "bbox": {"x1": 72.0, "x2": 291.36, "y1": 62.879999999999995, "y2": 358.08}}, {"filename": "../figure/image/996-Table4-1.png", "caption": "Table 4: Results on the Curated TREC corpus, YodaQA results extracted from its github page7", "page": 7, "bbox": {"x1": 86.88, "x2": 273.12, "y1": 61.44, "y2": 114.24}}, {"filename": "../figure/image/996-Table5-1.png", "caption": "Table 5: Error analysis on TriviaQA web.", "page": 7, "bbox": {"x1": 324.96, "x2": 506.4, "y1": 61.44, "y2": 132.0}}, {"filename": "../figure/image/996-Table1-1.png", "caption": "Table 1: Examples from SQuAD where a model was less confident in a correct extraction from one paragraph (left) than in an incorrect extraction from another (right). Even if the passage has no correct answer and does not contain any question words, the model assigns high confidence to phrases that match the category the question is asking about. Because the confidence scores are not well-calibrated, this confidence is often higher than the confidence assigned to correct answer spans in different paragraphs, even when those correct spans have better contextual evidence.", "page": 3, "bbox": {"x1": 75.84, "x2": 521.28, "y1": 62.879999999999995, "y2": 197.28}}, {"filename": "../figure/image/996-Table2-1.png", "caption": "Table 2: Results on TriviaQA web using our pipelined method.", "page": 4, "bbox": {"x1": 326.88, "x2": 503.03999999999996, "y1": 62.879999999999995, "y2": 135.35999999999999}}]}