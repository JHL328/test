{"title": "Visual Attention Model for Name Tagging in Multimodal Social Media", "abstract": "Everyday billions of multimodal posts containing both images and text are shared in social media sites such as Snapchat, Twitter or Instagram. This combination of image and text in a single message allows for more creative and expressive forms of communication, and has become increasingly common in such sites. This new paradigm brings new challenges for natural language understanding, as the textual component tends to be shorter, more informal, and often is only understood if combined with the visual context. In this paper, we explore the task of name tagging in multimodal social media posts. We start by creating two new multimodal datasets: one based on Twitter posts 1 and the other based on Snapchat captions (exclusively submitted to public and crowdsourced stories). We then propose a novel model based on Visual Attention that not only provides deeper visual understanding on the decisions of the model, but also significantly outperforms other state-of-theart baseline methods for this task. 2 * * This work was mostly done during the first author's internship at Snap Research. 1  The Twitter data and associated images presented in this paper were downloaded from https://archive.org/ details/twitterstream 2 We will make the annotations on Twitter data available for research purpose upon request.", "text": [{"id": 0, "string": "Introduction Social platforms, like Snapchat, Twitter, Instagram and Pinterest, have become part of our lives and play an important role in making communication easier and accessible."}, {"id": 1, "string": "Once textcentric, social media platforms are becoming in-creasingly multimodal, with users combining images, videos, audios, and texts for better expressiveness."}, {"id": 2, "string": "As social media posts become more multimodal, the natural language understanding of the textual components of these messages becomes increasingly challenging."}, {"id": 3, "string": "In fact, it is often the case that the textual component can only be understood in combination with the visual context of the message."}, {"id": 4, "string": "In this context, here we study the task of Name Tagging for social media containing both image and textual contents."}, {"id": 5, "string": "Name tagging is a key task for language understanding, and provides input to several other tasks such as Question Answering, Summarization, Searching and Recommendation."}, {"id": 6, "string": "Despite its importance, most of the research in name tagging has focused on news articles and longer text documents, and not as much in multimodal social media data (Baldwin et al., 2015) ."}, {"id": 7, "string": "However, multimodality is not the only challenge to perform name tagging on such data."}, {"id": 8, "string": "The textual components of these messages are often very short, which limits context around names."}, {"id": 9, "string": "Moreover, there linguistic variations, slangs, typos and colloquial language are extremely common, such as using 'looooove' for 'love', 'LosAngeles' for 'Los Angeles', and '#Chicago #Bull' for 'Chicago Bulls'."}, {"id": 10, "string": "These characteristics of social media data clearly illustrate the higher difficulty of this task, if compared to traditional newswire name tagging."}, {"id": 11, "string": "In this work, we modify and extend the current state-of-the-art model (Lample et al., 2016; Ma and Hovy, 2016) in name tagging to incorporate the visual information of social media posts using an Attention mechanism."}, {"id": 12, "string": "Although the usually short textual components of social media posts provide limited contextual information, the accompanying images often provide rich information that can be useful for name tagging."}, {"id": 13, "string": "For ex- ample, as shown in Figure 1 , both captions include the phrase 'Modern Baseball'."}, {"id": 14, "string": "It is not easy to tell if each Modern Baseball refers to a name or not from the textual evidence only."}, {"id": 15, "string": "However using the associated images as reference, we can easily infer that Modern Baseball in the first sentence should be the name of a band because of the implicit features from the objects like instruments and stage, and the Modern Baseball in the second sentence refers to the sport of baseball because of the pitcher in the image."}, {"id": 16, "string": "In this paper, given an image-sentence pair as input, we explore a new approach to leverage visual context for name tagging in text."}, {"id": 17, "string": "First, we propose an attention-based model to extract visual features from the regions in the image that are most related to the text."}, {"id": 18, "string": "It can ignore irrelevant visual information."}, {"id": 19, "string": "Secondly, we propose to use a gate to combine textual features extracted by a Bidirectional Long Short Term Memory (BLSTM) and extracted visual features, before feed them into a Conditional Random Fields(CRF) layer for tag predication."}, {"id": 20, "string": "The proposed gate architecture plays the role to modulate word-level multimodal features."}, {"id": 21, "string": "We evaluate our model on two labeled datasets collected from Snapchat and Twitter respectively."}, {"id": 22, "string": "Our experimental results show that the proposed model outperforms state-for-the-art name tagger in multimodal social media."}, {"id": 23, "string": "The main contributions of this work are as follows: \u2022 We create two new datasets for name tagging in multimedia data, one using Twitter and the other using crowd-sourced Snapchat posts."}, {"id": 24, "string": "These new datasets effectively constitute new benchmarks for the task."}, {"id": 25, "string": "\u2022 We propose a visual attention model specifically for name tagging in multimodal social media data."}, {"id": 26, "string": "The proposed end-to-end model only uses image-sentence pairs as input without any human designed features, and a Visual Attention component that helps understand the decision making of the model."}, {"id": 27, "string": "Figure 2 shows the overall architecture of our model."}, {"id": 28, "string": "We describe three main components of our model in this section: BLSTM-CRF sequence labeling model (Section 2.1), Visual Attention Model (Section 2.3) and Modulation Gate (Section 2.4)."}, {"id": 29, "string": "Given a pair of sentence and image as input, the Visual Attention Model extracts regional visual features from the image and computes the weighted sum of the regional visual features as the visual context vector, based on their relatedness with the sentence."}, {"id": 30, "string": "The BLSTM-CRF sequence labeling model predicts the label for each word in the sentence based on both the visual context vector and the textual information of the words."}, {"id": 31, "string": "The modulation gate controls the combination of the visual context vector and the word representations for each word before the CRF layer."}, {"id": 32, "string": "Model BLSTM-CRF Sequence Labeling We model name tagging as a sequence labeling problem."}, {"id": 33, "string": "Given a sequence of words: S = {s 1 , s 2 , ..., s n }, we aim to predict a sequence of labels: L = {l 1 , l 2 , ..., l n }, where l i \u2208 L and L is a pre-defined label set."}, {"id": 34, "string": "Bidirectional LSTM."}, {"id": 35, "string": "Long Short-term Memory Networks (LSTMs) (Hochreiter and Schmidhuber, 1997) are variants of Recurrent Neural Networks (RNNs) designed to capture long-range dependencies of input."}, {"id": 36, "string": "The equations of a LSTM cell are as follows: i t = \u03c3(W xi x t + W hi h t\u22121 + b i ) f t = \u03c3(W xf x t + W hf h t\u22121 + b f ) c t = tanh(W xc x t + W hc h t\u22121 + b c ) c t = f t c t\u22121 + i t c t o t = \u03c3(W xo x t + W ho h t\u22121 + b o ) h t = o t tanh(c t ) where x t , c t and h t are the input, memory and hidden state at time t respectively."}, {"id": 37, "string": "W xi , W hi , W xf , W hf , W xc , W hc , W xo , and W ho are weight matrices."}, {"id": 38, "string": "is the element-wise product function and \u03c3 is the element-wise sigmoid function."}, {"id": 39, "string": "Name Tagging benefits from both of the past (left) and the future (right) contexts, thus we implement the Bidirectional LSTM (Graves et al., 2013; Dyer et al., 2015) by concatenating the left and right context representations, h t = [ \u2212 \u2192 h t , \u2190 \u2212 h t ], for each word."}, {"id": 40, "string": "Character-level Representation."}, {"id": 41, "string": "Following (Lample et al., 2016) , we generate the character-level representation for each word using another BLSTM."}, {"id": 42, "string": "It receives character embeddings as input and generates representations combining implicit prefix, suffix and spelling information."}, {"id": 43, "string": "The final word representation x i is the concatenation of word embedding e i and character-level representation c i ."}, {"id": 44, "string": "c i = BLST M char (s i ) s i \u2208 S x i = [e i , c i ] Conditional random fields (CRFs)."}, {"id": 45, "string": "For name tagging, it is important to consider the constraints of the labels in neighborhood (e.g., I-LOC must follow B-LOC)."}, {"id": 46, "string": "CRFs (Lafferty et al., 2001 ) are effective to learn those constraints and jointly predict the best chain of labels."}, {"id": 47, "string": "We follow the implementation of CRFs in (Ma and Hovy, 2016) ."}, {"id": 48, "string": "Visual Feature Representation We use Convolutional Neural Networks (CNNs) (LeCun et al., 1989) to obtain the representations of images."}, {"id": 49, "string": "Particularly, we use Residual Net (ResNet) (He et al., 2016) , which (Lin et al., 2014) detection, and COCO segmentation tasks."}, {"id": 50, "string": "Given an input pair (S, I), where S represents the word sequence and I represents the image rescaled to 224x224 pixels, we use ResNet to extract visual features for regional areas as well as for the whole image ( Fig 3) : V g = ResN et g (I) V r = ResN et r (I) where the global visual vector V g , which represents the whole image, is the output before the last fully connected layer 3 ."}, {"id": 51, "string": "The dimension of V g is 1,024."}, {"id": 52, "string": "V r are the visual representations for regional areas and they are extracted from the last convolutional layer of ResNet, and the dimension is 1,024x7x7 as shown in Figure 3 ."}, {"id": 53, "string": "7x7 is the number of regions in the image and 1,024 is the dimension of the feature vector."}, {"id": 54, "string": "Thus each feature vector of V r corresponds to a 32x32 pixel region of the rescaled input image."}, {"id": 55, "string": "The global visual representation is a reasonable representation of the whole input image, but not the best."}, {"id": 56, "string": "Sometimes only parts of the image are related to the associated sentence."}, {"id": 57, "string": "For example, the visual features from the right part of the image in Figure 4 cannot contribute to inferring the information in the associated sentence 'I have just bought Jeremy Pied.'"}, {"id": 58, "string": "In this work we utilize visual attention mechanism to combat the problem, which has been proven effective for vision-language related tasks such as Image Captioning  and Visual Question Answering (Yang et al., 2016b; Lu et al., 2016) , by enforcing the model to focus on the regions in images that are mostly related to context textual information while ignoring irrelevant regions."}, {"id": 59, "string": "Also the visualization of attention can also help us to understand the decision making of the model."}, {"id": 60, "string": "Attention mechanism is mapping a query and a set of key-value pairs to an output."}, {"id": 61, "string": "The output is a weighted sum of the values and the assigned weight for each value is computed by a function of the query and corresponding key."}, {"id": 62, "string": "We encode the sentence into a query vector using an LSTM, and use regional visual representations V r as both keys and values."}, {"id": 63, "string": "Text Query Vector."}, {"id": 64, "string": "We use an LSTM to encode the sentence into a query vector, in which the inputs of the LSTM are the concatenations of word embeddings and character-level word representations."}, {"id": 65, "string": "Different from the LSTM model used for sequence labeling in Section 2.1, the LSTM here aims to get the semantic information of the sen-tence and it is unidirectional: Visual Attention Model Q = LST M query (S) (1) Attention Implementation."}, {"id": 66, "string": "There are many implementations of visual attention mechanism such as Multi-layer Perceptron (Bahdanau et al., 2014) , Bilinear (Luong et al., 2015) , dot product (Luong et al., 2015) , Scaled Dot Product (Vaswani et al., 2017) , and linear projection after summation (Yang et al., 2016b) ."}, {"id": 67, "string": "Based on our experimental results, dot product implementations usually result in more concentrated attentions and linear projection after summation results in more dispersed attentions."}, {"id": 68, "string": "In the context of name tagging, we choose the implementation of linear projection after summation because it is beneficial for the model to utilize as many related visual features as possible, and concentrated attentions may make the model bias."}, {"id": 69, "string": "For implementation, we first project the text query vector Q and regional visual features V r into the same dimensions: P t = tanh(W t Q) P v = tanh(W v V r ) then we sum up the projected query vector with each projected regional visual vector respectively: A = P t \u2295 P v the weights of the regional visual vectors: E = sof tmax(W a A + b a ) where W a is weights matrix."}, {"id": 70, "string": "The weighted sum of the regional visual features is: v c = \u03b1 i v i \u03b1 i \u2208 E, v i \u2208 V r We use v c as the visual context vector to initialize the BLSTM sequence labeling model in Section 2.1."}, {"id": 71, "string": "We compare the performances of the models using global visual vector V g and attention based visual context vector V c for initialization in Section 4."}, {"id": 72, "string": "Visual Modulation Gate The BLSTM-CRF sequence labeling model benefits from using the visual context vector to initialize the LSTM cell."}, {"id": 73, "string": "However, the better way to utilize visual features for sequence labeling is to incorporate the features at word level individually."}, {"id": 74, "string": "However visual features contribute quite differently when they are used to infer the tags of different words."}, {"id": 75, "string": "For example, we can easily find matched visual patterns from associated images for verbs such as 'sing', 'run', and 'play'."}, {"id": 76, "string": "Words/Phrases such as names of basketball players, artists, and buildings are often well-aligned with objects in images."}, {"id": 77, "string": "However it is difficult to align function words such as 'the', 'of ' and 'well' with visual features."}, {"id": 78, "string": "Fortunately, most of the challenging cases in name tagging involve nouns and verbs, the disambiguation of which can benefit more from visual features."}, {"id": 79, "string": "We propose to use a visual modulation gate, similar to (Miyamoto and Cho, 2016; Yang et al., 2016a) , to dynamically control the combination of visual features and word representation generated by BLSTM at word-level, before feed them into the CRF layer for tag prediction."}, {"id": 80, "string": "The equations for the implementation of modulation gate are as follows: \u03b2 v = \u03c3(W v h i + U v v c + b v ) \u03b2 w = \u03c3(W w h i + U w v c + b w ) m = tanh(W m h i + U m v c + b m ) w m = \u03b2 w \u00b7 h i + \u03b2 v \u00b7 m where h i is the word representation generated by BLSTM, v c is the computed visual context vector, W v , W w , W m , U v , U w and U m are weight matrices, \u03c3 is the element-wise sigmoid function, and w m is the modulated word representations fed into the CRF layer in Section 2.1."}, {"id": 81, "string": "We conduct experiments to evaluate the impact of modulation gate in Section 4."}, {"id": 82, "string": "Datasets We evaluate our model on two multimodal datasets, which are collected from Twitter and Snapchat respectively."}, {"id": 83, "string": "Table 1 summarizes the data statistics."}, {"id": 84, "string": "Both datasets contain four types of named entities: Location, Person, Organization and Miscellaneous."}, {"id": 85, "string": "Each data instance contains a pair of sentence and image, and the names in sentences are manually tagged by three expert labelers."}, {"id": 86, "string": "Twitter name tagging."}, {"id": 87, "string": "The Twitter name tagging dataset contains pairs of tweets and their associated images extracted from May 2016, January 2017 and June 2017."}, {"id": 88, "string": "We use sports and social event related key words, such as concert, festival, soccer, basketball, as queries."}, {"id": 89, "string": "We don't take into consideration messages without images for this experiment."}, {"id": 90, "string": "If a tweet has more than one image associated to it, we randomly select one of the images."}, {"id": 91, "string": "Snap name tagging."}, {"id": 92, "string": "The Snap name tagging dataset consists of caption and image pairs exclusively extracted from snaps submitted to public and live stories."}, {"id": 93, "string": "They were collected between May and July of 2017."}, {"id": 94, "string": "The data contains captions submitted to multiple community curated stories like the Electric Daisy Carnival (EDC) music festival and the Golden State Warrior's NBA parade."}, {"id": 95, "string": "Both Twitter and Snapchat are social media with plenty of multimodal posts, but they have obvious differences with sentence length and image styles."}, {"id": 96, "string": "In Twitter, text plays a more important role, and the sentences in the Twitter dataset are much longer than those in the Snap dataset (16.0 tokens vs 8.1 tokens)."}, {"id": 97, "string": "The image is often more related to the content of the text and added with the purpose of illustrating or giving more context."}, {"id": 98, "string": "On the other hand, as users of Snapchat use cameras to communicate, the roles of text and image are switched."}, {"id": 99, "string": "Captions are often added to complement what is being portrayed by the snap."}, {"id": 100, "string": "On our experiment section we will show that our proposed model outperforms baseline on both datasets."}, {"id": 101, "string": "We believe the Twitter dataset can be an important step towards more research in multimodal name tagging and we plan to provide it as a benchmark upon request."}, {"id": 102, "string": "Experiment Training Tokenization."}, {"id": 103, "string": "To tokenize the sentences, we use the same rules as (Owoputi et al., 2013) , except we separate the hashtag '#' with the words after."}, {"id": 104, "string": "Labeling Schema."}, {"id": 105, "string": "We use the standard BIO schema (Sang and Veenstra, 1999), because we see little difference when we switch to BIOES schema (Ratinov and Roth, 2009) ."}, {"id": 106, "string": "Word embeddings."}, {"id": 107, "string": "We use the 100-dimensional GloVe 4 (Pennington et al., 2014) embeddings trained on 2 billions tweets to initialize the lookup table and do fine-tuning during training."}, {"id": 108, "string": "Character embeddings."}, {"id": 109, "string": "As in (Lample et al., 2016) , we randomly initialize the character embeddings with uniform samples."}, {"id": 110, "string": "Based on experimental results, the size of the character embeddings affects little, and we set it as 50."}, {"id": 111, "string": "Pretrained CNNs."}, {"id": 112, "string": "We use the pretrained ResNet-152 (He et al., 2016) from Pytorch."}, {"id": 113, "string": "Early Stopping."}, {"id": 114, "string": "We use early stopping (Caruana et al., 2001; Graves et al., 2013) with a patience of 15 to prevent the model from over-fitting."}, {"id": 115, "string": "Fine Tuning."}, {"id": 116, "string": "The models are optimized with finetuning on both the word-embeddings and the pretrained ResNet."}, {"id": 117, "string": "Optimization."}, {"id": 118, "string": "The models achieve the best performance by using mini-batch stochastic gradient descent (SGD) with batch size 20 and momentum 0.9 on both datasets."}, {"id": 119, "string": "We set an initial learning rate of \u03b7 0 = 0.03 with decay rate of \u03c1 = 0.01."}, {"id": 120, "string": "We use a gradient clipping of 5.0 to reduce the effects of gradient exploding."}, {"id": 121, "string": "Hyper-parameters."}, {"id": 122, "string": "We summarize the hyperparameters in Table 2 ."}, {"id": 123, "string": "Hyper-parameter Value LSTM hidden state size 300 Char LSTM hidden state size 50 visual vector size 100 dropout rate 0.5 Table 2 : Hyper-parameters of the networks."}, {"id": 124, "string": "Table 3 shows the performance of the baseline, which is BLSTM-CRF with sentences as input only, and our proposed models on both datasets."}, {"id": 125, "string": "BLSTM-CRF + Global Image Vector: use global image vector to initialize the BLSTM-CRF."}, {"id": 126, "string": "BLSTM-CRF + Visual attention: use attention based visual context vector to initialize the BLSTM-CRF."}, {"id": 127, "string": "BLSTM-CRF + Visual attention + Gate: modulate word representations with visual vector."}, {"id": 128, "string": "Our final model BLSTM-CRF + VISUAL AT-TENTION + GATE, which has visual attention component and modulation gate, obtains the best F1 scores on both datasets."}, {"id": 129, "string": "Visual features successfully play a role of validating entity types."}, {"id": 130, "string": "For example, when there is a person in the image, it is more likely to include a person name in the associated sentence, but when there is a soccer field in the image, it is more likely to include a sports team name."}, {"id": 131, "string": "Results All the models get better scores on Twitter dataset than on Snap dataset, because the average length of the sentences in Snap dataset (8.1 tokens) is much smaller than that of Twitter dataset (16.0 tokens), which means there is much less contextual information in Snap dataset."}, {"id": 132, "string": "Also comparing the gains from visual features on different datasets, we find that the model benefits more from visual features on Twitter dataset, considering the much higher baseline scores on Twitter dataset."}, {"id": 133, "string": "Based on our observation, users of Snapchat often post selfies with captions, which means some of the images are not strongly related to their associated captions."}, {"id": 134, "string": "In contrast, users of Twitter prefer to post images to illustrate texts 4.3 Attention Visualization Figure 5 shows some good examples of the attention visualization and their corresponding name tagging results."}, {"id": 135, "string": "The model can successfully focus on appropriate regions when the images are well aligned with the associated sentences."}, {"id": 136, "string": "Based on our observation, the multimodal contexts in posts related to sports, concerts or festival are usually better aligned with each other, therefore the visual features easily contribute to these cases."}, {"id": 137, "string": "For example, the ball and shoot action in example (a) in Figure 5 indicates that the context should be related to basketball, thus the 'Warriors' should be the name of a sports team."}, {"id": 138, "string": "A singing person with a microphone in example (b) indicates that the name of an artist or a band ('Radiohead') may appear in the sentence."}, {"id": 139, "string": "The second and the third rows in Figure 5 show some more challenging cases whose tagging results benefit from visual features."}, {"id": 140, "string": "In example (d), the model pays attention to the big Apple logo, thus tags the 'Apple' in the sentence as an Organization name."}, {"id": 141, "string": "In example (e) and (i), a small Figure 6 shows some failed examples that are categorized into three types: (1) bad alignments between visual and textual information; Error Analysis (2) blur images; (3) wrong attention made by the model."}, {"id": 142, "string": "Name tagging greatly benefits from visual fea-tures when the sentences are well aligned with the associated image as we show in Section 4.3."}, {"id": 143, "string": "But it is not always the case in social media."}, {"id": 144, "string": "The example (a) in Figure 6 shows a failed example resulted from poor alignment between sentences and images."}, {"id": 145, "string": "In this image, there are two bins standing in front of a wall, but the sentence talks about basketball players."}, {"id": 146, "string": "The unrelated visual information makes the model tag 'Cleveland' as a Location, however it refers to the basketball team 'Cleveland Cavaliers'."}, {"id": 147, "string": "The image in example (b) is blur, so the extracted visual information extracted actually introduces noise instead of additional information."}, {"id": 148, "string": "The    image in example (c) is about a baseball pitcher, but our model pays attention to the top right corner of the image."}, {"id": 149, "string": "The visual context feature computed by our model is not related to the sentence, and results in missed tagging of 'SBU', which is an organization name."}, {"id": 150, "string": "Related Work In this section, we summarize relevant background on previous work on name tagging and visual attention."}, {"id": 151, "string": "Name Tagging."}, {"id": 152, "string": "In recent years, (Chiu and Nichols, 2015; Lample et al., 2016; Ma and Hovy, 2016) proposed several neural network architectures for named tagging that outperform traditional explicit features based methods (Chieu and Ng, 2002; Florian et al., 2003; Ando and Zhang, 2005; Ratinov and Roth, 2009; Lin and Wu, 2009; Passos et al., 2014; Luo et al., 2015) ."}, {"id": 153, "string": "They all use Bidirectional LSTM (BLSTM) to extract features from a sequence of words."}, {"id": 154, "string": "For characterlevel representations, (Lample et al., 2016) proposed to use another BLSTM to capture prefix and suffix information of words, and (Chiu and Nichols, 2015; Ma and Hovy, 2016) used CNN to extract position-independent character features."}, {"id": 155, "string": "On top of BLSTM, (Chiu and Nichols, 2015) used a softmax layer to predict the label for each word, and (Lample et al., 2016; Ma and Hovy, 2016) used a CRF layer for joint prediction."}, {"id": 156, "string": "Compared with traditional approaches, neural networks based approaches do not require hand-crafted features and achieved state-of-the-art performance on name tagging (Ma and Hovy, 2016) ."}, {"id": 157, "string": "However, these methods were mainly developed for newswire and paid little attention to social media."}, {"id": 158, "string": "For name tagging in social media, (Ritter et al., 2011) leveraged a large amount of unlabeled data and many dictionaries into a pipeline model."}, {"id": 159, "string": "(Limsopatham and Collier, 2016) adapted the BLSTM-CRF model with additional word shape information, and (Aguilar et al., 2017) utilized an effective multi-task approach."}, {"id": 160, "string": "Among these methods, our model is most similar to (Lample et al., 2016) , but we designed a new visual attention component and a modulation control gate."}, {"id": 161, "string": "Visual Attention."}, {"id": 162, "string": "Since the attention mechanism was proposed by (Bahdanau et al., 2014) , it has been widely adopted to language and vision related tasks, such as Image Captioning and Visual Question Answering (VQA), by retrieving the visual features most related to text context (Zhu et al., 2016; Anderson et al., 2017; Xu and Saenko, 2016; Chen et al., 2015) ."}, {"id": 163, "string": "proposed to predict a word based on the visual patch that is most related to the last predicted word for image captioning."}, {"id": 164, "string": "(Yang et al., 2016b; Lu et al., 2016) applied attention mechanism for VQA, to find the regions in images that are most related to the questions."}, {"id": 165, "string": "(Yu et al., 2016) applied the visual attention mechanism on video captioning."}, {"id": 166, "string": "Our attention implementation approach in this work is similar to those used for VQA."}, {"id": 167, "string": "The model finds the regions in images that are most related to the accompanying sentences, and then feed the visual features into an BLSTM-CRF sequence labeling model."}, {"id": 168, "string": "The differences are: (1) we add visual context feature at each step of sequence labeling; and (2) we propose to use a gate to control the combination of the visual information and textual information based on their relatedness."}, {"id": 169, "string": "2 Conclusions and Future Work We propose a gated Visual Attention for name tagging in multimodal social media."}, {"id": 170, "string": "We construct two multimodal datasets from Twitter and Snapchat."}, {"id": 171, "string": "Experiments show an absolute 3%-4% F-score gain."}, {"id": 172, "string": "We hope this work will encourage more research on multimodal social media in the future and we plan on making our benchmark available upon request."}, {"id": 173, "string": "Name Tagging for more fine-grained types (e.g."}, {"id": 174, "string": "soccer team, basketball team, politician, artist) can benefit more from visual features."}, {"id": 175, "string": "For example, an image including a pitcher indicates that the 'Giants' in context should refer to the baseball team 'San Francisco Giants'."}, {"id": 176, "string": "We plan to expand our model to tasks such as fine-grained Name Tagging or Entity Liking in the future."}], "headers": [{"section": "Introduction", "n": "1", "start": 0, "end": 31}, {"section": "BLSTM-CRF Sequence Labeling", "n": "2.1", "start": 32, "end": 47}, {"section": "Visual Feature Representation", "n": "2.2", "start": 48, "end": 64}, {"section": "Visual Attention Model", "n": "2.3", "start": 65, "end": 71}, {"section": "Visual Modulation Gate", "n": "2.4", "start": 72, "end": 81}, {"section": "Datasets", "n": "3", "start": 82, "end": 101}, {"section": "Training", "n": "4.1", "start": 102, "end": 130}, {"section": "Results", "n": "4.2", "start": 131, "end": 140}, {"section": "Error Analysis", "n": "4.4", "start": 141, "end": 149}, {"section": "Related Work", "n": "5", "start": 150, "end": 168}, {"section": "Conclusions and Future Work", "n": "6", "start": 169, "end": 176}], "figures": [{"filename": "../figure/image/1345-Table1-1.png", "caption": "Table 1: Sizes of the datasets in numbers of sentence and token.", "page": 5, "bbox": {"x1": 156.96, "x2": 441.12, "y1": 62.879999999999995, "y2": 132.96}}, {"filename": "../figure/image/1345-Table2-1.png", "caption": "Table 2: Hyper-parameters of the networks.", "page": 5, "bbox": {"x1": 90.72, "x2": 271.2, "y1": 417.59999999999997, "y2": 487.2}}, {"filename": "../figure/image/1345-Figure1-1.png", "caption": "Figure 1: Examples of Modern Baseball associated with different images.", "page": 1, "bbox": {"x1": 89.75999999999999, "x2": 280.32, "y1": 61.44, "y2": 135.35999999999999}}, {"filename": "../figure/image/1345-Figure5-1.png", "caption": "Figure 5: Examples of visual attentions and NER outputs.", "page": 6, "bbox": {"x1": 73.92, "x2": 524.16, "y1": 397.44, "y2": 739.1999999999999}}, {"filename": "../figure/image/1345-Table3-1.png", "caption": "Table 3: Results of our models on noisy social media data.", "page": 6, "bbox": {"x1": 72.0, "x2": 528.0, "y1": 62.879999999999995, "y2": 145.92}}, {"filename": "../figure/image/1345-Figure3-1.png", "caption": "Figure 3: CNN for visual features extraction.", "page": 2, "bbox": {"x1": 317.76, "x2": 511.2, "y1": 332.64, "y2": 411.35999999999996}}, {"filename": "../figure/image/1345-Figure2-1.png", "caption": "Figure 2: Overall Architecture of the Visual Attention Name Tagging Model.", "page": 2, "bbox": {"x1": 93.6, "x2": 503.03999999999996, "y1": 61.44, "y2": 289.92}}, {"filename": "../figure/image/1345-Figure6-1.png", "caption": "Figure 6: Examples of Failed Visual Attention.", "page": 7, "bbox": {"x1": 72.0, "x2": 526.0799999999999, "y1": 61.44, "y2": 156.48}}, {"filename": "../figure/image/1345-Figure4-1.png", "caption": "Figure 4: Example of partially related image and sentence. (\u2018I have just bought Jeremy Pied.\u2019)", "page": 3, "bbox": {"x1": 121.92, "x2": 241.44, "y1": 145.44, "y2": 264.96}}]}