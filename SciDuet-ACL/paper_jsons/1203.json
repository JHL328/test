{"title": "Personalized Review Generation by Expanding Phrases and Attending on Aspect-Aware Representations", "abstract": "In this paper, we focus on the problem of building assistive systems that can help users to write reviews. We cast this problem using an encoder-decoder framework that generates personalized reviews by expanding short phrases (e.g. review summaries, product titles) provided as input to the system. We incorporate aspect-level information via an aspect encoder that learns 'aspect-aware' user and item representations. An attention fusion layer is applied to control generation by attending on the outputs of multiple encoders. Experimental results show that our model is capable of generating coherent and diverse reviews that expand the contents of input phrases. In addition, the learned aspectaware representations discover those aspects that users are more inclined to discuss and bias the generated text toward their personalized aspect preferences.", "text": [{"id": 0, "string": "Introduction Contextual, or 'data-to-text' natural language generation is one of the core tasks in natural language processing and has a considerable impact on various fields (Gatt and Krahmer, 2017) ."}, {"id": 1, "string": "Within the field of recommender systems, a promising application is to estimate (or generate) personalized reviews that a user would write about a product, i.e., to discover their nuanced opinions about each of its individual aspects."}, {"id": 2, "string": "A successful model could work (for instance) as (a) a highly-nuanced recommender system that tells users their likely reaction to a product in the form of text fragments; (b) a writing tool that helps users 'brainstorm' the review-writing process; or (c) a querying system that facilitates personalized natural lan-guage queries (i.e., to find items about which a user would be most likely to write a particular phrase)."}, {"id": 3, "string": "Some recent works have explored the review generation task and shown success in generating cohesive reviews (Dong et al., 2017; Ni et al., 2017; Zang and Wan, 2017) ."}, {"id": 4, "string": "Most of these works treat the user and item identity as input; we seek a system with more nuance and more precision by allowing users to 'guide' the model via short phrases, or auxiliary data such as item specifications."}, {"id": 5, "string": "For example, a review writing assistant might allow users to write short phrases and expand these key points into a plausible review."}, {"id": 6, "string": "Review text has been widely studied in traditional tasks such as aspect extraction (Mukherjee and Liu, 2012; He et al., 2017) , extraction of sentiment lexicons (Zhang et al., 2014) , and aspectaware sentiment analysis (Wang et al., 2016; McAuley et al., 2012) ."}, {"id": 7, "string": "These works are related to review generation since they can provide prior knowledge to supervise the generative process."}, {"id": 8, "string": "We are interested in exploring how such knowledge (e.g."}, {"id": 9, "string": "extracted aspects) can be used in the review generation task."}, {"id": 10, "string": "In this paper, we focus on designing a review generation model that is able to leverage both user and item information as well as auxiliary, textual input and aspect-aware knowledge."}, {"id": 11, "string": "Specifically, we study the task of expanding short phrases into complete, coherent reviews that accurately reflect the opinions and knowledge learned from those phrases."}, {"id": 12, "string": "These short phrases could include snippets provided by the user, or manifest aspects about the items themselves (e.g."}, {"id": 13, "string": "brand words, technical specifications, etc.)."}, {"id": 14, "string": "We propose an encoderdecoder framework that takes into consideration three encoders (a sequence encoder, an attribute encoder, and an aspect encoder), and one decoder."}, {"id": 15, "string": "The sequence encoder uses a gated recurrent unit 0 0 0 \u2026 1 0 0 1 0 \u2026 0 0  (GRU) network to encode text information; the attribute encoder learns a latent representation of user and item identity; finally, the aspect encoder finds an aspect-aware representation of users and items, which reflects user-aspect preferences and item-aspect relationships."}, {"id": 16, "string": "The aspect-aware representation is helpful to discover what each user is likely to discuss about each item."}, {"id": 17, "string": "Finally, the output of these encoders is passed to the sequence decoder with an attention fusion layer."}, {"id": 18, "string": "The decoder attends on the encoded information and biases the model to generate words that are consistent with the input phrases and words belonging to the most relevant aspects."}, {"id": 19, "string": "Related Work Review generation belongs to a large body of work on data-to-text natural language generation (Gatt and Krahmer, 2017) , which has applications including summarization (See et al., 2017) , image captioning (Vinyals et al., 2015) , and dialogue response generation (Xing et al., 2017; Ghosh et al., 2017) , among others."}, {"id": 20, "string": "Among these, review generation is characterized by the need to generate long sequences and estimate high-order interactions between users and items."}, {"id": 21, "string": "Several approaches have been recently proposed to tackle these problems."}, {"id": 22, "string": "Dong et al."}, {"id": 23, "string": "(2017) proposed an attribute-to-sequence (Attr2Seq) method to encode user and item identities as well as rating information with a multi-layer perceptron and a decoder then generates reviews conditioned on this information."}, {"id": 24, "string": "They also used an attention mechanism to strengthen the alignment between output and input attributes."}, {"id": 25, "string": "Ni et al."}, {"id": 26, "string": "(2017) trained a collaborative-filtering generative concatenative network to jointly learn the tasks of review generation and item recommendation."}, {"id": 27, "string": "Zang and Wan (2017) proposed a hierarchical structure to generate long reviews; they assume each sentence is associated with an aspect score, and learn the attention between aspect scores and sentences during training."}, {"id": 28, "string": "Our approach differs from these mainly in our goal of incorporating auxiliary textual information (short phrases, product specifications, etc.)"}, {"id": 29, "string": "into the generative process, which facilitates the generation of higher-fidelity reviews."}, {"id": 30, "string": "Another line of work related to review generation is aspect extraction and opinion mining (Park et al., 2015; Qiu et al., 2017; He et al., 2017; Chen et al., 2014) ."}, {"id": 31, "string": "In this paper, we argue that the extra aspect (opinion) information extracted using these previous works can effectively improve the quality of generated reviews."}, {"id": 32, "string": "We propose a simple but effective way to combine aspect information into the generative model."}, {"id": 33, "string": "Approach We describe the review generation task as follows."}, {"id": 34, "string": "Given a user u, item i, several short phrases {d 1 , d 2 , ..., d M }, and a group of extracted aspects {A 1 , A 2 , ..., A k }, our goal is to generate a review (w 1 , w 2 , ..., w T ) that maximizes the probability P (w 1:T |u, i, d 1:M )."}, {"id": 35, "string": "To solve this task, we propose a method called ExpansionNet which contains two parts: 1) three encoders to leverage the input phrases and aspect information; and 2) a decoder with an attention fusion layer to generate sequences and align the generation with the input sources."}, {"id": 36, "string": "The model structure is shown in Figure 1 ."}, {"id": 37, "string": "Sequence encoder, attribute encoder and aspect encoder Our sequence encoder is a two-layer bi-directional GRU, as is commonly used in sequence-tosequence (Seq2Seq) models ."}, {"id": 38, "string": "Input phrases first pass a word embedding layer, then go through the GRU one-by-one and finally yield a sequence of hidden states {e 1 , e 2 ..., e L }."}, {"id": 39, "string": "In the case of multiple phrases, these share the same sequence encoder and have different lengths L. To simplify notation, we only consider one input phrase in this section."}, {"id": 40, "string": "The attribute encoder and aspect encoder both consist of two embedding layers and a projection layer."}, {"id": 41, "string": "For the attribute encoder, we define two general embedding layers E u \u2208 R |U |\u00d7m and E i \u2208 R |I|\u00d7m to obtain the attribute latent factors \u03b3 u and \u03b3 i ; for the aspect encoder, we use two aspect-aware embedding layers E u \u2208 R |U |\u00d7k and E i \u2208 R |I|\u00d7k to obtain aspect-aware latent factors \u03b2 u and \u03b2 i ."}, {"id": 42, "string": "Here |U|, |I|, m and k are the number of users, number of items, the dimension of attributes, and the number of aspects, respectively."}, {"id": 43, "string": "After the embedding layers, the attribute and aspect-aware latent factors are concatenated and fed into a projection layer with tanh activation."}, {"id": 44, "string": "The outputs are calculated as: \u03b3 u = E u (u), \u03b3 i = E i (i) (1) \u03b2 u = E u (u), \u03b2 i = E i (i) (2) u = tanh(W u [\u03b3 u ; \u03b3 i ] + b u ) (3) v = tanh(W v [\u03b2 u ; \u03b2 i ] + b v ) (4) where W u \u2208 R n\u00d72m , b u \u2208 R n , W v \u2208 R n\u00d72k , b v \u2208 R n are learnable parameters and n is the dimensionality of the hidden units in the decoder."}, {"id": 45, "string": "Decoder with attention fusion layer The decoder is a two-layer GRU that predicts the target words given the start token."}, {"id": 46, "string": "The hidden state of the decoder is initialized using the sum of the three encoders' outputs."}, {"id": 47, "string": "The hidden state at time-step t is updated via the GRU unit based on the previous hidden state and the input word."}, {"id": 48, "string": "Specifically: h 0 = e L + u + v (5) h t = GRU(w t , h t\u22121 ), (6) where h 0 \u2208 R n is the decoder's initial hidden state and h t \u2208 R n is the hidden state at time-step t. To fully exploit the encoder-side information, we apply an attention fusion layer to summarize the output of each encoder and jointly determine the final word distribution."}, {"id": 49, "string": "For the sequence encoder, the attention vector is defined as in many other applications Luong et al., 2015) : a 1 t = L j=1 \u03b1 1 tj e j (7) \u03b1 1 tj = exp(tanh(v 1 \u03b1 (W 1 \u03b1 [e j ; h t ] + b 1 \u03b1 )))/Z, (8) where a 1 t \u2208 R n is the attention vector on the sequence encoder at time-step t, \u03b1 1 tj is the attention score over the encoder hidden state e j and decoder hidden state h t , and Z is a normalization term."}, {"id": 50, "string": "For the attribute encoder, the attention vector is calculated as: a 2 t = j\u2208u,i \u03b1 2 tj \u03b3 j (9) \u03b1 2 tj = exp(tanh(v 2 \u03b1 (W 2 \u03b1 [\u03b3 j ; h t ] + b 2 \u03b1 )))/Z, (10) where a 2 t \u2208 R n is the attention vector on the attribute encoder, and \u03b1 2 tj is the attention score between the attribute latent factor \u03b3 j and decoder hidden state h t ."}, {"id": 51, "string": "Inspired by the copy mechanism (Gu et al., 2016; See et al., 2017) , we design an attention vector that estimates the probability that each aspect will be discussed in the next time-step: s ui = W s [\u03b2 u ; \u03b2 i ] + b s (11) a 3 t = tanh(W 3 \u03b1 [s ui ; e t ; h t ] + b 3 \u03b1 ), (12) where s ui \u2208 R k is the aspect importance considering the interaction between u and i, e t is the decoder input after embedding layer at time-step t, and a 3 t \u2208 R k is a probability vector to bias each aspect at time-step t. Finally, the first two attention vectors are concatenated with the decoder hidden state at time-step t and projected to obtain the output word distribution P v ."}, {"id": 52, "string": "The attention scores from the aspect encoder are then directly added to the aspect words in the final word distribution."}, {"id": 53, "string": "The output probability for word w at time-step t is given by: where w t is the target word at time-step t, a 3 t [k] is the probability that aspect k will be discussed at time-step t, A k represents all words belonging to aspect k and 1 wt\u2208A k is a binary variable indicating whether w t belongs to aspect k. During inference, we use greedy decoding by choosing the word with maximum probability, denoted as y t = argmax wt softmax(P (w t ))."}, {"id": 54, "string": "Decoding finishes when an end token is encountered."}, {"id": 55, "string": "Experiments We consider a real world dataset from Amazon Electronics (McAuley et al., 2015) to evaluate our model."}, {"id": 56, "string": "We convert all text into lowercase, add start and end tokens to each review, and perform tokenization using NLTK."}, {"id": 57, "string": "1 We discard reviews with length greater than 100 tokens and consider a vocabulary of 30,000 tokens."}, {"id": 58, "string": "After preprocessing, the dataset contains 182,850 users, 59,043 items, and 992,172 reviews (sparsity 99.993%), which is much sparser than the datasets used in previous works (Dong et al., 2017; Ni et al., 2017) ."}, {"id": 59, "string": "On average, each review contains 49.32 tokens as well as a short-text summary of 4.52 tokens."}, {"id": 60, "string": "In our experiments, the basic ExpansionNet uses these summaries as input phrases."}, {"id": 61, "string": "We split the dataset into training (80%), validation (10%) and test sets (10%)."}, {"id": 62, "string": "All results are reported on the test set."}, {"id": 63, "string": "Aspect Extraction We use the method 2 in (He et al., 2017) to extract 15 aspects and consider the top 100 words from each aspect."}, {"id": 64, "string": "Table 2 shows 10 inferred aspects and representative words (inferred aspects are manually labeled)."}, {"id": 65, "string": "ExpansionNet calculates an attention score based on the user and item aspect-aware representation, then determines how much these representative words are biased in the output word distribution."}, {"id": 66, "string": "1 https://www.nltk.org/ 2 https://github.com/ruidan/ Unsupervised-Aspect-Extraction Experiment Details We use PyTorch 3 to implement our model."}, {"id": 67, "string": "4 Parameter settings are shown in Table 1 ."}, {"id": 68, "string": "For the attribute encoder and aspect encoder, we set the dimensionality to 64 and 15 respectively."}, {"id": 69, "string": "For both the sequence encoder and decoder, we use a 2layer GRU with hidden size 512."}, {"id": 70, "string": "We also add dropout layers before and after the GRUs."}, {"id": 71, "string": "The dropout rate is set to 0.1."}, {"id": 72, "string": "During training, the input sequences of the same source (e.g."}, {"id": 73, "string": "review, summary) inside each batch are padded to the same length."}, {"id": 74, "string": "Performance Evaluation We evaluate the model on six automatic metrics (Table 3) : Perplexity, BLEU-1/BLEU-4, ROUGE-L and Distinct-1/2 (percentage of distinct unigrams and bi-grams) ."}, {"id": 75, "string": "We compare User/Item user A3G831BTCLWGVQ and item B007M50PTM Review summary \"easy to use and nice standard apps\" Item title \"samsung galaxy tab 2 (10.1-Inch, wi-fi) 2012 model\" Real review \"the display is beautiful and the tablet is very easy to use."}, {"id": 76, "string": "it comes with some really nice standard apps.\""}, {"id": 77, "string": "AttrsSeq \"i bought this for my wife 's new ipad air ."}, {"id": 78, "string": "it fits perfectly and looks great ."}, {"id": 79, "string": "the only thing i do n't like is that the cover is a little too small for the ipad air . \""}, {"id": 80, "string": "ExpansionNet \"i love this tablet ."}, {"id": 81, "string": "it is fast and easy to use ."}, {"id": 82, "string": "i have no complaints ."}, {"id": 83, "string": "i would recommend this tablet to anyone .\""}, {"id": 84, "string": "+title \"i love this tablet ."}, {"id": 85, "string": "it is fast and easy to use ."}, {"id": 86, "string": "i have a galaxy tab 2 and i love it .\""}, {"id": 87, "string": "+attribute & aspect \"i love this tablet ."}, {"id": 88, "string": "it is easy to use and the screen is very responsive ."}, {"id": 89, "string": "i love the fact that it has a micro sd slot ."}, {"id": 90, "string": "i have not tried the tablet app yet but i do n't have any problems with it ."}, {"id": 91, "string": "i am very happy with this tablet .\""}, {"id": 92, "string": "Figure 2 : Examples of a real review and reviews generated by different models given a user, item, review summary, and item title."}, {"id": 93, "string": "Highlights added for emphasis."}, {"id": 94, "string": "against three baselines: Rand (randomly choose a review from the training set), GRU-LM (the GRU decoder works alone as a language model) and a state-of-the-art model Attr2Seq that only considers user and item attribute (Dong et al., 2017) ."}, {"id": 95, "string": "ExpansionNet (with summary, item title, attribute and aspect as input) achieves significant improvements over Attr2Seq on all metrics."}, {"id": 96, "string": "As we add more input information, the model continues to obtain better results, except for the ROUGE-L metric."}, {"id": 97, "string": "This proves that our model can effectively learn from short input phrases and aspect information and improve the correctness and diversity of generated results."}, {"id": 98, "string": "Figure 2 presents a sample generation result."}, {"id": 99, "string": "ExpansionNet captures fine-grained item information (e.g."}, {"id": 100, "string": "that the item is a tablet), which Attr2Seq fails to recognize."}, {"id": 101, "string": "Moreover, given a phrase like \"easy to use\" in the summary, ExpansionNet generates reviews containing the same text."}, {"id": 102, "string": "This demonstrates the possibility of using our model in an assistive review generation scenario."}, {"id": 103, "string": "Finally, given extra aspect information, the model successfully estimates that the screen would be an important aspect (i.e., for the current user and item); it generates phrases such as \"screen is very respon- sive\" about the aspect \"screen\" which is also covered in the real (ground-truth) review (\"display is beautiful\")."}, {"id": 104, "string": "We are also interested in seeing how the aspectaware representation can find related aspects and bias the generation to discuss more about those aspects."}, {"id": 105, "string": "We analyze the average number of aspects in real and generated reviews and show on average how many aspects in real reviews are covered in generated reviews."}, {"id": 106, "string": "We consider a review as covering an aspect if any of the aspect's representative words exists in the review."}, {"id": 107, "string": "As shown in Table 4 , Attr2Seq tends to cover more aspects in generation, many of which are not discussed in real reviews."}, {"id": 108, "string": "On the other hand, ExpansionNet better captures the distribution of aspects that are discussed in real reviews."}], "headers": [{"section": "Introduction", "n": "1", "start": 0, "end": 18}, {"section": "Related Work", "n": "2", "start": 19, "end": 32}, {"section": "Approach", "n": "3", "start": 33, "end": 36}, {"section": "Sequence encoder, attribute encoder and aspect encoder", "n": "3.1", "start": 37, "end": 44}, {"section": "Decoder with attention fusion layer", "n": "3.2", "start": 45, "end": 54}, {"section": "Experiments", "n": "4", "start": 55, "end": 62}, {"section": "Aspect Extraction", "n": "4.1", "start": 63, "end": 65}, {"section": "Experiment Details", "n": "4.2", "start": 66, "end": 73}, {"section": "Performance Evaluation", "n": "4.3", "start": 74, "end": 108}], "figures": [{"filename": "../figure/image/1203-Table4-1.png", "caption": "Table 4: Aspect coverage analysis", "page": 4, "bbox": {"x1": 306.71999999999997, "x2": 529.4399999999999, "y1": 434.4, "y2": 507.35999999999996}}, {"filename": "../figure/image/1203-Table3-1.png", "caption": "Table 3: Results on automatic metrics", "page": 4, "bbox": {"x1": 97.92, "x2": 498.24, "y1": 85.44, "y2": 178.07999999999998}}, {"filename": "../figure/image/1203-Figure2-1.png", "caption": "Figure 2: Examples of a real review and reviews generated by different models given a user, item, review summary, and item title. Highlights added for emphasis.", "page": 4, "bbox": {"x1": 79.67999999999999, "x2": 516.0, "y1": 189.6, "y2": 357.12}}, {"filename": "../figure/image/1203-Figure1-1.png", "caption": "Figure 1: General structure of ExpansionNet.", "page": 1, "bbox": {"x1": 122.88, "x2": 473.28, "y1": 68.64, "y2": 240.0}}, {"filename": "../figure/image/1203-Table1-1.png", "caption": "Table 1: Parameter settings used in our experiments.", "page": 3, "bbox": {"x1": 105.6, "x2": 489.12, "y1": 85.44, "y2": 129.12}}, {"filename": "../figure/image/1203-Table2-1.png", "caption": "Table 2: List of representative words for inferred aspects on Amazon Electronics dataset.", "page": 3, "bbox": {"x1": 306.71999999999997, "x2": 530.4, "y1": 184.79999999999998, "y2": 466.08}}]}