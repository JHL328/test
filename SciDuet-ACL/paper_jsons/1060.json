{"title": "Language Generation via DAG Transduction", "abstract": "A DAG automaton is a formal device for manipulating graphs. By augmenting a DAG automaton with transduction rules, a DAG transducer has potential applications in fundamental NLP tasks. In this paper, we propose a novel DAG transducer to perform graph-to-program transformation. The target structure of our transducer is a program licensed by a declarative programming language rather than linguistic structures. By executing such a program, we can easily get a surface string. Our transducer is designed especially for natural language generation (NLG) from type-logical semantic graphs. Taking Elementary Dependency Structures, a format of English Resource Semantics, as input, our NLG system achieves a BLEU-4 score of 68.07. This remarkable result demonstrates the feasibility of applying a DAG transducer to resolve NLG, as well as the effectiveness of our design.", "text": [{"id": 0, "string": "Introduction The recent years have seen an increased interest as well as rapid progress in semantic parsing and surface realization based on graph-structured semantic representations, e.g."}, {"id": 1, "string": "Abstract Meaning Representation (AMR; Banarescu et al., 2013) , Elementary Dependency Structure (EDS; Oepen and L\u00f8nning, 2006) and Depedendency-based Minimal Recursion Semantics (DMRS; Copestake, 2009) ."}, {"id": 2, "string": "Still underexploited is a formal framework for manipulating graphs that parallels automata, tranducers or formal grammars for strings and trees."}, {"id": 3, "string": "Two such formalisms have recently been proposed and applied for NLP."}, {"id": 4, "string": "One is graph grammar, e.g."}, {"id": 5, "string": "Hyperedge Replacement Gram-mar (HRG; Ehrig et al., 1999) ."}, {"id": 6, "string": "The other is DAG automata, originally studied by Kamimura and Slutzki (1982) and extended by Chiang et al."}, {"id": 7, "string": "(2018) ."}, {"id": 8, "string": "In this paper, we study DAG transducers in depth, with the goal of building accurate, efficient yet robust natural language generation (NLG) systems."}, {"id": 9, "string": "The meaning representation studied in this work is what we call type-logical semantic graphs, i.e."}, {"id": 10, "string": "semantic graphs grounded under type-logical semantics (Carpenter, 1997) , one dominant theoretical framework for modeling natural language semantics."}, {"id": 11, "string": "In this framework, adjuncts, such as adjective and adverbal phrases, are analyzed as (higher-order) functors, the function of which is to consume complex arguments (Kratzer and Heim, 1998) ."}, {"id": 12, "string": "In the same spirit, generalized quantifiers, prepositions and function words in many languages other than English are also analyzed as higher-order functions."}, {"id": 13, "string": "Accordingly, all the linguistic elements are treated as roots in type-logical semantic graphs, such as EDS and DMRS."}, {"id": 14, "string": "This makes the typological structure quite flat rather than hierachical, which is an essential distinction between natural language semantics and syntax."}, {"id": 15, "string": "To the best of our knowledge, the only existing DAG transducer for NLG is the one proposed by Quernheim and Knight (2012) ."}, {"id": 16, "string": "Quernheim and Knight introduced a DAG-to-tree transducer that can be applied to AMR-to-text generation."}, {"id": 17, "string": "This transducer is designed to handle hierarchical structures with limited reentrencies, and it is unsuitable for meaning graphs transformed from type-logical semantics."}, {"id": 18, "string": "Furthermore, Quernheim and Knight did not describe how to acquire graph recognition and transduction rules from linguistic data, and reported no result of practical generation."}, {"id": 19, "string": "It is still unknown to what extent a DAG transducer suits realistic NLG."}, {"id": 20, "string": "The design for string and tree transducers (Comon et al., 1997) focuses on not only the logic of the computation for a new data structure, but also the corresponding control flow."}, {"id": 21, "string": "This is very similar the imperative programming paradigm: implementing algorithms with exact details in explicit steps."}, {"id": 22, "string": "This design makes it very difficult to transform a type-logical semantic graph into a string, due to the fact their internal structures are highly diverse."}, {"id": 23, "string": "We borrow ideas from declarative programming, another programming paradigm, which describes what a program must accomplish, rather than how to accomplish it."}, {"id": 24, "string": "We propose a novel DAG transducer to perform graphto-program transformation ( \u00a73)."}, {"id": 25, "string": "The input of our transducer is a semantic graph, while the output is a program licensed by a declarative programming language rather than linguistic structures."}, {"id": 26, "string": "By executing such a program, we can easily get a surface string."}, {"id": 27, "string": "This idea can be extended to other types of linguistic structures, e.g."}, {"id": 28, "string": "syntactic trees or semantic representations of another language."}, {"id": 29, "string": "We conduct experiments on richly detailed semantic annotations licensed by English Resource Grammar (ERG; Flickinger, 2000) ."}, {"id": 30, "string": "We introduce a principled method to derive transduction rules from DeepBank (Flickinger et al., 2012) ."}, {"id": 31, "string": "Furthermore, we introduce a fine-to-coarse strategy to ensure that at least one sentence is generated for any input graph."}, {"id": 32, "string": "Taking EDS graphs, a variable-free ERS format, as input, our NLG system achieves a BLEU-4 score of 68.07."}, {"id": 33, "string": "On average, it produces more than 5 sentences in a second on an x86 64 GNU/Linux platform with two Intel Xeon E5-2620 CPUs."}, {"id": 34, "string": "Since the data for experiments is newswire data, i.e."}, {"id": 35, "string": "WSJ sentences from PTB (Marcus et al., 1993) , the input graphs are quite large on average."}, {"id": 36, "string": "The remarkable accuracy, efficiency and robustness demonstrate the feasibility of applying a DAG transducer to resolve NLG, as well as the effectiveness of our transducer design."}, {"id": 37, "string": "Previous Work and Challenges Preliminaries A node-labeled simple graph over alphabet \u03a3 is a triple G = (V, E, \u2113), where V is a finite set of nodes, E \u2286 V \u00d7 V is an finite set of edges and \u2113 : V \u2192 \u03a3 is a labeling function."}, {"id": 38, "string": "For a node v \u2208 V , sets of its incoming and outgoing edges are denoted by in(v) and out(v) respectively."}, {"id": 39, "string": "For an edge e \u2208 E, its source node and target node are denoted by src(e) and tar(e) respectively."}, {"id": 40, "string": "Gen-erally speaking, a DAG is a directed acyclic simple graph."}, {"id": 41, "string": "Different from trees, a DAG allows nodes to have multiple incoming edges."}, {"id": 42, "string": "In this paper, we only consider DAGs that are unordered, node-labeled, multi-rooted 1 and connected."}, {"id": 43, "string": "Conceptual graphs, including AMR and EDS, are both node-labeled and edge-labeled."}, {"id": 44, "string": "It seems that without edge labels, a DAG is inadequate, but this problem can be solved easily by using the strategies introduced in (Chiang et al., 2018) ."}, {"id": 45, "string": "Take a labeled edge proper q BV \u2212\u2192 named for example 2 ."}, {"id": 46, "string": "We can represent the same information by replacing it with two unlabeled edges and a new labeled node: proper q \u2192 BV \u2192 named."}, {"id": 47, "string": "Previous Work DAG automata are the core engines of graph transducers (Bohnet and Wanner, 2010; Quernheim and Knight, 2012) ."}, {"id": 48, "string": "In this work, we adopt Chiang et al."}, {"id": 49, "string": "(2018) 's design and define a weighted DAG automaton as a tuple M = \u27e8\u03a3, Q, \u03b4, K\u27e9: \u2022 \u03a3 is an alphabet of node labels."}, {"id": 50, "string": "\u2022 Q is a finite set of states."}, {"id": 51, "string": "\u2022 (K, \u2295, \u2297, 0, 1) is a semiring of weights."}, {"id": 52, "string": "\u2022 \u03b4 : \u0398 \u2192 K\\{0} is a weight function that assigns nonzero weights to a finite transition set \u0398."}, {"id": 53, "string": "Every transition t \u2208 \u0398 is of the form {q 1 , \u00b7 \u00b7 \u00b7 , q m } \u03c3 \u2212 \u2192 {r 1 , \u00b7 \u00b7 \u00b7 , r n } where q i and r j are states in Q."}, {"id": 54, "string": "A transition t gets m states on the incoming edges of a node and puts n states on the outgoing edges."}, {"id": 55, "string": "A transition that does not belong to \u0398 recieves a weight of zero."}, {"id": 56, "string": "A run of M on a DAG D = \u27e8V, E, \u2113\u27e9 is an edge labeling function \u03c1 : E \u2192 Q."}, {"id": 57, "string": "The weight of a run \u03c1 (denoted as \u03b4 \u2032 (\u03c1)) is the product of all weights of local transitions: \u03b4 \u2032 (\u03c1) = \u2297 v\u2208V \u03b4 ( \u03c1(in(v)) \u2113(v) \u2212 \u2212 \u2192 \u03c1(out(v)) ) Here, for a function f , we use f ({a 1 , \u00b7 \u00b7 \u00b7 , a n }) to represent {f (a 1 ), \u00b7 \u00b7 \u00b7 , f (a n )}."}, {"id": 58, "string": "If K is a boolean semiring, the automata fall backs to an unweighted DAG automata or DAG acceptor."}, {"id": 59, "string": "A accepting run or recognition is a run, the weight of which is 1, meaning true."}, {"id": 60, "string": "Challenges The DAG automata defined above can only be used for recognition."}, {"id": 61, "string": "In order to generate sentences from semantic graphs, we need DAG transducers."}, {"id": 62, "string": "A DAG transducer is a DAG automata-augmented computation model for transducing well-formed DAGs to other data structures."}, {"id": 63, "string": "Quernheim and Knight (2012) focused on feature structures and introduced a DAG-to-Tree transducer to perform graph-to-tree transformation."}, {"id": 64, "string": "The input of their transducer is limited to single-rooted DAGs."}, {"id": 65, "string": "When the labels of the leaves of an output tree in order are interpreted as words, this transducer can be applied to generate natural language sentences."}, {"id": 66, "string": "When applying Quernheim and Knight's DAGto-Tree transducer on type-logic semantic graphs, e.g."}, {"id": 67, "string": "ERS, there are some significant problems."}, {"id": 68, "string": "First, it lacks the ability to reverse the direction of edges during transduction because it is difficult to keep acyclicy anymore if edge reversing is allowed."}, {"id": 69, "string": "Second, it cannot handle multiple roots."}, {"id": 70, "string": "But we have discussed and reached the conclusion that multi-rootedness is a necessary requirement for representing type-logical semantic graphs."}, {"id": 71, "string": "It is difficult to decide which node should be the tree root during a 'top-down' transduction and it is also difficult to merge multiple unconnected nodes into one during a 'bottom-up' transduction."}, {"id": 72, "string": "At the risk of oversimplifying, we argue that the function of the existing DAG-to-Tree transducer is to transform a hierachical structure into another hierarchical structure."}, {"id": 73, "string": "Since the type-local semantic graphs are so flat, it is extremely difficult to adopt Quernheim and Knight's design to handle such graphs."}, {"id": 74, "string": "Third, there are unconnected nodes with direct dependencies, meaning that their correpsonding surface expressions appear to be very close."}, {"id": 75, "string": "The conceptual nodes even x deg and steep a 1 in Figure 4 are an example."}, {"id": 76, "string": "It is extremely difficult for the DAG-to-Tree transducer to handle this situation."}, {"id": 77, "string": "A New DAG Transducer Basic Idea In this paper, we introduce a design of transducers that can perform structure transformation towards many data structures, including but not limited to trees."}, {"id": 78, "string": "The basic idea is to give up the rewritting method to directly generate a new data structure piece by piece, while recognizing an input DAG."}, {"id": 79, "string": "Instead, our transducer obtains target structures based on side effects of DAG recognition."}, {"id": 80, "string": "The output of our transducer is no longer the target data structure itself, e.g."}, {"id": 81, "string": "a tree or another DAG, and is now a program, i.e."}, {"id": 82, "string": "a bunch of statements licensed by a particular declarative programming language."}, {"id": 83, "string": "The target structures are constructed by executing such programs."}, {"id": 84, "string": "Since our main concern of this paper is natural language generation, we take strings, namely sequences of words, as our target structures."}, {"id": 85, "string": "In this section, we introduce an extremely simple programming language for string concatenation and then details about how to leverage the power of declarative programming to perform DAG-tostring transformation."}, {"id": 86, "string": "A Declarative Programming Language The syntax in the BNF format of our declarative programming language, denoted as L c , for string calculation is: \u27e8program\u27e9 ::= \u27e8statement\u27e9 * \u27e8statement\u27e9 ::= \u27e8variable\u27e9 = \u27e8expr\u27e9 \u27e8expr\u27e9 ::= \u27e8variable\u27e9 | \u27e8string\u27e9 | \u27e8expr\u27e9 + \u27e8expr\u27e9 Here a string is a sequence of characters selected from an alphabet (denoted as \u03a3 out ) and can be empty (denoted as \u03f5)."}, {"id": 87, "string": "The semantics of '=' is value assignment, while the semantics of '+' is string concatenation."}, {"id": 88, "string": "The value of variables are strings."}, {"id": 89, "string": "For every statement, the left hand side is a variable and the right hand side is a sequence of string literals and variables that are combined through '+'."}, {"id": 90, "string": "Equation (1) presents an exmaple program licensed by this language."}, {"id": 91, "string": "S = x 21 + want + x 11 x 11 = to + go x 21 = x 41 + John x 41 = \u03f5 (1) After solving these statements, we can query the values of all variables."}, {"id": 92, "string": "In particular, we are interested in S, which is related to the desired natural language expression John want to go 3 ."}, {"id": 93, "string": "Using the relation between the variables, we can easily convert the statements in (1) to a rooted tree."}, {"id": 94, "string": "The result is shown in Figure 1 ."}, {"id": 95, "string": "This tree is significantly different from the target structures discussed by Quernheim and Knight (2012) or other normal tree transducers (Comon et al., 1997) ."}, {"id": 96, "string": "This tree represents calculation to solve the program."}, {"id": 97, "string": "Constructing such internal trees is an essential function of the compiler of our programming language."}, {"id": 98, "string": "Informal Illustration We introduce our DAG transducer using a simple example."}, {"id": 99, "string": "Figure 2 shows the original input graph D = (V, E, \u2113)."}, {"id": 100, "string": "Without any loss of generality, we remove edge labels."}, {"id": 101, "string": "Table 1 lists the rule set-R-for this example."}, {"id": 102, "string": "Every row represents an applicable transduction rule that consists of two parts."}, {"id": 103, "string": "The left column is the recognition part displayed in the form I \u03c3 \u2212 \u2192 O, where I, O and \u03c3 decode the state set of incoming edges, the state set of outgoing edges and the node label respectively."}, {"id": 104, "string": "The right column is the generation part which consists of (multiple) templates of statements licensed by the programming language defined in the previous section."}, {"id": 105, "string": "In practice, two different rules may have a same recognition part but different generation parts."}, {"id": 106, "string": "Every state q is of the form l(n, d) where l is the finite state label, n is the count of possible variables related to q, and d denotes the direction."}, {"id": 107, "string": "The value of d can only be r (reversed), u (unchanged) or e(empty)."}, {"id": 108, "string": "Variable v l(j,d) represents the jth (1 \u2264 j \u2264 n) variable related to state q."}, {"id": 109, "string": "For example, v X(2,r) means the second variable of state X(3,r)."}, {"id": 110, "string": "There are two special variables: S, which corresponds to the whole sentence and L, which corresponds to the output string associated to current node label."}, {"id": 111, "string": "It is reasonable to assume that there exists a function \u03c8 : \u03a3 \u2192 \u03a3 * out that maps a particular node label, i.e."}, {"id": 112, "string": "concept, to a surface string."}, {"id": 113, "string": "Therefore L is determined by \u03c8."}, {"id": 114, "string": "Now we are ready to apply transduction rules to translate D into a string."}, {"id": 115, "string": "The transduction consists of two steps: Recognition The goal of this step is to find an edge labeling function \u03c1 : E \u2192 Q which satisfies that for every node v, \u03c1(in(v)) \u2113(v) \u2212 \u2212 \u2192 \u03c1(out(v)) matches the recognition part of a rule in R. The recognition result is shown in Figure 3 ."}, {"id": 116, "string": "The red dashed edges in Figure 3 make up an intermediate graph T (\u03c1), which is a subgraph of D if edge direction is not taken into account."}, {"id": 117, "string": "Sometimes, T (\u03c1) paralles the syntactic structure of an output sentence."}, {"id": 118, "string": "For a labeling function \u03c1, we can construct intermediate graph T (\u03c1) by checking the direction parameter of every edge state."}, {"id": 119, "string": "For an u) is included."}, {"id": 120, "string": "The recognition process is slightly different from the one in Chiang et al."}, {"id": 121, "string": "(2018) ."}, {"id": 122, "string": "Since incoming edges with an Empty(0,e) state carry no semantic information, they will be ignored during recognition."}, {"id": 123, "string": "For example, in Figure 3 , we will only use e 2 and e 4 to match transducation rules for node named(John)."}, {"id": 124, "string": "edge e = (u, v) \u2208 E, if the direction of \u03c1(e) is r, then (v, u) is in T (\u03c1)."}, {"id": 125, "string": "If the direction is u, then (u, v) is in T (\u03c1)."}, {"id": 126, "string": "If the direction is e, neither (u, v) nor (v, Instantiation We use rule(v) to denotes the rule used on node v. Assume s is the generation part of rule(v)."}, {"id": 127, "string": "For every edge e i adjacent to v, assume \u03c1(e i ) = l (n, d) ."}, {"id": 128, "string": "We replace L with \u03c8(\u2113(v)) and replace every occurrence of v l (j,d) in s with a new variable x ij (1 \u2264 j \u2264 n)."}, {"id": 129, "string": "Then we Q = {DET(1,r) , Empty(0,e), VP(1,u), NP(1,u)} Rule For Recognition For Table 1 : Sets of states (Q) and rules (R) that can be used to process the graph in Figure 2 ."}, {"id": 130, "string": "Generation 1 {} proper q \u2212 \u2212\u2212\u2212\u2212\u2212 \u2192 {DET(1,r)} v DET(1,r) = \u03f5 2 {} want v 1 \u2212 \u2212\u2212\u2212\u2212\u2212 \u2192 {VP(1,u), NP(1,u)} S = v NP(1,u) + L + v VP(1,u) 3 {VP(1,u)} go v 1 \u2212\u2212\u2212\u2212\u2192 {Empty(0,e)} v VP(1,u) = to + L 4 {NP(1,u), DET(1,r)} named \u2212 \u2212\u2212\u2212 \u2192 {} v NP(1,u) = v DET(1,r) + L get a newly generated expression for v. For example, node want v 1 is recognized using Rule 2, so we replace v NP(1,u) with x 21 , v VP(1,u) with x 11 and L with want."}, {"id": 131, "string": "After instantiation, we get all the statements in Equation (1) ."}, {"id": 132, "string": "Our transducer is suitable for type-logical semantic graphs."}, {"id": 133, "string": "Because declarative programming brings in more freedom for graph transduction."}, {"id": 134, "string": "We can arrange the variables in almost any order without regard to the edge directions in original graphs."}, {"id": 135, "string": "Meanwhile, the multi-rooted problem can be solved easily because the generation is based on side effects."}, {"id": 136, "string": "We do not need to decide which node is the tree root."}, {"id": 137, "string": "Definition The formal definition of our DAG transducer described above is a tuple M = (\u03a3, Q, R, w, V, S) where: \u2022 \u03a3 is an alphabet of node labels."}, {"id": 138, "string": "\u2022 Q is a finite set of edge states."}, {"id": 139, "string": "Every state q \u2208 Q is of the form l(n, d) where l is the state label, n is the variable count and d is the direction of state which can be r, u or e. \u2022 R is a finite set of rules."}, {"id": 140, "string": "Every rule is of the form I \u03c3 \u2212 \u2192 \u27e8O, E\u27e9."}, {"id": 141, "string": "E can be any kind of statement in a declarative programming language."}, {"id": 142, "string": "It is called the generation part."}, {"id": 143, "string": "I, \u03c3 and O have the same meanings as they do in the previous section and they are called the recognition part."}, {"id": 144, "string": "\u2022 w is a score function."}, {"id": 145, "string": "Given a particular run and an anchor node, w assigns a score to measure the preference for a particular rule at this anchor node."}, {"id": 146, "string": "\u2022 V is the set of parameterized variables that can be used in every expression."}, {"id": 147, "string": "\u2022 S \u2208 V is a distinguished, global variable."}, {"id": 148, "string": "It is like the 'goal' of a program."}, {"id": 149, "string": "DAG Transduction-based NLG Different languages exhibit different morphosyntactic and syntactico-semantic properties."}, {"id": 150, "string": "For example, Russian and Arabic are morphologically-rich languages and heavily utilize grammatical markers to indicate grammatical as well as semantic functions."}, {"id": 151, "string": "On the contrary, Chinese, as an analytic language, encodes grammatical and semantic information in a highly configurational rather than either inflectional or derivational way."}, {"id": 152, "string": "Such differences affects NLG significantly."}, {"id": 153, "string": "Considering generating Chinese sentences, it seems sufficient to employ our DAG transducer to obtain a sequence of lemmas, since no morpholical production is needed."}, {"id": 154, "string": "But for morphologically-rich languages, we do need to model complex morphological changes."}, {"id": 155, "string": "To unify a general framework for DAG transduction-based NLG, we propose a two-step strategy to achive meaning-to-text transformation."}, {"id": 156, "string": "\u2022 In the first phase, we are concerned with syntactico-semantic properties and utilize our DAG transducer to translate a semantic graph into sequential lemmas."}, {"id": 157, "string": "Information such as tense, apsects, gender, etc."}, {"id": 158, "string": "is attached to anchor lemmas."}, {"id": 159, "string": "Actually, our transducer generates \"want.PRES\" rather than \"wants\"."}, {"id": 160, "string": "Here, \"PRES\" indicates a particular tense."}, {"id": 161, "string": "\u2022 In the second phase, we are concerned with morpho-syntactic properties and utilize a neural sequence-to-sequence model to obtain final surface strings from the outputs of the DAG transducer."}, {"id": 162, "string": "Inducing Transduction Rules We present an empirical study on the feasibility of DAG transduction-based NLG."}, {"id": 163, "string": "We focus on Figure 4 : An example graph."}, {"id": 164, "string": "The intended reading is \"the decline is even steeper than in September\", he said."}, {"id": 165, "string": "Original edge labels are removed for clarity."}, {"id": 166, "string": "Every edge is associated with a span list, and spans are written in the form label<begin:end>."}, {"id": 167, "string": "The red dashed edges belong to the intermediate graph T ."}, {"id": 168, "string": "variable-free MRS representations, namely EDS (Oepen and L\u00f8nning, 2006) ."}, {"id": 169, "string": "The data set used in this work is DeepBank 1.1 (Flickinger et al., 2012) ."}, {"id": 170, "string": "EDS-specific Constraints In order to generate reasonable strings, three constraints must be kept during transduction."}, {"id": 171, "string": "First, for a rule I \u03c3 \u2212 \u2192 \u27e8O, E\u27e9, a state with direction u in I or a state with direction r in O is called head state and its variables are called head variables."}, {"id": 172, "string": "For example, the head state of rule 3 in Table 1 is VP(1,u) and the head state of rule 2 is DET(1,r)."}, {"id": 173, "string": "There is at most one head state in a rule and only head variables or S can be the left sides of statements."}, {"id": 174, "string": "If there is no head state, we assign the global S as its head."}, {"id": 175, "string": "Otherwise, the number of statements is equal to the number of head variables and each statement has a distinguished left side variable."}, {"id": 176, "string": "An empty state does not have any variables."}, {"id": 177, "string": "Second, every rule has no-copying, no-deleting statements."}, {"id": 178, "string": "In other words, all variables must be used exactly once in a statement."}, {"id": 179, "string": "Third, during recognition, a labeling function \u03c1 is valid only if T (\u03c1) is a rooted tree."}, {"id": 180, "string": "After transduction, we get result \u03c1 * ."}, {"id": 181, "string": "The first and second constraints ensure that for all nodes, there is at most one incoming red dashed edge in T (\u03c1 * ) and 'data' carried by variables of the only incoming red dashed edge or S is separated into variables of outgoing red dashed edges."}, {"id": 182, "string": "The last constraint ensures that we can solve all statements by a bottom-up process on tree T (\u03c1 * )."}, {"id": 183, "string": "Fine-to-Coarse Transduction Almost all NLG systems that heavily utilize a symbolic system to encode deep syntacticosemantic information lack some robustness, meaning that some input graphs may not be successfully processed."}, {"id": 184, "string": "There are two reasons: (1) some explicit linguistic constraints are not included; (2) exact decoding is too time-consuming while inexact decoding cannot cover the whole search space."}, {"id": 185, "string": "To solve the robustness problem, we introduce a fine-to-coarse strategy to ensure that at least one sentence is generated for any input graph."}, {"id": 186, "string": "There are three types of rules in our system, namely induced rules, extended rules and dynamic rules."}, {"id": 187, "string": "The most fine-grained rules are applied to bring us precision, while the most coarse-grained rules are for robustness."}, {"id": 188, "string": "In order to extract reasonable rules, we will use both EDS graphs and the corresponding derivation trees provided by ERG."}, {"id": 189, "string": "The details will be described step-by-step in the following sections."}, {"id": 190, "string": "Figure 4 shows an example for obtaining induced rules."}, {"id": 191, "string": "The induced rules are directly obtained by following three steps: Induced Rules Finding intermediate tree T EDS graphs are highly regular semantic graphs."}, {"id": 192, "string": "It is not difficult to generate T based on a highly customized 'breadthfirst' search."}, {"id": 193, "string": "The generation starts from the 'top' node ( say v to in Figure 4) given by the EDS graph and traverse the whole graph."}, {"id": 194, "string": "No more than thirty heuristic rules are used to decide the visiting order of nodes."}, {"id": 195, "string": "Assigning states EDS graphs also provide span information for nodes."}, {"id": 196, "string": "We select a group of lexical nodes which have corresponding substrings in the original sentence."}, {"id": 197, "string": "In Figure 4 , these nodes are in bold font and directly followed by a span."}, {"id": 198, "string": "Then we merge spans from the bottom of T to the top to assign each red edge a span list."}, {"id": 199, "string": "For each node n in T , we collect spans of every outgoing dashed edge of n into a list s. Some additional spans may be inserted into s. These spans do not occur in the EDS graph but they do occur in the sentence, i.e."}, {"id": 200, "string": "than<29:33>."}, {"id": 201, "string": "Then we merge continuous spans in s and assign the remaining spans in s to the incoming red dashed edge of n. We apply a similar method to the derivation tree."}, {"id": 202, "string": "As a result, every inner node of the derivation tree is associated with a span."}, {"id": 203, "string": "Then we align the edges in T to nodes of the inner derivation tree by comparing their spans."}, {"id": 204, "string": "Finally edge labels in Figure 4 are generated."}, {"id": 205, "string": "We use the concatenation of the edge labels in a span list as the state label."}, {"id": 206, "string": "The edge labels are joined in order with ' '."}, {"id": 207, "string": "Empty(0,e) is the state of the edges that do not belong to T (ignoring direction), such as e 12 ."}, {"id": 208, "string": "The variable count of a state is equal to the size of the span list and the direction of a state is decided by whether the edge in T related to the state and its corresponding edge in D have different directions."}, {"id": 209, "string": "For example, the state of e 5 should be ADV PP(2,r)."}, {"id": 210, "string": "Generating statements After the above two steps, we are ready to generate statements according to how spans are merged."}, {"id": 211, "string": "For all nodes, spans of the incoming edges represent the left hand side and the outgoing edges represent the right hand side."}, {"id": 212, "string": "For example, the rule for node comp will be: {ADV(1,r)} comp \u2212 \u2212\u2212 \u2192 {PP(1,u), ADV PP(2,r)} v ADV PP(1,r) = v ADV(1,r) v ADV PP(2,r) = than + v PP(1,u) Extended Rules Extended rules are used when no induced rules can cover a given node."}, {"id": 213, "string": "In theory, there can be unlimited modifier nodes pointing to a given node, such as PP and ADJ."}, {"id": 214, "string": "We use some manually written rules to slightly change an induced rule (prototype) by addition or deletion to generate a group of extended rules."}, {"id": 215, "string": "The motivation here is to deal with the data sparseness problem."}, {"id": 216, "string": "For a group of selected non-head states in I, such as PP and ADJ."}, {"id": 217, "string": "We can produce new rules by removing or duplicating more of them."}, {"id": 218, "string": "For example: {NP(1,u), ADJ(1,r)} X n 1 \u2212 \u2212\u2212\u2212 \u2192 {} v NP(1,u) = v ADJ(1,r) + L As a result, we get the two rules below: {NP(1,u)} X n 1 \u2212 \u2212\u2212\u2212 \u2192 {} v NP(1,u) = L {NP(1,u), ADJ(1,r) 1 , ADJ(1,r) 2 } X n 1 \u2212 \u2212\u2212\u2212 \u2192 {} v NP(1,u) = v ADJ(1,r) 1 + v ADJ(1,r) 2 + L Dynamic Rules During decoding, when neither induced nor extended rule is applicable, we create a dynamic rule on-the-fly."}, {"id": 219, "string": "Our rule creator builds a new rule following the Markov assumption: P (O|C) = P (q 1 |C) n \u220f i=2 P (q i |C)P (q i |q i\u22121 , C) C = \u27e8I, D\u27e9 represents the context."}, {"id": 220, "string": "O = {q 1 , \u00b7 \u00b7 \u00b7 , q n } denotes the outgoing states and I, D have the same meaning as before."}, {"id": 221, "string": "Though they are unordered multisets, we can give them an explicit alphabet order by their edge labels."}, {"id": 222, "string": "There is also a group of hard constraints to make sure that the predicted rules are well-formed as the definition in \u00a75 requires."}, {"id": 223, "string": "This Markovization strategy is widely utilized by lexicalized and unlexicalized PCFG parsers (Collins, 1997; Klein and Manning, 2003) ."}, {"id": 224, "string": "For a dynamic rule, all variables in this rule will appear in the statement."}, {"id": 225, "string": "We use a simple perceptron-based scorer to assign every variable a score and arrange them in an decreasing order."}, {"id": 226, "string": "6 Evaluation and Analysis 6.1 Set-up We use DeepBank 1.1 (Flickinger et al., 2012) , i.e."}, {"id": 227, "string": "gold-standard ERS annotations, as our main experimental data set to train a DAG transducer as well as a sequence-to-sequence morpholyzer, and wikiwoods (Flickinger et al., 2010) , i.e."}, {"id": 228, "string": "automatically-generated ERS annotations by ERG, as additional data set to enhance the sequence-to-sequence morpholyzer."}, {"id": 229, "string": "The training, development and test data sets are from DeepBank and split according to DeepBank's recommendation."}, {"id": 230, "string": "There are 34,505, 1,758 and 1,444 sentences (all disconnected graphs as well as their associated sentences are removed) in the training, development and test data sets."}, {"id": 231, "string": "We use a small portion of wikiwoods data, c.a."}, {"id": 232, "string": "300K sentences, for experiments."}, {"id": 233, "string": "37,537 induced rules are directly extracted from the training data set, and 447,602 extended rules are obtained."}, {"id": 234, "string": "For DAG recognition, at one particular position, there may be more than one rule applicable."}, {"id": 235, "string": "In this case, we need a disambiguation model as well as a decoder to search for a globally optimal solution."}, {"id": 236, "string": "In this work, we train a structured perceptron model (Collins, 2002) for disambiguation and employ a beam decoder."}, {"id": 237, "string": "The perceptron model used by our dynamic rule generator are trained with the induced rules."}, {"id": 238, "string": "To get a sequence-to-sequence model, we use the open source tool-OpenNMT 4 ."}, {"id": 239, "string": "The Decoder We implement a fine-to-coarse beam search decoder."}, {"id": 240, "string": "Given a DAG D, our goal is to find the highest scored labeling function \u03c1: \u03c1 = arg max \u03c1 n \u220f i=1 \u2211 j w j \u00b7 f j (rule(v i ), D) s.t."}, {"id": 241, "string": "rule(v i ) = \u03c1(in(v i )) \u2113(v i ) \u2212 \u2212\u2212 \u2192 \u27e8\u03c1(out(v i )), E i \u27e9 where n is the node count and f j (\u00b7, \u00b7) and w j represent a feature and the corresponding weight, respectively."}, {"id": 242, "string": "The features are chosen from the context of the given node v i ."}, {"id": 243, "string": "We perform 'topdown' search to translate an input DAG into a morphology-function-enhanced lemma sequence."}, {"id": 244, "string": "Each hypothesis consists of the current DAG graph, the partial labeling function, the current hypothesis score and other graph information used to perform rule selection."}, {"id": 245, "string": "The decoder will keep the corresponding partial intermediate graph T acyclic when decoding."}, {"id": 246, "string": "The algorithm used by our decoder is displayed in Algorithm 1."}, {"id": 247, "string": "Function FindRules(h, n, R) will use hard constraints to select rules from the rule set R according to the contextual information."}, {"id": 248, "string": "It will also perform an acyclic check on T ."}, {"id": 249, "string": "Function Insert(h, r, n, B) will create and score a new hypothesis made from the given context and then insert it into beam B. E \u2190 E \u222a {e} 23 if in(tar(e)) \u2286 E: 24 Q \u2190 Q \u222a {tar(e)} 25 Extract \u03c1 from best hypothesis in B1 Accuracy In order to evaluate the effectiveness of our transducer for NLG, we try a group of tests showed in Table 2 ."}, {"id": 250, "string": "All sequence-to-sequence models (either from lemma sequences to lemma sequences or lemma sequences to sentences) are trained on DeepBank and wikiwoods data set and tuned on the development data."}, {"id": 251, "string": "The second column shows the BLEU-4 scores between generated lemma sequences and golden sequences of lemmas."}, {"id": 252, "string": "The third column shows the BLEU-4 scores between generated sentences and golden sentences."}, {"id": 253, "string": "The fourth column shows the fraction of graphs in the test data set that can reach output sentences."}, {"id": 254, "string": "(Song et al., 2017) ."}, {"id": 255, "string": "The graphs that cannot received any natural language sentences are removed while conducting the BLEU evaluation."}, {"id": 256, "string": "As we can conclude from Table 2 , using only induced rules achieves the highest accuracy but the coverage is not satisfactory."}, {"id": 257, "string": "Extended rules lead to a slight accuracy drop but with a great improvement of coverage (c.a."}, {"id": 258, "string": "10%)."}, {"id": 259, "string": "Using dynamic rules, we observe a significant accuracy drop."}, {"id": 260, "string": "Nevertheless, we are able to handle all EDS graphs."}, {"id": 261, "string": "The full-coverage robustness may benefit many NLP applications."}, {"id": 262, "string": "The lemma sequences generated by our transducer are really close to the golden one."}, {"id": 263, "string": "This means that our model actually works and most reordering patterns are handled well by induced rules."}, {"id": 264, "string": "Compared to the AMR generation task, our transducer on EDS graphs achieves much higher accuracies."}, {"id": 265, "string": "To make clear how much improvement is from the data and how much is from our DAG transducer, we implement a purely neural baseline."}, {"id": 266, "string": "The baseline converts a DAG into a concept sequence by a pre-order DFS traversal on the intermediate tree of this DAG."}, {"id": 267, "string": "Then we use a sequenceto-sequence model to transform this concept sequence to the lemma sequence for comparison."}, {"id": 268, "string": "This is a kind of implementation of Konstas et al."}, {"id": 269, "string": "'s model but evaluated on the EDS data."}, {"id": 270, "string": "We can see that on this task, our transducer is much better than a pure sequence-to-sequence model on DeepBank data."}, {"id": 271, "string": "Table 3 : Efficiency of our NL generator."}, {"id": 272, "string": "Table 3 shows the efficiency of the beam search decoder with a beam size of 128."}, {"id": 273, "string": "The platform for this experiment is x86 64 GNU/Linux with two Intel Xeon E5-2620 CPUs."}, {"id": 274, "string": "The second and third columns represent the average and the maximal time (in seconds) to translate an EDS graph."}, {"id": 275, "string": "Using dynamic rules slow down the decoder to a great degree."}, {"id": 276, "string": "Since the data for experiments is newswire data, i.e."}, {"id": 277, "string": "WSJ sentences from PTB (Marcus et al., 1993) , the input graphs are quite large on average."}, {"id": 278, "string": "On average, it produces more than 5 sentences per second on CPU."}, {"id": 279, "string": "We consider this a promising speed."}, {"id": 280, "string": "Efficiency Conclusion We extend the work on DAG automata in Chiang et al."}, {"id": 281, "string": "(2018) and propose a general method to build flexible DAG transducer."}, {"id": 282, "string": "The key idea is to leverage a declarative programming language to minimize the computation burden of a graph transducer."}, {"id": 283, "string": "We think may NLP tasks that involve graph manipulation may benefit from this design."}, {"id": 284, "string": "To exemplify our design, we develop a practical system for the semantic-graph-to-string task."}, {"id": 285, "string": "Our system is accurate (BLEU 68.07), efficient (more than 5 sentences per second on a CPU) and robust (fullcoverage)."}, {"id": 286, "string": "The empirical evaluation confirms the usefulness a DAG transducer to resolve NLG, as well as the effectiveness of our design."}], "headers": [{"section": "Introduction", "n": "1", "start": 0, "end": 36}, {"section": "Preliminaries", "n": "2.1", "start": 37, "end": 46}, {"section": "Previous Work", "n": "2.2", "start": 47, "end": 59}, {"section": "Challenges", "n": "2.3", "start": 60, "end": 76}, {"section": "Basic Idea", "n": "3.1", "start": 77, "end": 85}, {"section": "A Declarative Programming Language", "n": "3.2", "start": 86, "end": 97}, {"section": "Informal Illustration", "n": "3.3", "start": 98, "end": 136}, {"section": "Definition", "n": "3.4", "start": 137, "end": 148}, {"section": "DAG Transduction-based NLG", "n": "4", "start": 149, "end": 161}, {"section": "Inducing Transduction Rules", "n": "5", "start": 162, "end": 169}, {"section": "EDS-specific Constraints", "n": "5.1", "start": 170, "end": 182}, {"section": "Fine-to-Coarse Transduction", "n": "5.2", "start": 183, "end": 190}, {"section": "Induced Rules", "n": "5.3", "start": 191, "end": 212}, {"section": "Extended Rules", "n": "5.4", "start": 213, "end": 218}, {"section": "Dynamic Rules", "n": "5.5", "start": 219, "end": 238}, {"section": "The Decoder", "n": "6.2", "start": 239, "end": 249}, {"section": "Accuracy", "n": "6.3", "start": 250, "end": 279}, {"section": "Conclusion", "n": "7", "start": 280, "end": 286}], "figures": [{"filename": "../figure/image/1060-Table2-1.png", "caption": "Table 2: Accuracy (BLEU-4 score) and coverage of different systems. I denotes transduction only using induced rules; I+E denotes transduction using both induced and extended rules; I+E+D denotes transduction using all kinds of rules. DFSNN is a rough implementation of Konstas et al. (2017) but with the EDS data, while AMR-NN includes the results originally reported by Konstas et al., which are evaluated on the AMR data. AMR-NRG includes the results obtained by a synchronous graph grammar (Song et al., 2017).", "page": 8, "bbox": {"x1": 72.0, "x2": 295.2, "y1": 62.879999999999995, "y2": 160.32}}, {"filename": "../figure/image/1060-Table3-1.png", "caption": "Table 3: Efficiency of our NL generator.", "page": 8, "bbox": {"x1": 321.59999999999997, "x2": 511.2, "y1": 62.879999999999995, "y2": 119.03999999999999}}, {"filename": "../figure/image/1060-Figure4-1.png", "caption": "Figure 4: An example graph. The intended reading is \u201cthe decline is even steeper than in September\u201d, he said. Original edge labels are removed for clarity. Every edge is associated with a span list, and spans are written in the form label<begin:end>. The red dashed edges belong to the intermediate graph T .", "page": 5, "bbox": {"x1": 110.88, "x2": 485.28, "y1": 65.28, "y2": 192.95999999999998}}, {"filename": "../figure/image/1060-Table1-1.png", "caption": "Table 1: Sets of states (Q) and rules (R) that can be used to process the graph in Figure 2.", "page": 4, "bbox": {"x1": 115.67999999999999, "x2": 481.44, "y1": 62.879999999999995, "y2": 156.0}}, {"filename": "../figure/image/1060-Figure1-1.png", "caption": "Figure 1: Variable relation tree.", "page": 3, "bbox": {"x1": 106.08, "x2": 253.92, "y1": 214.07999999999998, "y2": 274.08}}, {"filename": "../figure/image/1060-Figure3-1.png", "caption": "Figure 3: A run of the graph in Figure 2.", "page": 3, "bbox": {"x1": 332.64, "x2": 497.76, "y1": 181.44, "y2": 270.71999999999997}}, {"filename": "../figure/image/1060-Figure2-1.png", "caption": "Figure 2: An input graph. The intended reading is John wants to go.", "page": 3, "bbox": {"x1": 346.56, "x2": 484.32, "y1": 62.879999999999995, "y2": 128.64}}]}