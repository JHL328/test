,slide title,number of gt figures,content,figures
0,Semantic Similarity Task,0,"The task of determining the degree of equivalence between the semantics of two sentences. An example being ""to watch a film"" and ""to see a movie."" Although relatively easy for humans, this task remains one of the most difficult natural language understanding problems.",
1,Multi Relational Semantic Similarity Task,0,This marks the similarity score of multiple annotations for sentence pairs and is motivated by different possible relations between two sentences. Current models are evaluating distinct relations separately. Hypothesis: each relation contains useful information and are interconnected. We propose joint multi-label to improve overall performance.,
2,Human Activity,0,"One of the datasets we will be evaluating on (Wilson and Mihalcea, 2017). Contains 4 relations: Similairty, Relatedness, Motivational alignment, and Perceived actor congruence.",
3,Sick,0,"Sentences Involving Compositional Knowledge (Marelli et al., 2014). Contains large number of sentence pairs that are rich in their annotations.",
4,Typed Similarity,0,"(Agirre et al., 2013). A collection of meta-data describing books, paintings, films, museum objects and archival records taken from Europeana. ",
5,Existing Model Single Task,1,"Past models treat each relation independently, in which each relation's model is trained and tested, ignoring the annotations of all other relations.",
6,Proposed Multi Label Model,1,"Multi Label Learning. Pick a batch of sentences pairs, consider both Label L and Label R, calculate Loss L and Loss R, aggregate them as the total loss, and update all parameters.",Fig 1
7,Alternative Multi Task Model,1,Only one relation is involved during each round of feed-forward and back-propagation.,
8,Comparison Between the Models,0,"The key difference between multi-task and our multi-label learning method are in the forward-backward pass and update steps of the model. The multi-task only trains on a single relation per batch, whereas the multi-label trains jointly on multiple relations simultaneously.",
9,Results,2,Multi-label learning is superior in almost all tasks (significance = 0.05) on all 3 datasets.,"Table 1, 2, 3"
10,Discussion and Conclusion,0,"An interesting case to note is the general similairty on the Typed-Similairty dataset, in which the single task performs better. This can be attributed to the past model's use of information retrieval techniques like named entity recognition. A combination of this and sentence embeddings might achieve more optimal scores. In conclusion, we introduced a multi-label transfer learning setting designed specifically for semantic similarity tasks with multiple relations annotations that outperforms existing state-of-the-art methods across 3 datasets.",