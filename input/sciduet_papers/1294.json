{"title": "Taylor's Law for Human Linguistic Sequences", "abstract": "Taylor's law describes the fluctuation characteristics underlying a system in which the variance of an event within a time span grows by a power law with respect to the mean. Although Taylor's law has been applied in many natural and social systems, its application for language has been scarce. This article describes a new quantification of Taylor's law in natural language and reports an analysis of over 1100 texts across 14 languages. The Taylor exponents of written natural language texts were found to exhibit almost the same value. The exponent was also compared for other language-related data, such as the child-directed speech, music, and programming language code. The results show how the Taylor exponent serves to quantify the fundamental structural complexity underlying linguistic time series. The article also shows the applicability of these findings in evaluating language models.", "text": [{"id": 0, "string": "Introduction Taylor's law characterizes how the variance of the number of events for a given time and space grows with respect to the mean, forming a power law."}, {"id": 1, "string": "It is a quantification method for the clustering behavior of a system."}, {"id": 2, "string": "Since the pioneering studies of this concept (Smith, 1938; Taylor, 1961) , a substantial number of studies have been conducted across various domains, including ecology, life science, physics, finance, and human dynamics, as well summarized in (Eisler, Bartos, and Kert\u00e9sz, 2007) ."}, {"id": 3, "string": "More recently, Cohen and Xu (2015) reported Taylor exponents for random sampling from various distributions, and Calif and Schmitt (2015) reported Taylor's law in wind energy data using a non-parametric regression."}, {"id": 4, "string": "Those two papers also refer to research about Taylor's law in a wide range of fields."}, {"id": 5, "string": "Despite such diverse application across domains, there has been little analysis based on Taylor's law in studying natural language."}, {"id": 6, "string": "The only such report, to the best of our knowledge, is Gerlach and Altmann (2014) , but they measured the mean and variance by means of the vocabulary size within a document."}, {"id": 7, "string": "This approach essentially differs from the original concept of Taylor analysis, which fundamentally counts the number of events, and thus the theoretical background of Taylor's law as presented in Eisler, Bartos, and Kert\u00e9sz (2007) cannot be applied to interpret the results."}, {"id": 8, "string": "For the work described in this article, we applied Taylor's law for texts, in a manner close to the original concept."}, {"id": 9, "string": "We considered lexical fluctuation within texts, which involves the cooccurrence and burstiness of word alignment."}, {"id": 10, "string": "The results can thus be interpreted according to the analytical results of Taylor's law, as described later."}, {"id": 11, "string": "We found that the Taylor exponent is indeed a characteristic of texts and is universal across various kinds of texts and languages."}, {"id": 12, "string": "These results are shown here for data including over 1100 singleauthor texts across 14 languages and large-scale newspaper data."}, {"id": 13, "string": "Moreover, we found that the Taylor exponents for other symbolic sequential data, including child-directed speech, programming language code, and music, differ from those for written natural language texts, thus distinguishing different kinds of data sources."}, {"id": 14, "string": "The Taylor exponent in this sense could categorize and quantify the structural complexity of language."}, {"id": 15, "string": "The Chomsky hierarchy (Chomsky, 1956 ) is, of course, the most important framework for such categorization."}, {"id": 16, "string": "The Taylor exponent is another way to quantify the complexity of natural language: it allows for continuous quantification based on lexical fluctuation."}, {"id": 17, "string": "Since the Taylor exponent can quantify and characterize one aspect of natural language, our findings are applicable in computational linguistics to assess language models."}, {"id": 18, "string": "At the end of this article, in \u00a75, we report how the most basic character-based long short-term memory (LSTM) unit produces texts with a Taylor exponent of 0.50, equal to that of a sequence of independent and identically distributed random variables (an i.i.d."}, {"id": 19, "string": "sequence)."}, {"id": 20, "string": "This shows how such models are limited in producing consistent co-occurrence among words, as compared with a real text."}, {"id": 21, "string": "Taylor analysis thus provides a possible direction to reconsider the limitations of language models."}, {"id": 22, "string": "Related Work This work can be situated as a study to quantify the complexity underlying texts."}, {"id": 23, "string": "As summarized in (Tanaka-Ishii and Aihara, 2015), measures for this purpose include the entropy rate (Takahira, Tanaka-Ishii, and Lukasz, 2016; Bentz et al., 2017) and those related to the scaling behaviors of natural language."}, {"id": 24, "string": "Regarding the latter, certain power laws are known to hold universally in linguistic data."}, {"id": 25, "string": "The most famous among these are Zipf's law (Zipf, 1965) and Heaps' law (Heaps, 1978) ."}, {"id": 26, "string": "Other, different kinds of power laws from Zipf's law are obtained through various methods of fluctuation analysis, but the question of how to quantify the fluctuation existing in language data has been controversial."}, {"id": 27, "string": "Our work is situated as one such case of fluctuation analysis."}, {"id": 28, "string": "In real data, the occurrence timing of a particular event is often biased in a bursty, clustered manner, and fluctuation analysis quantifies the degree of this bias."}, {"id": 29, "string": "Originally, this was motivated by a study of how floods of the Nile River occur in clusters (i.e., many floods coming after an initial flood) (Hurst, 1951) ."}, {"id": 30, "string": "Such clustering phenomena have been widely reported in both natural and social domains (Eisler, Bartos, and Kert\u00e9sz, 2007) ."}, {"id": 31, "string": "Fluctuation analysis for language originates in (Ebeling and P\u00f6eschel, 1994) , which applied the approach to characters."}, {"id": 32, "string": "That work corresponds to observing the average of the variances of each character's number of occurrences within a time span."}, {"id": 33, "string": "Their method is strongly related to ours but different from two viewpoints: (1) Taylor analysis considers the variance with respect to the mean, rather than time; and (2) Taylor analysis does not average results over all elements."}, {"id": 34, "string": "Because of these differences, the method in (Ebeling and P\u00f6eschel, 1994) cannot distinguish real texts from an i.i.d."}, {"id": 35, "string": "process when applied to word sequences (Takahashi and Tanaka-Ishii, 2018) ."}, {"id": 36, "string": "Event clustering phenomena cause a sequence to resemble itself in a self-similar manner."}, {"id": 37, "string": "Therefore, studies of the fluctuation underlying a sequence can take another form of long-range correlation analysis, to consider the similarity between two subsequences underlying a time series."}, {"id": 38, "string": "This approach requires a function to calculate the similarity of two sequences, and the autocorrelation function (ACF) is the main function considered."}, {"id": 39, "string": "Since the ACF only applies to numerical data, both Altmann, Pierrehumbert, and Motter (2009) and Tanaka-Ishii and Bunde (2016) applied long-range correlation analysis by transforming text into intervals and showed how natural language texts are long-range correlated."}, {"id": 40, "string": "Another recent work (Lin and Tegmark, 2016) proposed using mutual information instead of the ACF."}, {"id": 41, "string": "Mutual information, however, cannot detect the long-range correlation underlying texts."}, {"id": 42, "string": "All these works studied correlation phenomena via only a few texts and did not show any underlying universality with respect to data and language types."}, {"id": 43, "string": "One reason is that analysis methods for long-range correlation are nontrivial to apply to texts."}, {"id": 44, "string": "Overall, the analysis based on Taylor's law in the present work belongs to the former approach of fluctuation analysis and shows the law's vast applicability and stability for written texts and even beyond, quantifying universal complexity underlying human linguistic sequences."}, {"id": 45, "string": "Measuring the Taylor Exponent Proposed method Given a set of elements W (words), let X = X 1 , X 2 , ."}, {"id": 46, "string": "."}, {"id": 47, "string": "."}, {"id": 48, "string": ", X N be a discrete time series of length N , where X i \u2208 W for all i = 1, 2, ."}, {"id": 49, "string": "."}, {"id": 50, "string": "."}, {"id": 51, "string": ", N , i.e., each X i represents a word."}, {"id": 52, "string": "For a given segment length \u2206t \u2208 N (a positive integer), a data sample X is segmented by the length \u2206t."}, {"id": 53, "string": "The number of occurrences of a specific word w k \u2208 W is counted for every segment, and the mean \u00b5 k and standard deviation \u03c3 k across segments are obtained."}, {"id": 54, "string": "Doing this for all word kinds w 1 , ."}, {"id": 55, "string": "."}, {"id": 56, "string": "."}, {"id": 57, "string": ", w |W | \u2208 W gives the distribution of \u03c3 with respect to \u00b5."}, {"id": 58, "string": "Following a previous work (Eisler, Bartos, and Kert\u00e9sz, 2007) , in this article Taylor's law is defined to hold when \u00b5 and \u03c3 are correlated by a power law in the following way: \u03c3 \u221d \u00b5 \u03b1 ."}, {"id": 59, "string": "(1) Experimentally, the Taylor exponent \u03b1 is known to take a value within the range of 0.5 \u2264 \u03b1 \u2264 1.0 across a wide variety of domains as reported in (Eisler, Bartos, and Kert\u00e9sz, 2007) , including finance, meteorology, agriculture, and biology."}, {"id": 60, "string": "Mathematically, it is analytically proven that \u03b1 = 0.5 for an i.i.d process, and the proof is included as Supplementary Material."}, {"id": 61, "string": "On the other hand, \u03b1 = 1.0 when all segments always contain the same proportion of the elements of W ."}, {"id": 62, "string": "For example, suppose that W = {a, b}."}, {"id": 63, "string": "If b always occurs twice as often as a in all segments (e.g., three a and six b in one segment, two a and four b in another, etc."}, {"id": 64, "string": "), then both the mean and standard deviation for b are twice those for a, so the exponent is 1.0."}, {"id": 65, "string": "In a real text, this cannot occur for all W , so \u03b1 < 1.0 for natural language text."}, {"id": 66, "string": "Nevertheless, for a subset of words in W , this could happen, especially for a template-like sequence."}, {"id": 67, "string": "For instance, consider a programming statement: while (i < 1000) do i-."}, {"id": 68, "string": "Here, the words while and do always occur once, whereas i always occurs twice."}, {"id": 69, "string": "This example shows that the exponent indicates how consistently words depend on each other in W , i.e., how words co-occur systematically in a coherent manner, thus indicating that the Taylor exponent is partly related to grammaticality."}, {"id": 70, "string": "To measure the Taylor exponent \u03b1, the mean and standard deviation are computed for every word kind 1 and then plotted in log-log coordinates."}, {"id": 71, "string": "The number of points in this work was the number of different words."}, {"id": 72, "string": "We fitted the points to a linear function in log-log coordinates by the least-squares method."}, {"id": 73, "string": "We naturally took the logarithm of both c\u00b5 \u03b1 and \u03c3 to estimate the exponent, because Taylor's law is a power law."}, {"id": 74, "string": "The coefficient\u0109, and exponent\u03b1 are then estimated as the 1 In this work, words are not lemmatized, e.g."}, {"id": 75, "string": "\"say,\" \"said,\" and \"says\" are all considered different words."}, {"id": 76, "string": "This was chosen so in this work because the Taylor exponent considers systematic co-occurrence of words, and idiomatic phrases should thus be considered in their original forms."}, {"id": 77, "string": "following: c,\u03b1 = arg min c,\u03b1 \u03f5(c, \u03b1), \u03f5(c, \u03b1) = 1 |W | |W | \u2211 k=1 (log \u03c3 k \u2212 log c\u00b5 \u03b1 k ) 2 ."}, {"id": 78, "string": "This fit function could be a problem depending on the distribution of errors between the data points and the regression line."}, {"id": 79, "string": "As seen later, the error distribution seems to differ with the kind of data: for a random source the error seems Gaussian, and so the above formula is relevant, whereas for real data, the distribution is biased."}, {"id": 80, "string": "Changing the fit function according to the data source, however, would cause other essential problems for fair comparison."}, {"id": 81, "string": "Here, because Cohen and Xu (2015) reported that most empirical works on Taylor's law used least-squares regression (including their own), this work also uses the above scheme 2 , with the error defined as \u03f5(\u0109,\u03b1)."}, {"id": 82, "string": "large representative archives, parsed, and stripped of natural language comments), and 12 pieces of musical data (long symphonies and so forth, transformed from MIDI into text with the software SMF2MML 5 , with annotations removed)."}, {"id": 83, "string": "As for the randomized data listed in the last block, we took the text of Moby Dick and generated 10 different shuffled samples and bigramgenerated sequences."}, {"id": 84, "string": "We also introduced LSTMgenerated texts to consider the utility of our findings, as explained in \u00a75."}, {"id": 85, "string": "Figure 1 shows typical distributions for natural language texts, with two single-author texts ((a) 5 http://shaw.la.coocan.jp/smf2mml/ and (b)) and two multiple-author texts (newspapers, (c) and (d)), in English and Chinese, respectively."}, {"id": 86, "string": "The segment size was \u2206t = 5620 words 6 , i.e., each segment had 5620 words and the horizontal axis indicates the averaged frequency of a specific word within a segment of 5620 words."}, {"id": 87, "string": "Data Taylor Exponents for Real Data The points at the upper right represent the most frequent words, whereas those at the lower left represent the least frequent."}, {"id": 88, "string": "Although the plots exhibited different distributions, they could globally be considered roughly aligned in a power-law manner."}, {"id": 89, "string": "This finding is non-trivial, as seen in other analyses based on Taylor's law (Eisler, Bartos, and Kert\u00e9sz, 2007) ."}, {"id": 90, "string": "The exponent \u03b1 was almost the same even though English and Chinese are different languages using different kinds of script."}, {"id": 91, "string": "As explained in \u00a73.1, the Taylor exponent indicates the degree of consistent co-occurrence among words."}, {"id": 92, "string": "The value of 0.58 obtained here suggests that the words of natural language texts are not strongly or consistently coherent with respect to each other."}, {"id": 93, "string": "Nevertheless, the value is well above 0.5, and for the real data listed in Table 1 (first to third blocks), not a single sample gave an exponent as low as 0.5."}, {"id": 94, "string": "Although the overall global tendencies in Figure 1 followed power laws, many points deviated significantly from the regression lines."}, {"id": 95, "string": "The words with the greatest fluctuation were often keywords."}, {"id": 96, "string": "For example, among words in Moby Dick with large \u00b5, those with the largest \u03c3 included whale, captain, and sailor, whereas those with the smallest \u03c3 included functional words such as to, that, and with."}, {"id": 97, "string": "The Taylor exponent depended only slightly on the data size."}, {"id": 98, "string": "Figure 2 shows this dependency Figure 2: Taylor exponent\u03b1 (vertical axis) calculated for the two largest texts: The New York Times and The Mainichi newspapers."}, {"id": 99, "string": "To evaluate the exponent's dependence on the text size, parts of each text were taken and the exponents were calculated for those parts, with points taken logarithmically."}, {"id": 100, "string": "The window size was \u2206t = 5620."}, {"id": 101, "string": "As the text size grew, the Taylor exponent slightly decreased."}, {"id": 102, "string": "for the two largest data sets used, The New York Times (NYT, 1.5 billion words) and The Mainichi (24 years) newspapers."}, {"id": 103, "string": "When the data size was increased, the exponent exhibited a slight tendency to decrease."}, {"id": 104, "string": "For the NYT, the decrease seemed to have a lower limit, as the figure shows that the exponent stabilized at around 10 7 words."}, {"id": 105, "string": "The reason for this decrease can be explained as follows."}, {"id": 106, "string": "The Taylor exponent becomes larger when some words occur in a clustered manner."}, {"id": 107, "string": "Making the text size larger increases the number of segments (since \u2206t was fixed in this experiment)."}, {"id": 108, "string": "If the number of clusters does not increase as fast as the increase in the number of segments, then the number of clusters per segment becomes smaller, leading to a smaller exponent."}, {"id": 109, "string": "In other words, the influence of each consecutive co-occurrence of a particular word decays slightly as the overall text size grows."}, {"id": 110, "string": "Analysis of different kinds of data showed how the Taylor exponent differed according to the data source."}, {"id": 111, "string": "Figure 3 shows plots for samples from enwiki8 (tagged Wikipedia), the child-directed speech of Thomas (taken from CHILDES), programming language data sets, and music."}, {"id": 112, "string": "The distributions appear different from those for the natural language texts, and the exponents were significantly larger."}, {"id": 113, "string": "This means that these data sets contained expressions with fixed forms much more frequently than did the natural language texts."}, {"id": 114, "string": "Figure 4 summarizes the overall picture among the different data sources."}, {"id": 115, "string": "The median and quantiles of the Taylor exponent were calculated for the different kinds of data listed in Table 1 ."}, {"id": 116, "string": "The first two boxes show results with an exponent of 0.50."}, {"id": 117, "string": "These results were each obtained from 10 random samples of the randomized sequences."}, {"id": 118, "string": "We will return to these results in the next section."}, {"id": 119, "string": "The remaining boxes show results for real data."}, {"id": 120, "string": "The exponents for texts from Project Gutenberg ranged from 0.53 to 0.68."}, {"id": 121, "string": "Figure 5 shows a histogram of these texts with respect to the value of \u03b1."}, {"id": 122, "string": "The number of texts decreased significantly at a value of 0.63, showing that the distribution of the Taylor exponent was rather tight."}, {"id": 123, "string": "The kinds of texts at the upper limit of exponents for Project Gutenberg included structured texts of fixed style, such as dictionaries, lists of histories, and Bibles."}, {"id": 124, "string": "The majority of texts were in English, followed by French and then other languages, as listed in Table 1 ."}, {"id": 125, "string": "Whether \u03b1 distinguishes languages is a difficult question."}, {"id": 126, "string": "The histogram suggests that Chinese texts exhibited larger values than did texts in Indo-European languages."}, {"id": 127, "string": "We conducted a statistical test to evaluate whether this difference was significant as compared to English."}, {"id": 128, "string": "Since the numbers of texts were very different, we used the non-parametric statistical test of the Brunner-Munzel method, among various possible methods, to test a null hypothesis of whether \u03b1 was equal for the two distributions (Brunner and Munzel, 2000) ."}, {"id": 129, "string": "The p-value for Chinese was p = 1.24 \u00d7 10 \u221216 , thus rejecting the null hypothesis at the significance level of 0.01."}, {"id": 130, "string": "This confirms that \u03b1 was generally larger for Chinese texts than for English texts."}, {"id": 131, "string": "Similarly, the null hypothesis was rejected for Finnish and French, but it was accepted for German and Japanese at the 0.01 significance level."}, {"id": 132, "string": "Since Japanese was accepted despite its large difference from English, we could not conclude whether the Taylor exponent distinguishes languages."}, {"id": 133, "string": "Turning to the last four columns of Figure 4 , representing the enwiki8, child-directed speech (CHILDES), programming language, and music data, the Taylor exponents clearly differed from those of the natural language texts."}, {"id": 134, "string": "Given the template-like nature of these four data sources, the results were somewhat expected."}, {"id": 135, "string": "The kind of data thus might be distinguishable using the Taylor exponent."}, {"id": 136, "string": "To confirm this, however, would require assembling a larger data set."}, {"id": 137, "string": "Applying this approach with Twitter data and adult utterances would produce interesting results and remains for our future work."}, {"id": 138, "string": "The Taylor exponent also differed according to \u2206t, and Figure 6 shows the dependence of\u03b1 on \u2206t."}, {"id": 139, "string": "For each kind of data shown in Figure 4 , the mean exponent is plotted for various \u2206t."}, {"id": 140, "string": "As reported in (Eisler, Bartos, and Kert\u00e9sz, 2007) , the exponent is known to grow when the segment size gets larger."}, {"id": 141, "string": "The reason is that words occur in a bursty, clustered manner at all length scales: no matter how large the segment size becomes, a segment will include either many or few instances of a given word, leading to larger variance growth."}, {"id": 142, "string": "This phenomenon suggests how word cooccurrences in natural language are self-similar."}, {"id": 143, "string": "The Taylor exponent is initially 0.5 when the segment size is very small."}, {"id": 144, "string": "This can be analytically explained as follows (Eisler, Bartos, and Kert\u00e9sz, 2007) ."}, {"id": 145, "string": "Consider the case of \u2206t=1."}, {"id": 146, "string": "Let n be the frequency of a particular word in a segment."}, {"id": 147, "string": "We have \u27e8n\u27e9 \u226a 1.0, because the possibility of a specific word appearing in a segment becomes very small."}, {"id": 148, "string": "Because \u27e8n\u27e9 2 \u2248 0, \u03c3 2 = \u27e8n 2 \u27e9 \u2212 \u27e8n\u27e9 2 \u2248 \u27e8n 2 \u27e9."}, {"id": 149, "string": "Because n = 1 or 0 (with \u2206t=1), \u27e8n 2 \u27e9 = \u27e8n\u27e9 = \u00b5."}, {"id": 150, "string": "Thus, \u03c3 2 \u2248 \u00b5."}, {"id": 151, "string": "Overall, the results show the possibility of ap- Figure 4 : Box plots of the Taylor exponents for different kinds of data."}, {"id": 152, "string": "Each point represents one sample, and samples from the same kind of data are contained in each box plot."}, {"id": 153, "string": "The first two boxes are for the randomized data, while the remaining boxes are for real data, including both the natural language texts and language-related sequences."}, {"id": 154, "string": "Each box ranges between the quantiles, with the middle line indicating the median, the whiskers showing the maximum and minimum, and some extreme values lying beyond."}, {"id": 155, "string": "Figure 5 : Histogram of Taylor exponents for long texts in Project Gutenberg (1129 texts)."}, {"id": 156, "string": "The legend indicates the languages, in frequency order."}, {"id": 157, "string": "Each bar shows the number of texts with that value of\u03b1."}, {"id": 158, "string": "Because of the skew of languages in the original conception of Project Gutenberg, the majority of the texts are in English, shown in blue, whereas texts in other languages are shown in other colors."}, {"id": 159, "string": "The histogram shows how the Taylor exponent ranged fairly tightly around the mean, and natural language texts with an exponent larger than 0.63 were rare."}, {"id": 160, "string": "plying Taylor's exponent to quantify the complexity underlying coherence among words."}, {"id": 161, "string": "Grammatical complexity was formalized by Chomsky via the Chomsky hierarchy (Chomsky, 1956) , which describes grammar via rewriting rules."}, {"id": 162, "string": "The constraints placed on the rules distinguish four different levels of grammar: regular, context-free, context-sensitive, and phrase structure."}, {"id": 163, "string": "As indicated in (Badii and Politi, 1997) , however, this does not quantify the complexity on a continuous scale."}, {"id": 164, "string": "For example, we might want to quantify the complexity of child-directed speech as compared to that of adults, and this could be addressed in only a limited way through the Chomsky hierarchy."}, {"id": 165, "string": "Another point is that the hierarchy is sentence-based and does not consider fluctuation in the kinds of words appearing."}, {"id": 166, "string": "Evaluation of Machine-Generated Text by the Taylor Exponent The main contribution of this paper is the findings of Taylor's law behavior for real texts as presented thus far."}, {"id": 167, "string": "This section explains the applicability of these findings, through results obtained with baseline language models."}, {"id": 168, "string": "As mentioned previously, i.i.d."}, {"id": 169, "string": "mathematical processes have a Taylor exponent of 0.50."}, {"id": 170, "string": "We show here that, even if a process is not trivially i.i.d., the exponent often takes a value of 0.50 Figure 6 : Growth of\u03b1 with respect to \u2206t, averaged across data sets within each data kind."}, {"id": 171, "string": "The plot labeled \"random\" shows the average for the two datasets of randomized text from Moby Dick (shuffled and bigrams, as explained in \u00a75)."}, {"id": 172, "string": "Since this analysis required a large amount of computation, for the large data sets (such as newspaper and programming language data), 4 million words were taken from each kind of data and used here."}, {"id": 173, "string": "When \u2206t was small, the Taylor exponent was close to 0.5, as theoretically described in the main text."}, {"id": 174, "string": "As \u2206t was increased, the value of\u03b1 grew."}, {"id": 175, "string": "The maximum \u2206t was about 10,000, or about one-tenth of the length of one long literary text."}, {"id": 176, "string": "For the kinds of data investigated here,\u03b1 grew almost linearly."}, {"id": 177, "string": "The results show that, at a given \u2206t, the Taylor exponent has some capability to distinguish different kinds of text data."}, {"id": 178, "string": "for random processes, including texts produced by standard language models such as n-gram based models."}, {"id": 179, "string": "A more complete work in this direction is reported in (Takahashi and Tanaka-Ishii, 2018) ."}, {"id": 180, "string": "Figure 7 shows samples from each of two simple random processes."}, {"id": 181, "string": "Figure 7a shows the behavior of a shuffled text of Moby Dick."}, {"id": 182, "string": "Obviously, (a) Text produced by LSTM (3-layer stacked character-based) (b) Machine-translated text using neural language model Figure 8 : Taylor analysis for two texts produced by standard neural language models: (a) a stacked LSTM model that learned the complete works of Shakespeare; and (b) a machine translation of Les Mis\u00e9rables (originally in French, translated into English), from a neural language model."}, {"id": 183, "string": "since the sequence was almost i.i.d."}, {"id": 184, "string": "following Zipf distribution, the Taylor exponent was 0.50."}, {"id": 185, "string": "Given that the Taylor exponent becomes larger for a sequence with words dependent on each other, as explained in \u00a73, we would expect that a sequence generated by an n-gram model would exhibit an exponent larger than 0.50."}, {"id": 186, "string": "The simplest such model is the bigram model, so a sequence of 300,000 words was probabilistically generated using a bigram model of Moby Dick."}, {"id": 187, "string": "Figure 7b shows the Taylor analysis, revealing that the exponent remained 0.50."}, {"id": 188, "string": "This result does not depend much on the quality of the individual samples."}, {"id": 189, "string": "The first and second box plots in Figure 4 show the distribution of exponents for 10 different samples for the shuffled and bigram-generated texts, respectively."}, {"id": 190, "string": "The exponents were all around 0.50, with small variance."}, {"id": 191, "string": "State-of-the-art language models are based on neural models, and they are mainly evaluated by perplexity and in terms of the performance of individual applications."}, {"id": 192, "string": "Since their architecture is complex, quality evaluation has become an issue."}, {"id": 193, "string": "One possible improvement would be to use an evaluation method that qualitatively differs from judging application performance."}, {"id": 194, "string": "One such method is to verify whether the properties underlying natural language hold for texts generated by language models."}, {"id": 195, "string": "The Taylor exponent is one such possibility, among various properties of natural language texts."}, {"id": 196, "string": "As a step toward this approach, Figure 8 shows two results produced by neural language models."}, {"id": 197, "string": "Figure 8a shows the result for a sample of 2 million characters produced by a stan-dard (three-layer) stacked character-based LSTM unit that learned the complete works of Shakespeare."}, {"id": 198, "string": "The model was optimized to minimize the cross-entropy with a stochastic gradient algorithm to predict the next character from the previous 128 characters."}, {"id": 199, "string": "See (Takahashi and Tanaka-Ishii, 2017) for the details of the experimental settings."}, {"id": 200, "string": "The Taylor exponent of the generated text was 0.50."}, {"id": 201, "string": "This indicates that the character-level language model could not capture or reproduce the word-level clustering behavior in text."}, {"id": 202, "string": "This analysis sheds light on the quality of the language model, separate from the prediction accuracy."}, {"id": 203, "string": "The application of Taylor's law for a wider range of language models appears in (Takahashi and Tanaka-Ishii, 2018) ."}, {"id": 204, "string": "Briefly, state-of-theart word-level language models can generate text whose Taylor exponent is larger than 0.50 but smaller than that of the dataset used for training."}, {"id": 205, "string": "This indicates both the capability of modeling burstiness in text and the room for improvement."}, {"id": 206, "string": "Also, the perplexity values correlate well with the Taylor exponents."}, {"id": 207, "string": "Therefore, Taylor exponent can reasonably serve for evaluating machinegenerated text."}, {"id": 208, "string": "In contrast to character-level neural language models, neural-network-based machine translation (NMT) models are, in fact, capable of maintaining the burstiness of the original text."}, {"id": 209, "string": "Figure 8b shows the Taylor analysis for a machinetranslated text of Les Mis\u00e9rables (from French to English), obtained from Google NMT (Wu et al., 2016) ."}, {"id": 210, "string": "We split the text into 5000-character portions because of the API's limitation (See (Takahashi and Tanaka-Ishii, 2017) for the details)."}, {"id": 211, "string": "As is expected and desirable, the translated text retains the clustering behavior of the original text, as the Taylor exponent of 0.57 is equivalent to that of the original text."}, {"id": 212, "string": "Conclusion We have proposed a method to analyze whether a natural language text follows Taylor's law, a scaling property quantifying the degree of consistent co-occurrence among words."}, {"id": 213, "string": "In our method, a sequence of words is divided into given segments, and the mean and standard deviation of the frequency of every kind of word are measured."}, {"id": 214, "string": "The law is considered to hold when the standard deviation varies with the mean according to a power law, thus giving the Taylor exponent."}, {"id": 215, "string": "Theoretically, an i.i.d."}, {"id": 216, "string": "process has a Taylor exponent of 0.5, whereas larger exponents indicate sequences in which words co-occur systematically."}, {"id": 217, "string": "Using over 1100 texts across 14 languages, we showed that written natural language texts follow Taylor's law, with the exponent distributed around 0.58."}, {"id": 218, "string": "This value differed greatly from the exponents for other data sources: enwiki8 (tagged Wikipedia, 0.63), child-directed speech (CHILDES, around 0.68), and programming language and music data (around 0.79)."}, {"id": 219, "string": "These Taylor exponents imply that a written text is more complex than programming source code or music with regard to fluctuation of its components."}, {"id": 220, "string": "None of the real data exhibited an exponent equal to 0.5."}, {"id": 221, "string": "We conducted more detailed analysis varying the data size and the segment size."}, {"id": 222, "string": "Taylor's law and its exponent can also be applied to evaluate machine-generated text."}, {"id": 223, "string": "We showed that a character-based LSTM language model generated text with a Taylor exponent of 0.5."}, {"id": 224, "string": "This indicates one limitation of that model."}, {"id": 225, "string": "Our future work will include an analysis using other kinds of data, such as Twitter data and adult utterances, and a study of how Taylor's law relates to grammatical complexity for different sequences."}, {"id": 226, "string": "Another direction will be to apply fluctuation analysis in formulating a statistical test to evaluate the structural complexity underlying a sequence."}], "headers": [{"section": "Introduction", "n": "1", "start": 0, "end": 21}, {"section": "Related Work", "n": "2", "start": 22, "end": 44}, {"section": "Proposed method", "n": "3.1", "start": 45, "end": 86}, {"section": "Taylor Exponents for Real Data", "n": "4", "start": 87, "end": 165}, {"section": "Evaluation of Machine-Generated Text by the Taylor Exponent", "n": "5", "start": 166, "end": 211}, {"section": "Conclusion", "n": "6", "start": 212, "end": 226}], "figures": [{"filename": "../figure/image/1294-Figure3-1.png", "caption": "Figure 3: Examples of Taylor\u2019s law for alternative data sets listed in Table 1: enwiki8 (tag-annotated Wikipedia), Thomas (longest in CHILDES), Lisp source code, and the music of Bach\u2019s St Matthew Passion. These examples exhibited larger Taylor exponents than did typical natural language texts.", "page": 5, "bbox": {"x1": 75.84, "x2": 286.08, "y1": 61.44, "y2": 268.32}}, {"filename": "../figure/image/1294-Figure4-1.png", "caption": "Figure 4: Box plots of the Taylor exponents for different kinds of data. Each point represents one sample, and samples from the same kind of data are contained in each box plot. The first two boxes are for the randomized data, while the remaining boxes are for real data, including both the natural language texts and language-related sequences. Each box ranges between the quantiles, with the middle line indicating the median, the whiskers showing the maximum and minimum, and some extreme values lying beyond.", "page": 6, "bbox": {"x1": 116.64, "x2": 481.44, "y1": 61.44, "y2": 264.0}}, {"filename": "../figure/image/1294-Figure5-1.png", "caption": "Figure 5: Histogram of Taylor exponents for long texts in Project Gutenberg (1129 texts). The legend indicates the languages, in frequency order. Each bar shows the number of texts with that value of \u03b1\u0302. Because of the skew of languages in the original conception of Project Gutenberg, the majority of the texts are in English, shown in blue, whereas texts in other languages are shown in other colors. The histogram shows how the Taylor exponent ranged fairly tightly around the mean, and natural language texts with an exponent larger than 0.63 were rare.", "page": 6, "bbox": {"x1": 76.8, "x2": 285.12, "y1": 359.52, "y2": 568.3199999999999}}, {"filename": "../figure/image/1294-Figure8-1.png", "caption": "Figure 8: Taylor analysis for two texts produced by standard neural language models: (a) a stacked LSTM model that learned the complete works of Shakespeare; and (b) a machine translation of Les Mis\u00e9rables (originally in French, translated into English), from a neural language model.", "page": 7, "bbox": {"x1": 310.56, "x2": 522.24, "y1": 61.44, "y2": 174.72}}, {"filename": "../figure/image/1294-Figure6-1.png", "caption": "Figure 6: Growth of \u03b1\u0302 with respect to \u2206t, averaged across data sets within each data kind. The plot labeled \u201crandom\u201d shows the average for the two datasets of randomized text from Moby Dick (shuffled and bigrams, as explained in \u00a75). Since this analysis required a large amount of computation, for the large data sets (such as newspaper and programming language data), 4 million words were taken from each kind of data and used here. When \u2206t was small, the Taylor exponent was close to 0.5, as theoretically described in the main text. As \u2206t was increased, the value of \u03b1\u0302 grew. The maximum \u2206t was about 10,000, or about one-tenth of the length of one long literary text. For the kinds of data investigated here, \u03b1\u0302 grew almost linearly. The results show that, at a given \u2206t, the Taylor exponent has some capability to distinguish different kinds of text data.", "page": 7, "bbox": {"x1": 72.0, "x2": 291.36, "y1": 62.879999999999995, "y2": 225.12}}, {"filename": "../figure/image/1294-Figure7-1.png", "caption": "Figure 7: Taylor analysis of a shuffled text of Moby Dick and a randomized text generated by a bigram model. Both exhibited an exponent of 0.50.", "page": 7, "bbox": {"x1": 75.84, "x2": 286.08, "y1": 490.56, "y2": 583.68}}, {"filename": "../figure/image/1294-Table1-1.png", "caption": "Table 1: Data we used in this article. For each dataset, length is the number of words, vocabulary is the number of different words. For detail of the data kind, see \u00a73.2.", "page": 3, "bbox": {"x1": 58.559999999999995, "x2": 537.12, "y1": 99.84, "y2": 513.12}}, {"filename": "../figure/image/1294-Figure1-1.png", "caption": "Figure 1: Examples of Taylor\u2019s law for natural language texts. Moby Dick and Hong Lou Meng are representative of single-author texts, and the two newspapers are representative of multipleauthor texts, in English and Chinese, respectively. Each point represents a kind of word. The values of \u03c3 and \u00b5 for each word kind are plotted across texts within segments of size \u2206t = 5620. The Taylor exponents obtained by the least-squares method were all around 0.58.", "page": 4, "bbox": {"x1": 75.84, "x2": 286.08, "y1": 61.44, "y2": 249.6}}, {"filename": "../figure/image/1294-Figure2-1.png", "caption": "Figure 2: Taylor exponent \u03b1\u0302 (vertical axis) calculated for the two largest texts: The New York Times and The Mainichi newspapers. To evaluate the exponent\u2019s dependence on the text size, parts of each text were taken and the exponents were calculated for those parts, with points taken logarithmically. The window size was \u2206t = 5620. As the text size grew, the Taylor exponent slightly decreased.", "page": 4, "bbox": {"x1": 306.71999999999997, "x2": 526.0799999999999, "y1": 61.44, "y2": 208.32}}]}