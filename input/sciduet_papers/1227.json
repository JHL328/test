{"title": "Judicious Selection of Training Data in Assisting Language for Multilingual Neural NER", "abstract": "Multilingual learning for Neural Named Entity Recognition (NNER) involves jointly training a neural network for multiple languages. Typically, the goal is improving the NER performance of one of the languages (the primary language) using the other assisting languages. We show that the divergence in the tag distributions of the common named entities between the primary and assisting languages can reduce the effectiveness of multilingual learning. To alleviate this problem, we propose a metric based on symmetric KL divergence to filter out the highly divergent training instances in the assisting language. We empirically show that our data selection strategy improves NER performance in many languages, including those with very limited training data.", "text": [{"id": 0, "string": "Existing approaches add all training sentences from the assisting language to the primary language and train the neural network on the combined data."}, {"id": 1, "string": "However, data from assisting languages can introduce a drift in the tag distribution for named entities, since the common named entities from the two languages may have vastly divergent tag distributions."}, {"id": 2, "string": "For example, the entity China appears in training split of Spanish (primary) and English (assisting) (Tjong Kim Sang, 2002; Tjong Kim Sang and De Meulder, 2003) with the corresponding tag frequencies, Spanish = { Loc : 20, Org : 49, Misc : 1 } and English = { Loc : 91, Org : 7 }."}, {"id": 3, "string": "By adding English data to Spanish, the tag distribution of China is skewed towards Location entity in Spanish."}, {"id": 4, "string": "This leads to a drop in named entity recognition performance."}, {"id": 5, "string": "In this work, we address this problem of drift in tag distribution owing to adding training data from a supporting language."}, {"id": 6, "string": "The problem is similar to the problem of data selection for domain adaptation of various NLP tasks, except that additional complexity is introduced due to the multilingual nature of the learning task."}, {"id": 7, "string": "For domain adaptation in various NLP tasks, several approaches have been proposed to address drift in data distribution (Moore and Lewis, 2010; Axelrod et al., 2011; Ruder and Plank, 2017) ."}, {"id": 8, "string": "For instance, in machine translation, sentences from out-of-domain data are selected based on a suitably defined metric (Moore and Lewis, 2010; Axelrod et al., 2011) ."}, {"id": 9, "string": "The metric attempts to capture similarity of the out-of-domain sentences with the in-domain data."}, {"id": 10, "string": "Out-of-domain sentences most similar to the in-domain data are added."}, {"id": 11, "string": "Like the domain adaptation techniques summarized above, we propose to judiciously add sentences from the assisting language to the primary language data based on the divergence between the tag distributions of named entities in the train-  Following are the contributions of the paper: (a) We present a simple approach to select assisting language sentences based on symmetric KL-Divergence of overlapping entities (b) We demonstrate the benefits of multilingual Neural NER on low-resource languages."}, {"id": 12, "string": "We compare the proposed data selection approach with monolingual Neural NER system, and the multilingual Neural NER system trained using all assisting language sentences."}, {"id": 13, "string": "To the best of our knowledge, ours is the first work for judiciously selecting a subset of sentences from an assisting language for multilingual Neural NER."}, {"id": 14, "string": "Judicious Selection of Assisting Language Sentences For every assisting language sentence, we calculate the sentence score based on the average symmetric KL-Divergence score of overlapping entities present in that sentence."}, {"id": 15, "string": "By overlapping entities, we mean entities whose surface form appears in both the languages' training data."}, {"id": 16, "string": "The symmetric KL-Divergence SKL(x), of a named entity x, is defined as follows, SKL(x) = KL( P p (x) || P a (x) ) + KL( P a (x) || P p (x) ) /2 (1) where P p (x) and P a (x) are the probability distributions for entity x in the primary (p) and the assisting (a) languages respectively."}, {"id": 17, "string": "KL refers to the standard KL-Divergence score between the two probability distributions."}, {"id": 18, "string": "KL-Divergence calculates the distance between the two probability distributions."}, {"id": 19, "string": "Lower the KL-Divergence score, higher is the tag agreement for an entity in both the languages thereby, reducing the possibility of entity drift in multilingual learning."}, {"id": 20, "string": "Assisting language sentences with the sentence score below a threshold value are added to the primary language data for multilingual learning."}, {"id": 21, "string": "If an assisting language sentence contains no overlapping entities, the corresponding sentence score is zero resulting in its selection."}, {"id": 22, "string": "Network Architecture Several deep learning models (Collobert et al., 2011; Ma and Hovy, 2016; Murthy and Bhattacharyya, 2016; Lample et al., 2016; Yang et al., 2017) have been proposed for monolingual NER in the literature."}, {"id": 23, "string": "Apart from the model by Collobert et al."}, {"id": 24, "string": "(2011) , remaining approaches extract sub-word features using either Convolution Neural Networks (CNNs) or Bi-LSTMs."}, {"id": 25, "string": "The proposed data selection strategy for multilingual Neural NER can be used with any of the existing models."}, {"id": 26, "string": "We choose the model by Murthy and Bhattacharyya (2016) 1 in our experiments."}, {"id": 27, "string": "Multilingual Learning We consider two parameter sharing configurations for multilingual learning (i) sub-word feature extractors shared across languages (Yang et al., 2017 ) (Sub-word) (ii) the entire network trained in a language independent way (All)."}, {"id": 28, "string": "As Murthy and Bhattacharyya (2016) use CNNs to extract sub-word features, only the character-level CNNs are shared for the Sub-word configuration."}, {"id": 29, "string": "Experimental Setup In this section we list the datasets used and the network configurations used in our experiments."}, {"id": 30, "string": "Datasets The Table 1 lists the datasets used in our experiments along with pre-trained word embeddings used and other dataset statistics."}, {"id": 31, "string": "For German NER, we use ep-96-04-16.conll to create train and development splits, and use ep-96-04-15.conll as test split."}, {"id": 32, "string": "As Italian has a different tag set compared to English, Spanish and Dutch, we do not share output layer for All configuration in multilingual experiments involving Italian."}, {"id": 33, "string": "Even though the languages considered are resource-rich languages, we consider German and Italian as primary languages due to their relatively lower number of train tokens."}, {"id": 34, "string": "The German NER data followed IO notation and for all experiments involving German, we converted other language data to IO notation."}, {"id": 35, "string": "Similarly, the Italian NER data followed IOBES notation and for all experiments involving Italian, we converted other language data to IOBES notation."}, {"id": 36, "string": "For low-resource language setup, we consider the following Indian languages: Hindi, Marathi 2 , Bengali, Tamil and Malayalam."}, {"id": 37, "string": "Except for Hindi all are low-resource languages."}, {"id": 38, "string": "We consider only Person, Location and Organization tags."}, {"id": 39, "string": "Though the scripts of these languages are different, they share the same set of phonemes making script mapping across languages easier."}, {"id": 40, "string": "We convert Tamil, Bengali and Malayalam data to the Devanagari script using the Indic NLP li-2 Data is available here: http://www.cfilt.iitb."}, {"id": 41, "string": "ac.in/ner/annotated_corpus/ brary 3 (Kunchukuttan et al., 2015) thereby, allowing sharing of sub-word features across the Indian languages."}, {"id": 42, "string": "For Indian languages, the annotated data followed the IOB format."}, {"id": 43, "string": "Network Hyper-parameters With the exception of English, Spanish and Dutch, remaining language datasets did not have official train and development splits provided."}, {"id": 44, "string": "We randomly select 70% of the train split for training the model and remaining as development split."}, {"id": 45, "string": "The threshold for sentence score SKL, is selected based on cross-validation for every language pair."}, {"id": 46, "string": "The dimensions of the Bi-LSTM hidden layer are 200 and 400 for the monolingual and multilingual experiments respectively."}, {"id": 47, "string": "We extract 20 features per convolution filter, with width varying from 1 to 9."}, {"id": 48, "string": "The initial learning rate is 0.4 and multiplied by 0.7 when validation error increases."}, {"id": 49, "string": "The training is stopped when the learning rate drops below 0.002."}, {"id": 50, "string": "We assign a weight of 0.1 to assisting language sentences and oversample primary language sentences to match the assisting language sentence count in all multilingual experiments."}, {"id": 51, "string": "For European languages, we have performed hyper-parameter tuning for both the monolingual and multilingual learning (with all assisting language sentences) configurations."}, {"id": 52, "string": "The best hyperparameter values for the language pair involved were observed to be within similar range."}, {"id": 53, "string": "Hence, we chose the same set of hyper-parameter values for all languages."}, {"id": 54, "string": "Results We now present the results on both resource-rich and resource-poor languages."}, {"id": 55, "string": "Table 2 presents the results for German and Italian NER."}, {"id": 56, "string": "We consistently observe improvements for German and Italian NER using our data selection strategy, irrespective of whether only subword features are shared (Sub-word) or the entire network (All) is shared across languages."}, {"id": 57, "string": "Resource-Rich Languages Adding all Spanish/Dutch sentences to Italian data leads to drop in Italian NER performance when all layers are shared."}, {"id": 58, "string": "Label drift from overlapping entities is one of the reasons for the poor results."}, {"id": 59, "string": "This can be observed by comparing the histograms of English and Spanish sentences ranked by the SKL scores for Italian multilingual learning (Figure 1) ."}, {"id": 60, "string": "Most English sentences have lower SKL scores indicating higher tag agreement for overlapping entities and lower drift in tag distribution."}, {"id": 61, "string": "Hence, adding all English sentences improves Italian NER accuracy."}, {"id": 62, "string": "In contrast, most Spanish sentences have larger SKL scores and adding these sentences adversely impacts Italian NER performance."}, {"id": 63, "string": "By judiciously selecting assisting language sentences, we eliminate sentences which are responsible for drift occurring during multilingual learning."}, {"id": 64, "string": "To understand how overlapping entities impact the NER performance, we study the statistics of overlapping named entities between Italian-English and Italian-Spanish pairs."}, {"id": 65, "string": "911 and 916 unique entities out of 4061 unique Italian entities appear in the English and Spanish data respectively."}, {"id": 66, "string": "We had hypothesized that entities with divergent tag distribution are responsible for hindering the performance in multilingual learning."}, {"id": 67, "string": "If we sort the common entities based on their SKL divergence value."}, {"id": 68, "string": "We observe that 484 out of 911 common entities in English and 535 out of 916 common entities in Spanish have an SKL score greater than 1.0."}, {"id": 69, "string": "162 out of 484 common entities in English-Italian data having SKL divergence value greater than 1.0 also appear more than 10 times in the English corpus."}, {"id": 70, "string": "Similarly, 123 out of 535 common entities in Spanish-Italian data having SKL divergence value greater than 1.0 also appear more than 10 times in the Spanish corpus."}, {"id": 71, "string": "However, these common 162 entities have a combined frequency of 12893 in English, meanwhile the 123 common entities have a combined frequency of 34945 in Spanish."}, {"id": 72, "string": "To summarize, although the number of overlapping entities is comparable in English and Spanish sentences, entities with larger SKL divergence score appears more frequently in Spanish sentences compared to English sentences."}, {"id": 73, "string": "As a consequence, adding all Spanish sentences leads to significant drop in Italian NER performance which is not the case when all English sentences are added."}, {"id": 74, "string": "Table 3 : Test set F-Score from monolingual and multilingual learning on Indian languages."}, {"id": 75, "string": "Result from monolingual training on the primary language is underlined."}, {"id": 76, "string": "\u2020 indicates SKL results statistically significant compared to adding all assisting language data with p-value < 0.05 using two-sided Welch t-test."}, {"id": 77, "string": "Resource-Poor Languages As Indian languages exhibit high lexical overlap (Kunchukuttan and Bhattacharyya, 2016) and syntactic relatedness (V Subb\u00e3r\u00e3o, 2012), we share all layers of the network across languages."}, {"id": 78, "string": "Influence of SKL Threshold Here, we study the influence of SKL score threshold on the NER performance."}, {"id": 79, "string": "We run experiments for Italian NER by adding Spanish training sentences and sharing all layers except for output layer across languages."}, {"id": 80, "string": "We vary the threshold value from 1.0 to 9.0 in steps of 1, and select sentences with score less than the threshold."}, {"id": 81, "string": "A threshold of 0.0 indicates monolingual training and threshold greater than 9.0 indicates all assist-ing language sentences considered."}, {"id": 82, "string": "The plot of Italian test F-Score against SKL score is shown in the Figure 2 ."}, {"id": 83, "string": "Italian test F-Score increases initially as we add more and more Spanish sentences and then drops due to influence of drift becoming significant."}, {"id": 84, "string": "Finding the right SKL threshold is important, hence we use a validation set to tune the SKL threshold."}, {"id": 85, "string": "Conclusion In this paper, we address the problem of divergence in tag distribution between primary and assisting languages for multilingual Neural NER."}, {"id": 86, "string": "We show that filtering out the assisting language sentences exhibiting significant divergence in the tag distribution can improve NER accuracy."}, {"id": 87, "string": "We propose to use the symmetric KL-Divergence metric to measure the tag distribution divergence."}, {"id": 88, "string": "We observe consistent improvements in multilingual Neural NER performance using our data selection strategy."}, {"id": 89, "string": "The strategy shows benefits for extremely low resource primary languages too."}, {"id": 90, "string": "This problem of drift in data distribution may not be unique to multilingual NER, and we plan to study the influence of data selection for multilingual learning on other NLP tasks like sentiment analysis, question answering, neural machine translation, etc."}, {"id": 91, "string": "We also plan to explore more metrics for multilingual learning, specifically for morphologically rich languages."}], "headers": [{"section": "Judicious Selection of Assisting Language Sentences", "n": "2", "start": 14, "end": 27}, {"section": "Experimental Setup", "n": "3", "start": 28, "end": 29}, {"section": "Datasets", "n": "3.1", "start": 30, "end": 42}, {"section": "Network Hyper-parameters", "n": "3.2", "start": 43, "end": 53}, {"section": "Results", "n": "4", "start": 54, "end": 56}, {"section": "Resource-Rich Languages", "n": "4.1", "start": 57, "end": 74}, {"section": "Resource-Poor Languages", "n": "4.2", "start": 75, "end": 77}, {"section": "Influence of SKL Threshold", "n": "4.3", "start": 78, "end": 84}, {"section": "Conclusion", "n": "5", "start": 85, "end": 91}], "figures": [{"filename": "../figure/image/1227-Table2-1.png", "caption": "Table 2: F-Score for German and Italian Test data using Monolingual and Multilingual learning strategies. \u2020 indicates that the SKL results are statistically significant compared to adding all assisting language data with p-value < 0.05 using two-sided Welch t-test.", "page": 2, "bbox": {"x1": 74.88, "x2": 523.1999999999999, "y1": 67.2, "y2": 186.23999999999998}}, {"filename": "../figure/image/1227-Table3-1.png", "caption": "Table 3: Test set F-Score from monolingual and multilingual learning on Indian languages. Result from monolingual training on the primary language is underlined. \u2020 indicates SKL results statistically significant compared to adding all assisting language data with p-value < 0.05 using two-sided Welch t-test.", "page": 4, "bbox": {"x1": 84.96, "x2": 512.16, "y1": 67.2, "y2": 171.35999999999999}}, {"filename": "../figure/image/1227-Table1-1.png", "caption": "Table 1: Dataset Statistics", "page": 1, "bbox": {"x1": 84.96, "x2": 513.12, "y1": 67.2, "y2": 210.23999999999998}}, {"filename": "../figure/image/1227-Figure1-1.png", "caption": "Figure 1: Histogram of assisting language sentences ranked by their sentence scores", "page": 3, "bbox": {"x1": 82.56, "x2": 288.0, "y1": 61.44, "y2": 366.71999999999997}}, {"filename": "../figure/image/1227-Figure2-1.png", "caption": "Figure 2: Spanish-Italian Multilingual Learning: Influence of Sentence score (SKL) on Italian NER", "page": 3, "bbox": {"x1": 306.71999999999997, "x2": 526.0799999999999, "y1": 579.84, "y2": 727.1999999999999}}]}