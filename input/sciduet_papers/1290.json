{"title": "The Best of Both Worlds: Combining Recent Advances in Neural Machine Translation", "abstract": "The past year has witnessed rapid advances in sequence-to-sequence (seq2seq) modeling for Machine Translation (MT). The classic RNN-based approaches to MT were first out-performed by the convolutional seq2seq model, which was then outperformed by the more recent Transformer model. Each of these new approaches consists of a fundamental architecture accompanied by a set of modeling and training techniques that are in principle applicable to other seq2seq architectures. In this paper, we tease apart the new architectures and their accompanying techniques in two ways. First, we identify several key modeling and training techniques, and apply them to the RNN architecture, yielding a new RNMT+ model that outperforms all of the three fundamental architectures on the benchmark WMT'14 English\u2192French and English\u2192German tasks. Second, we analyze the properties of each fundamental seq2seq architecture and devise new hybrid architectures intended to combine their strengths. Our hybrid models obtain further improvements, outperforming the RNMT+ model on both benchmark datasets.", "text": [{"id": 0, "string": "Introduction In recent years, the emergence of seq2seq models (Kalchbrenner and Blunsom, 2013; Sutskever et al., 2014; Cho et al., 2014) has revolutionized the field of MT by replacing traditional phrasebased approaches with neural machine translation (NMT) systems based on the encoder-decoder paradigm."}, {"id": 1, "string": "In the first architectures that surpassed * Equal contribution."}, {"id": 2, "string": "the quality of phrase-based MT, both the encoder and decoder were implemented as Recurrent Neural Networks (RNNs), interacting via a soft-attention mechanism (Bahdanau et al., 2015) ."}, {"id": 3, "string": "The RNN-based NMT approach, or RNMT, was quickly established as the de-facto standard for NMT, and gained rapid adoption into large-scale systems in industry, e.g."}, {"id": 4, "string": "Baidu (Zhou et al., 2016) , Google (Wu et al., 2016) , and Systran (Crego et al., 2016) ."}, {"id": 5, "string": "Following RNMT, convolutional neural network based approaches (LeCun and Bengio, 1998) to NMT have recently drawn research attention due to their ability to fully parallelize training to take advantage of modern fast computing devices."}, {"id": 6, "string": "such as GPUs and Tensor Processing Units (TPUs) (Jouppi et al., 2017) ."}, {"id": 7, "string": "Well known examples are ByteNet (Kalchbrenner et al., 2016) and ConvS2S (Gehring et al., 2017 )."}, {"id": 8, "string": "The ConvS2S model was shown to outperform the original RNMT architecture in terms of quality, while also providing greater training speed."}, {"id": 9, "string": "Most recently, the Transformer model (Vaswani et al., 2017) , which is based solely on a selfattention mechanism (Parikh et al., 2016) and feed-forward connections, has further advanced the field of NMT, both in terms of translation quality and speed of convergence."}, {"id": 10, "string": "In many instances, new architectures are accompanied by a novel set of techniques for performing training and inference that have been carefully optimized to work in concert."}, {"id": 11, "string": "This 'bag of tricks' can be crucial to the performance of a proposed architecture, yet it is typically under-documented and left for the enterprising researcher to discover in publicly released code (if any) or through anecdotal evidence."}, {"id": 12, "string": "This is not simply a problem for reproducibility; it obscures the central scientific question of how much of the observed gains come from the new architecture and how much can be attributed to the associated training and inference techniques."}, {"id": 13, "string": "In some cases, these new techniques may be broadly applicable to other architectures and thus constitute a major, though implicit, contribution of an architecture paper."}, {"id": 14, "string": "Clearly, they need to be considered in order to ensure a fair comparison across different model architectures."}, {"id": 15, "string": "In this paper, we therefore take a step back and look at which techniques and methods contribute significantly to the success of recent architectures, namely ConvS2S and Transformer, and explore applying these methods to other architectures, including RNMT models."}, {"id": 16, "string": "In doing so, we come up with an enhanced version of RNMT, referred to as RNMT+, that significantly outperforms all individual architectures in our setup."}, {"id": 17, "string": "We further introduce new architectures built with different components borrowed from RNMT+, ConvS2S and Transformer."}, {"id": 18, "string": "In order to ensure a fair setting for comparison, all architectures were implemented in the same framework, use the same pre-processed data and apply no further post-processing as this may confound bare model performance."}, {"id": 19, "string": "Our contributions are three-fold: We quickly note two prior works that provided empirical solutions to the difficulty of training NMT architectures (specifically RNMT)."}, {"id": 20, "string": "In (Britz et al., 2017) the authors systematically explore which elements of NMT architectures have a significant impact on translation quality."}, {"id": 21, "string": "In (Denkowski and Neubig, 2017) the authors recommend three specific techniques for strengthening NMT systems and empirically demonstrated how incorporating those techniques improves the reliability of the experimental results."}, {"id": 22, "string": "Background In this section, we briefly discuss the commmonly used NMT architectures."}, {"id": 23, "string": "RNN-based NMT Models -RNMT RNMT models are composed of an encoder RNN and a decoder RNN, coupled with an attention network."}, {"id": 24, "string": "The encoder summarizes the input sequence into a set of vectors while the decoder conditions on the encoded input sequence through an attention mechanism, and generates the output sequence one token at a time."}, {"id": 25, "string": "The most successful RNMT models consist of stacked RNN encoders with one or more bidirectional RNNs (Schuster and Paliwal, 1997; Graves and Schmidhuber, 2005) , and stacked decoders with unidirectional RNNs."}, {"id": 26, "string": "Both encoder and decoder RNNs consist of either LSTM (Hochreiter and Schmidhuber, 1997; Gers et al., 2000) or GRU units (Cho et al., 2014) , and make extensive use of residual (He et al., 2015) or highway (Srivastava et al., 2015) connections."}, {"id": 27, "string": "In Google-NMT (GNMT) (Wu et al., 2016) , the best performing RNMT model on the datasets we consider, the encoder network consists of one bi-directional LSTM layer, followed by 7 uni-directional LSTM layers."}, {"id": 28, "string": "The decoder is equipped with a single attention network and 8 uni-directional LSTM layers."}, {"id": 29, "string": "Both the encoder and the decoder use residual skip connections between consecutive layers."}, {"id": 30, "string": "In this paper, we adopt GNMT as the starting point for our proposed RNMT+ architecture."}, {"id": 31, "string": "Convolutional NMT Models -ConvS2S In the most successful convolutional sequence-tosequence model (Gehring et al., 2017) , both the encoder and decoder are constructed by stacking multiple convolutional layers, where each layer contains 1-dimensional convolutions followed by a gated linear units (GLU) (Dauphin et al., 2016) ."}, {"id": 32, "string": "Each decoder layer computes a separate dotproduct attention by using the current decoder layer output and the final encoder layer outputs."}, {"id": 33, "string": "Positional embeddings are used to provide explicit positional information to the model."}, {"id": 34, "string": "Following the practice in (Gehring et al., 2017) , we scale the gradients of the encoder layers to stabilize training."}, {"id": 35, "string": "We also use residual connections across each convolutional layer and apply weight normalization (Salimans and Kingma, 2016) to speed up convergence."}, {"id": 36, "string": "We follow the public ConvS2S codebase 1 in our experiments."}, {"id": 37, "string": "Conditional Transformation-based NMT Models -Transformer The Transformer model (Vaswani et al., 2017) is motivated by two major design choices that aim to address deficiencies in the former two model families: (1) Unlike RNMT, but similar to the ConvS2S, the Transformer model avoids any sequential dependencies in both the encoder and decoder networks to maximally parallelize training."}, {"id": 38, "string": "(2) To address the limited context problem (limited receptive field) present in ConvS2S, the Transformer model makes pervasive use of selfattention networks (Parikh et al., 2016) so that each position in the current layer has access to information from all other positions in the previous layer."}, {"id": 39, "string": "The Transformer model still follows the encoder-decoder paradigm."}, {"id": 40, "string": "Encoder transformer layers are built with two sub-modules: (1) a selfattention network and (2) a feed-forward network."}, {"id": 41, "string": "Decoder transformer layers have an additional cross-attention layer sandwiched between the selfattention and feed-forward layers to attend to the encoder outputs."}, {"id": 42, "string": "There are two details which we found very important to the model's performance: (1) Each sublayer in the transformer (i.e."}, {"id": 43, "string": "self-attention, crossattention, and the feed-forward sub-layer) follows a strict computation sequence: normalize \u2192 transform \u2192 dropout\u2192 residual-add."}, {"id": 44, "string": "(2) In addition to per-layer normalization, the final encoder output is again normalized to prevent a blow up after consecutive residual additions."}, {"id": 45, "string": "In this paper, we follow the latest version of the 1 https://github.com/facebookresearch/fairseq-py Transformer model in the Tensor2Tensor 2 codebase."}, {"id": 46, "string": "A Theory-Based Characterization of NMT Architectures From a theoretical point of view, RNNs belong to the most expressive members of the neural network family (Siegelmann and Sontag, 1995) 3 ."}, {"id": 47, "string": "Possessing an infinite Markovian structure (and thus an infinite receptive fields) equips them to model sequential data (Elman, 1990) , especially natural language (Grefenstette et al., 2015) effectively."}, {"id": 48, "string": "In practice, RNNs are notoriously hard to train (Hochreiter, 1991; Bengio et al., 1994; Hochreiter et al., 2001) , confirming the well known dilemma of trainability versus expressivity."}, {"id": 49, "string": "Convolutional layers are adept at capturing local context and local correlations by design."}, {"id": 50, "string": "A fixed and narrow receptive field for each convolutional layer limits their capacity when the architecture is shallow."}, {"id": 51, "string": "In practice, this weakness is mitigated by stacking more convolutional layers (e.g."}, {"id": 52, "string": "15 layers as in the ConvS2S model), which makes the model harder to train and demands meticulous initialization schemes and carefully designed regularization techniques."}, {"id": 53, "string": "The transformer network is capable of approximating arbitrary squashing functions (Hornik et al., 1989) , and can be considered a strong feature extractor with extended receptive fields capable of linking salient features from the entire sequence."}, {"id": 54, "string": "On the other hand, lacking a memory component (as present in the RNN models) prevents the network from modeling a state space, reducing its theoretical strength as a sequence model, thus it requires additional positional information (e.g."}, {"id": 55, "string": "sinusoidal positional encodings)."}, {"id": 56, "string": "Above theoretical characterizations will drive our explorations in the following sections."}, {"id": 57, "string": "Experiment Setup We train our models on the standard WMT'14 En\u2192Fr and En\u2192De datasets that comprise 36.3M and 4.5M sentence pairs, respectively."}, {"id": 58, "string": "Each sentence was encoded into a sequence of sub-word units obtained by first tokenizing the sentence with the Moses tokenizer, then splitting tokens into subword units (also known as \"wordpieces\") using the approach described in (Schuster and Nakajima, 2012) ."}, {"id": 59, "string": "At the end of each bidirectional layer, the outputs of the forward layer and the backward layer are concatenated."}, {"id": 60, "string": "On the right side, the decoder network has 8 unidirectional LSTM layers, with the first layer used for obtaining the attention context vector through multi-head additive attention."}, {"id": 61, "string": "The attention context vector is then fed directly into the rest of the decoder layers as well as the softmax layer."}, {"id": 62, "string": "We use a shared vocabulary of 32K sub-word units for each source-target language pair."}, {"id": 63, "string": "No further manual or rule-based post processing of the output was performed beyond combining the subword units to generate the targets."}, {"id": 64, "string": "We report all our results on newstest 2014, which serves as the test set."}, {"id": 65, "string": "A combination of newstest 2012 and newstest 2013 is used for validation."}, {"id": 66, "string": "To evaluate the models, we compute the BLEU metric on tokenized, true-case output."}, {"id": 67, "string": "4 For each training run, we evaluate the model every 30 minutes on the dev set."}, {"id": 68, "string": "Once the model converges, we determine the best window based on the average dev-set BLEU score over 21 consecutive evaluations."}, {"id": 69, "string": "We report the mean test score and standard deviation over the selected window."}, {"id": 70, "string": "This allows us to compare model architectures based on their mean performance after convergence rather than individual checkpoint evaluations, as the latter can be quite noisy for some models."}, {"id": 71, "string": "To enable a fair comparison of architectures, we use the same pre-processing and evaluation methodology for all our experiments."}, {"id": 72, "string": "We refrain from using checkpoint averaging (exponential moving averages of parameters) (Junczys-Dowmunt et al., 2016) or checkpoint ensembles (Jean et al., 2015; Chen et al., 2017) to focus on evaluating the performance of individual models."}, {"id": 73, "string": "RNMT+ Model Architecture of RNMT+ The newly proposed RNMT+ model architecture is shown in Figure 1 ."}, {"id": 74, "string": "Here we highlight the key architectural choices that are different between the RNMT+ model and the GNMT model."}, {"id": 75, "string": "There are 6 bidirectional LSTM layers in the encoder instead of 1 bidirectional LSTM layer followed by 7 unidirectional layers as in GNMT."}, {"id": 76, "string": "For each bidirectional layer, the outputs of the forward layer and the backward layer are concatenated before being fed into the next layer."}, {"id": 77, "string": "The decoder network consists of 8 unidirectional LSTM layers similar to the GNMT model."}, {"id": 78, "string": "Residual connections are added to the third layer and above for both the encoder and decoder."}, {"id": 79, "string": "Inspired by the Transformer model, pergate layer normalization (Ba et al., 2016) is applied within each LSTM cell."}, {"id": 80, "string": "Our empirical results show that layer normalization greatly stabilizes training."}, {"id": 81, "string": "No non-linearity is applied to the LSTM output."}, {"id": 82, "string": "A projection layer is added to the encoder final output."}, {"id": 83, "string": "5 Multi-head additive attention is used instead of the single-head attention in the GNMT model."}, {"id": 84, "string": "Similar to GNMT, we use the bottom decoder layer and the final encoder layer output after projection for obtaining the recurrent attention context."}, {"id": 85, "string": "In addition to feeding the attention context to all decoder LSTM layers, we also feed it to the softmax by concatenating it with the layer input."}, {"id": 86, "string": "This is important for both the quality of the models with multi-head attention and the stability of the training process."}, {"id": 87, "string": "Since the encoder network in RNMT+ consists solely of bi-directional LSTM layers, model parallelism is not used during training."}, {"id": 88, "string": "We compensate for the resulting longer per-step time with increased data parallelism (more model replicas), so that the overall time to reach convergence of the RNMT+ model is still comparable to that of GNMT."}, {"id": 89, "string": "We apply the following regularization techniques during training."}, {"id": 90, "string": "\u2022 Dropout: We apply dropout to both embedding layers and each LSTM layer output before it is added to the next layer's input."}, {"id": 91, "string": "Attention dropout is also applied."}, {"id": 92, "string": "\u2022 Label Smoothing: We use uniform label smoothing with an uncertainty=0.1 (Szegedy et al., 2015) ."}, {"id": 93, "string": "Label smoothing was shown to have a positive impact on both Transformer and RNMT+ models, especially in the case of RNMT+ with multi-head attention."}, {"id": 94, "string": "Similar to the observations in (Chorowski and Jaitly, 2016) , we found it beneficial to use a larger beam size (e.g."}, {"id": 95, "string": "16, 20, etc.)"}, {"id": 96, "string": "during decoding when models are trained with label smoothing."}, {"id": 97, "string": "\u2022 Weight Decay: For the WMT'14 En\u2192De task, we apply L2 regularization to the weights with \u03bb = 10 \u22125 ."}, {"id": 98, "string": "Weight decay is only applied to the En\u2192De task as the corpus is smaller and thus more regularization is required."}, {"id": 99, "string": "We use the Adam optimizer (Kingma and Ba, 2014) with \u03b2 1 = 0.9, \u03b2 2 = 0.999, = 10 \u22126 and vary the learning rate according to this schedule: lr = 10 \u22124 \u00b7 min 1 + t \u00b7 (n \u2212 1) np , n, n \u00b7 (2n) s\u2212nt e\u2212s (1) Here, t is the current step, n is the number of concurrent model replicas used in training, p is the number of warmup steps, s is the start step of the exponential decay, and e is the end step of the decay."}, {"id": 100, "string": "Specifically, we first increase the learning rate linearly during the number of warmup steps, keep it a constant until the decay start step s, then exponentially decay until the decay end step e, and keep it at 5 \u00b7 10 \u22125 after the decay ends."}, {"id": 101, "string": "This learning rate schedule is motivated by a similar schedule that was successfully applied in training the Resnet-50 model with a very large batch size (Goyal et al., 2017) ."}, {"id": 102, "string": "In contrast to the asynchronous training used for GNMT (Dean et al., 2012) , we train RNMT+ models with synchronous training ."}, {"id": 103, "string": "Our empirical results suggest that when hyper-parameters are tuned properly, synchronous training often leads to improved convergence speed and superior model quality."}, {"id": 104, "string": "To further stabilize training, we also use adaptive gradient clipping."}, {"id": 105, "string": "We discard a training step completely if an anomaly in the gradient norm value is detected, which is usually an indication of an imminent gradient explosion."}, {"id": 106, "string": "More specifically, we keep track of a moving average and a moving standard deviation of the log of the gradient norm values, and we abort a step if the norm of the gradient exceeds four standard deviations of the moving average."}, {"id": 107, "string": "Model Analysis and Comparison In this section, we compare the results of RNMT+ with ConvS2S and Transformer."}, {"id": 108, "string": "All models were trained with synchronous training."}, {"id": 109, "string": "RNMT+ and ConvS2S were trained with 32 NVIDIA P100 GPUs while the Transformer Base and Big models were trained using 16 GPUs."}, {"id": 110, "string": "For RNMT+, we use sentence-level crossentropy loss."}, {"id": 111, "string": "Each training batch contained 4096 sentence pairs (4096 source sequences and 4096 target sequences)."}, {"id": 112, "string": "For ConvS2S and Transformer models, we use token-level cross-entropy loss."}, {"id": 113, "string": "Each training batch contained 65536 source tokens and 65536 target tokens."}, {"id": 114, "string": "For the GNMT baselines on both tasks, we cite the largest BLEU score reported in (Wu et al., 2016) Table 2 shows our results on the WMT'14 En\u2192De task."}, {"id": 115, "string": "The Transformer Base model improves over GNMT and ConvS2S by more than 2 BLEU points while the Big model improves by over 3 BLEU points."}, {"id": 116, "string": "RNMT+ further outperforms the Transformer Big model and establishes a new state of the art with an averaged value of 28.49."}, {"id": 117, "string": "In this case, RNMT+ converged slightly faster than the Transformer Big model and maintained much more stable performance after convergence with a very small standard deviation, which is similar to what we observed on the En-Fr task."}, {"id": 118, "string": "Table 3 summarizes training performance and model statistics."}, {"id": 119, "string": "The Transformer Base model 6 Since the ConvS2S model convergence is very slow we did not explore further tuning on En\u2192Fr, and validated our implementation on En\u2192De."}, {"id": 120, "string": "7 The BLEU scores for Transformer model are slightly lower than those reported in (Vaswani et al., 2017) due to four differences: 1) We report the mean test BLEU score using the strategy described in section 3."}, {"id": 121, "string": "2) We did not perform checkpoint averaging since it would be inconsistent with our evaluation for other models."}, {"id": 122, "string": "3) We avoided any manual post-processing, like unicode normalization using Moses replace-unicode-punctuation.perl or output tokenization using Moses tokenizer.perl, to rule out its effect on the evaluation."}, {"id": 123, "string": "We observed a significant BLEU increase (about 0.6) on applying these post processing techniques."}, {"id": 124, "string": "4) In (Vaswani et al., 2017) , reported BLEU scores are calculated using mteval-v13a.pl from Moses, which re-tokenizes its input."}, {"id": 125, "string": "Model Test Ablation Experiments In this section, we evaluate the importance of four main techniques for both the RNMT+ and the Transformer Big models."}, {"id": 126, "string": "We believe that these techniques are universally applicable across different model architectures, and should always be employed by NMT practitioners for best performance."}, {"id": 127, "string": "We take our best RNMT+ and Transformer Big models and remove each one of these techniques independently."}, {"id": 128, "string": "By doing this we hope to learn two things about each technique: (1) How much does it affect the model performance?"}, {"id": 129, "string": "(2) From Table 4 we draw the following conclusions about the four techniques: \u2022 Label Smoothing We observed that label smoothing improves both models, leading to an average increase of 0.7 BLEU for RNMT+ and 0.2 BLEU for Transformer Big models."}, {"id": 130, "string": "\u2022 Multi-head Attention Multi-head attention contributes significantly to the quality of both models, resulting in an average increase of 0.6 BLEU for RNMT+ and 0.9 BLEU for Transformer Big models."}, {"id": 131, "string": "\u2022 Layer Normalization Layer normalization is most critical to stabilize the training process of either model, especially when multi-head attention is used."}, {"id": 132, "string": "Removing layer normalization results in unstable training runs for both models."}, {"id": 133, "string": "Since by design, we remove one technique at a time in our ablation experiments, we were unable to quantify how much layer normalization helped in either case."}, {"id": 134, "string": "To be able to successfully train a model without layer normalization, we would have to adjust other parts of the model and retune its hyper-parameters."}, {"id": 135, "string": "Hybrid NMT Models In this section, we explore hybrid architectures that shed some light on the salient behavior of each model family."}, {"id": 136, "string": "These hybrid models outperform the individual architectures on both benchmark datasets and provide a better understanding of the capabilities and limitations of each model family."}, {"id": 137, "string": "Assessing Individual Encoders and Decoders In an encoder-decoder architecture, a natural assumption is that the role of an encoder is to build feature representations that can best encode the meaning of the source sequence, while a decoder should be able to process and interpret the representations from the encoder and, at the same time, track the current target history."}, {"id": 138, "string": "Decoding is inherently auto-regressive, and keeping track of the state information should therefore be intuitively beneficial for conditional generation."}, {"id": 139, "string": "We set out to study which family of encoders is more suitable to extract rich representations from a given input sequence, and which family of decoders can make the best of such rich representations."}, {"id": 140, "string": "We start by combining the encoder and decoder from different model families."}, {"id": 141, "string": "Since it takes a significant amount of time for a ConvS2S model to converge, and because the final translation quality was not on par with the other models, we focus on two types of hybrids only: Transformer encoder with RNMT+ decoder and RNMT+ encoder with Transformer decoder."}, {"id": 142, "string": "From Table 5 , it is clear that the Transformer encoder is better at encoding or feature extraction than the RNMT+ encoder, whereas RNMT+ is better at decoding or conditional language modeling, confirming our intuition that a stateful de-coder is beneficial for conditional language generation."}, {"id": 143, "string": "Assessing Encoder Combinations Next, we explore how the features extracted by an encoder can be further enhanced by incorporating additional information."}, {"id": 144, "string": "Specifically, we investigate the combination of transformer layers with RNMT+ layers in the same encoder block to build even richer feature representations."}, {"id": 145, "string": "We exclusively use RNMT+ decoders in the following architectures since stateful decoders show better performance according to Table 5 ."}, {"id": 146, "string": "We study two mixing schemes in the encoder (see Fig."}, {"id": 147, "string": "2 ): (1) Cascaded Encoder: The cascaded encoder aims at combining the representational power of RNNs and self-attention."}, {"id": 148, "string": "The idea is to enrich a set of stateful representations by cascading a feature extractor with a focus on vertical mapping, similar to (Pascanu et al., 2013; Devlin, 2017) ."}, {"id": 149, "string": "Our best performing cascaded encoder involves fine tuning transformer layers stacked on top of a pre-trained frozen RNMT+ encoder."}, {"id": 150, "string": "Using a pre-trained encoder avoids optimization difficulties while significantly enhancing encoder capacity."}, {"id": 151, "string": "As shown in Table 6 , the cascaded encoder improves over the Transformer encoder by more than 0.5 BLEU points on the WMT'14 En\u2192Fr task."}, {"id": 152, "string": "This suggests that the Transformer encoder is able to extract richer representations if the input is augmented with sequential context."}, {"id": 153, "string": "(2) Multi-Column Encoder: As illustrated in Fig."}, {"id": 154, "string": "2b , a multi-column encoder merges the outputs of several independent encoders into a single combined representation."}, {"id": 155, "string": "Unlike a cascaded encoder, the multi-column encoder enables us to investigate whether an RNMT+ decoder can distinguish information received from two different channels and benefit from its combination."}, {"id": 156, "string": "A crucial operation in a multi-column encoder is therefore how different sources of information are merged into a unified representation."}, {"id": 157, "string": "Our best multi-column encoder performs a simple concatenation of individual column outputs."}, {"id": 158, "string": "The model details and hyperparameters of the above two encoders are described in Appendix A.5 and A.6."}, {"id": 159, "string": "As shown in Table 6 , the multi-column encoder followed by an RNMT+ decoder achieves better results than the Transformer and the RNMT model on both WMT'14 benchmark tasks."}, {"id": 160, "string": "28.84 \u00b1 0.06 Table 6 : Results for hybrids with cascaded encoder and multi-column encoder."}, {"id": 161, "string": "Conclusion In this work we explored the efficacy of several architectural and training techniques proposed in recent studies on seq2seq models for NMT."}, {"id": 162, "string": "We demonstrated that many of these techniques are broadly applicable to multiple model architectures."}, {"id": 163, "string": "Applying these new techniques to RNMT models yields RNMT+, an enhanced RNMT model that significantly outperforms the three fundamental architectures on WMT'14 En\u2192Fr and En\u2192De tasks."}, {"id": 164, "string": "We further presented several hybrid models developed by combining encoders and decoders from the Transformer and RNMT+ models, and empirically demonstrated the superiority of the Transformer encoder and the RNMT+ decoder in comparison with their counterparts."}, {"id": 165, "string": "We then enhanced the encoder architecture by horizontally and vertically mixing components borrowed from these architectures, leading to hybrid architectures that obtain further improvements over RNMT+."}, {"id": 166, "string": "We hope that our work will motivate NMT researchers to further investigate generally applicable training and optimization techniques, and that our exploration of hybrid architectures will open paths for new architecture search efforts for NMT."}, {"id": 167, "string": "Our focus on a standard single-language-pair translation task leaves important open questions to be answered: How do our new architectures compare in multilingual settings, i.e., modeling an interlingua?"}, {"id": 168, "string": "Which architecture is more efficient and powerful in processing finer grained inputs and outputs, e.g., characters or bytes?"}, {"id": 169, "string": "How transferable are the representations learned by the different architectures to other tasks?"}, {"id": 170, "string": "And what are the characteristic errors that each architecture makes, e.g., linguistic plausibility?"}], "headers": [{"section": "Introduction", "n": "1", "start": 0, "end": 19}, {"section": "Background", "n": "2", "start": 20, "end": 22}, {"section": "RNN-based NMT Models -RNMT", "n": "2.1", "start": 23, "end": 30}, {"section": "Convolutional NMT Models -ConvS2S", "n": "2.2", "start": 31, "end": 36}, {"section": "Conditional Transformation-based NMT Models -Transformer", "n": "2.3", "start": 37, "end": 45}, {"section": "A Theory-Based Characterization of NMT Architectures", "n": "2.4", "start": 46, "end": 56}, {"section": "Experiment Setup", "n": "3", "start": 57, "end": 72}, {"section": "Model Architecture of RNMT+", "n": "4.1", "start": 73, "end": 106}, {"section": "Model Analysis and Comparison", "n": "4.2", "start": 107, "end": 124}, {"section": "Ablation Experiments", "n": "5", "start": 125, "end": 133}, {"section": "Hybrid NMT Models", "n": "6", "start": 134, "end": 136}, {"section": "Assessing Individual Encoders and Decoders", "n": "6.1", "start": 137, "end": 142}, {"section": "Assessing Encoder Combinations", "n": "6.2", "start": 143, "end": 160}, {"section": "Conclusion", "n": "7", "start": 161, "end": 170}], "figures": [{"filename": "../figure/image/1290-Table1-1.png", "caption": "Table 1: Results on WMT14 En\u2192Fr. The numbers before and after \u2018\u00b1\u2019 are the mean and standard deviation of test BLEU score over an evaluation window. Note that Transformer models are trained using 16 GPUs, while ConvS2S and RNMT+ are trained using 32 GPUs.", "page": 5, "bbox": {"x1": 72.0, "x2": 292.32, "y1": 170.88, "y2": 270.24}}, {"filename": "../figure/image/1290-Table2-1.png", "caption": "Table 2: Results on WMT14 En\u2192De. Note that Transformer models are trained using 16 GPUs, while ConvS2S and RNMT+ are trained using 32 GPUs.", "page": 5, "bbox": {"x1": 306.71999999999997, "x2": 526.0799999999999, "y1": 62.879999999999995, "y2": 162.23999999999998}}, {"filename": "../figure/image/1290-Table3-1.png", "caption": "Table 3: Performance comparison. Examples/s are normalized by the number of GPUs used in the training job. FLOPs are computed assuming that source and target sequence length are both 50.", "page": 5, "bbox": {"x1": 308.64, "x2": 524.16, "y1": 446.88, "y2": 519.36}}, {"filename": "../figure/image/1290-Table6-1.png", "caption": "Table 6: Results for hybrids with cascaded encoder and multi-column encoder.", "page": 7, "bbox": {"x1": 308.64, "x2": 524.16, "y1": 62.879999999999995, "y2": 135.35999999999999}}, {"filename": "../figure/image/1290-Figure2-1.png", "caption": "Figure 2: Vertical and horizontal mixing of Transformer and RNMT+ components in an encoder.", "page": 7, "bbox": {"x1": 324.47999999999996, "x2": 526.0799999999999, "y1": 183.84, "y2": 348.47999999999996}}, {"filename": "../figure/image/1290-Figure1-1.png", "caption": "Figure 1: Model architecture of RNMT+. On the left side, the encoder network has 6 bidirectional LSTM layers. At the end of each bidirectional layer, the outputs of the forward layer and the backward layer are concatenated. On the right side, the decoder network has 8 unidirectional LSTM layers, with the first layer used for obtaining the attention context vector through multi-head additive attention. The attention context vector is then fed directly into the rest of the decoder layers as well as the softmax layer.", "page": 3, "bbox": {"x1": 116.64, "x2": 481.44, "y1": 61.44, "y2": 262.08}}, {"filename": "../figure/image/1290-Table4-1.png", "caption": "Table 4: Ablation results of RNMT+ and the Transformer Big model on WMT\u201914 En\u2192 Fr. We report average BLEU scores on the test set. An asterisk \u2019*\u2019 indicates an unstable training run (training halts due to non-finite elements).", "page": 6, "bbox": {"x1": 72.0, "x2": 292.32, "y1": 112.8, "y2": 199.2}}, {"filename": "../figure/image/1290-Table5-1.png", "caption": "Table 5: Results for encoder-decoder hybrids.", "page": 6, "bbox": {"x1": 308.64, "x2": 524.16, "y1": 585.6, "y2": 658.0799999999999}}]}