{"title": "Zero-Shot Transfer Learning for Event Extraction", "abstract": "Most previous supervised event extraction methods have relied on features derived from manual annotations, and thus cannot be applied to new event types without extra annotation effort. We take a fresh look at event extraction and model it as a generic grounding problem: mapping each event mention to a specific type in a target event ontology. We design a transferable architecture of structural and compositional neural networks to jointly represent and map event mentions and types into a shared semantic space. Based on this new framework, we can select, for each event mention, the event type which is semantically closest in this space as its type. By leveraging manual annotations available for a small set of existing event types, our framework can be applied to new unseen event types without additional manual annotations. When tested on 23 unseen event types, this zeroshot framework, without manual annotations, achieves performance comparable to a supervised model trained from 3,000 sentences annotated with 500 event mentions. 1", "text": [{"id": 0, "string": "Most previous supervised event extraction methods have relied on features derived from manual annotations, and thus cannot be applied to new event types without extra annotation effort."}, {"id": 1, "string": "We take a fresh look at event extraction and model it as a generic grounding problem: mapping each event mention to a specific type in a target event ontology."}, {"id": 2, "string": "We design a transferable architecture of structural and compositional neural networks to jointly represent and map event mentions and types into a shared semantic space."}, {"id": 3, "string": "Based on this new framework, we can select, for each event mention, the event type which is semantically closest in this space as its type."}, {"id": 4, "string": "By leveraging manual annotations available for a small set of existing event types, our framework can be applied to new unseen event types without additional manual annotations."}, {"id": 5, "string": "When tested on 23 unseen event types, this zeroshot framework, without manual annotations, achieves performance comparable to a supervised model trained from 3,000 sentences annotated with 500 event mentions."}, {"id": 6, "string": "1 Introduction The goal of event extraction is to identify event triggers and their arguments in unstructured text data, and then to assign an event type to each trigger and a semantic role to each argument."}, {"id": 7, "string": "An example is shown in Figure 1 ."}, {"id": 8, "string": "Traditional supervised methods have typically modeled this task of event 1 The programs are publicly available for research purpose at: https://github.com/wilburOne/ZeroShotEvent extraction as a classification problem, by assigning event triggers to event types from a pre-defined fixed set."}, {"id": 9, "string": "These methods rely heavily on manual annotations and features specific to each event type, and thus are not easily adapted to new event types without extra annotation effort."}, {"id": 10, "string": "Handling new event types may even entail starting over, without being able to re-use annotations from previous event types."}, {"id": 11, "string": "To make event extraction effective as new realworld scenarios emerge, we take a look at this task from the perspective of zero-shot learning, ZSL Norouzi et al., 2013; Socher et al., 2013a) ."}, {"id": 12, "string": "ZSL, as a type of transfer learning, makes use of separate, pre-existing classifiers to build a semantic, cross-concept space that maps between their respective classes."}, {"id": 13, "string": "The resulting shared semantic space then allows for building a novel \"zero-shot\" classifier, i,e,, requiring no (zero) additional training examples, to handle unseen cases."}, {"id": 14, "string": "We observe that each event mention has a structure consisting of a candidate trigger and arguments, with corresponding predefined name labels for the event type and argument roles."}, {"id": 15, "string": "We propose to enrich the semantic representations of each event mention and event type with rich structures, and determine the type based on the semantic similarity between an event mention and each event type defined in a target ontology."}, {"id": 16, "string": "Let's consider two example sentences: E1."}, {"id": 17, "string": "The Government of China has ruled Tibet since 1951 after dispatching troops to the Himalayan region in 1950."}, {"id": 18, "string": "E2."}, {"id": 19, "string": "Iranian state television stated that the conflict between the Iranian police and the drug smugglers took place near the town of mirjaveh."}, {"id": 20, "string": "In E1, as also diagrammed in Figure 1 , dis- patching is the trigger for the event mention of type Transport Person and in E2, conflict is the trigger for the event mention of type Attack."}, {"id": 21, "string": "We make use of Abstract Meaning Representations (AMR) (Banarescu et al., 2013) to identify the candidate arguments and construct event mention structures as shown in Figure 2 (top)."}, {"id": 22, "string": "Figure 2 (bottom) also shows event type structures defined in the Automatic Content Extraction (ACE) guideline."}, {"id": 23, "string": "2 We can see that a trigger and its event type name usually have some shared meaning."}, {"id": 24, "string": "Furthermore, their structures also tend to be similar: a Transport Person event typically involves a Person as its patient role, while an Attack event involves a Person or Location as an Attacker."}, {"id": 25, "string": "This observation matches the theory by Pustejovsky (1991) : \"the semantics of an event structure can be generalized and mapped to event mention structures in a systematic and predictable way\"."}, {"id": 26, "string": "Inspired by this theory, for the first time, we model event extraction as a generic grounding problem, by mapping each mention to its semantically closest event type."}, {"id": 27, "string": "Given an event ontology, 2 https://en.wikipedia.org/wiki/Automatic content extraction where each event type structure is well-defined, we will refer to the event types for which we have annotated event mentions as seen types, while those without annotations as unseen types."}, {"id": 28, "string": "Our goal is to learn a generic mapping function independent of event types, which can be trained from annotations for a limited number of seen event types and then used for any new unseen event types."}, {"id": 29, "string": "We design a transferable neural architecture, which jointly learns and maps the structural representations of event mentions and types into a shared semantic space, by minimizing the distance between each event mention and its corresponding type."}, {"id": 30, "string": "For event mentions with unseen types, their structures will be projected into the same semantic space using the same framework and assigned types with top-ranked similarity values."}, {"id": 31, "string": "To summarize, to apply our new zero-shot transfer learning framework to any new unseen event types, we only need (1) a structured definition of the unseen event type (its type name along with role names for its arguments, from the event ontology); and (2) some annotations for one or a few seen event types."}, {"id": 32, "string": "Without requiring any additional manual annotations for the new unseen types, our ZSL framework achieves performance comparable to supervised methods trained from a substantial amount of training data for the same types."}, {"id": 33, "string": "Approach Overview Briefly here, we overview the phases involved in building our framework's shared semantic space that, in turn, is the basis for the ZSL framework."}, {"id": 34, "string": "Given a sentence s, we start by identifying candidate triggers and arguments based on AMR parsing (Wang et al., 2015b) ."}, {"id": 35, "string": "For the example shown in Figure 1 , we identify dispatching as a trigger, and its candidate arguments: China, troops, Himalayan and 1950."}, {"id": 36, "string": "The details will be described in Section 3."}, {"id": 37, "string": "After this identification phase, we use our new neural architecture, as depicted in Figure 3 , to classify triggers into event types."}, {"id": 38, "string": "(The classification of arguments into roles follows the same pipeline.)"}, {"id": 39, "string": "For each trigger t, e.g., dispatch-01, we determine its type by comparing its semantic representation with that of any event type in the event ontology."}, {"id": 40, "string": "In order to incorporate the contexts into the semantic representation of t, we build a structure S t using AMR as shown in Figure 3."}, {"id": 41, "string": "Each structure is composed of a set of tuples, e.g, dispatch-01, :ARG0, China ."}, {"id": 42, "string": "We use a matrix to represent each AMR relation, composing its semantics with two concepts for each tuple (in Section 4), and feed all tuple representations into a CNN to generate a dense vector representation V St for the event mention structure (in Section 5.1)."}, {"id": 43, "string": "Given a target event ontology, for each type y, e.g., Transport Person, we construct a type structure S y consisting of its predefined roles, and use a tensor to denote the implicit relation between any type and argument role."}, {"id": 44, "string": "We compose the semantics of type and argument role with the tensor for each tuple, e.g., Transport Person, Destination (in Section 4)."}, {"id": 45, "string": "Then we generate the event type structure representation V Sy using the same CNN (in Section 5.1)."}, {"id": 46, "string": "By minimizing the semantic distance between dispatch-01 and Trans-port Person using their dense vectors, V St and V Sy respectively, we jointly map the representations of event mention and event types into a shared semantic space, where each mention is closest to its annotated type."}, {"id": 47, "string": "After training that completes the construction of the semantic space, the compositional functions and CNNs are then used to project any new event mention (e.g., donate-01) into the semantic space and find its closest event type (e.g., Donation) (in Section 5.3)."}, {"id": 48, "string": "In the next sections we will elaborate each step in great detail."}, {"id": 49, "string": "Trigger and Argument Identification Similar to , we identify candidate triggers and arguments based on AMR Parsing (Wang et al., 2015b ) and apply the same word sense disambiguation (WSD) tool (Zhong and Ng, 2010) to disambiguate word senses and link each sense to OntoNotes, as shown in Figure 1 ."}, {"id": 50, "string": "Given a sentence, we consider all noun and verb concepts that can be mapped to OntoNotes senses by WSD as candidate event triggers."}, {"id": 51, "string": "In addition, the concepts that can be matched with verbs or nominal lexical units in FrameNet (Baker et al., 1998) are also considered as candidate triggers."}, {"id": 52, "string": "For each candidate trigger, we consider any concepts that are involved in a subset of AMR rela-tions as candidate arguments 3 ."}, {"id": 53, "string": "We manually select this subset of AMR relations that are useful for capturing generic relations between event triggers and arguments, as shown in Table 1 ."}, {"id": 54, "string": "Categories Relations Core roles ARG0, ARG1, ARG2, ARG3, ARG4 Non-core roles mod, location, instrument, poss, manner, topic, medium, prep-X Temporal year, duration, decade, weekday, time Spatial destination, path, location Trigger and Type Structure Composition As Figure 3 shows, for each candidate trigger t, we construct its event mention structure S t based on its candidate arguments and AMR parsing."}, {"id": 55, "string": "For each type y in the target event ontology, we construct a structure S y by including its pre-defined roles and using its type as the root."}, {"id": 56, "string": "Each S t or S y is composed of a collection of tuples."}, {"id": 57, "string": "For each event mention structure, a tuple consists of two AMR concepts and an AMR relation."}, {"id": 58, "string": "For each event type structure, a tuple consists of a type name and an argument role name."}, {"id": 59, "string": "Next we will describe how to compose semantic representations for event mention and event type respectively based on these structures."}, {"id": 60, "string": "Event Mention Structure For each tuple u = w 1 , \u03bb, w 2 in an event mention structure, we use a matrix to represent each AMR relation \u03bb, and compose the semantics of \u03bb between two concepts w 1 and w 2 as: V u = [V w 1 ; V w 2 ] = f ([V w 1 ; V w 2 ] \u00b7 M \u03bb ) where V w 1 , V w 2 \u2208 R d are the vector representations of words w 1 and w 2 ."}, {"id": 61, "string": "d is the dimension size of each word vector."}, {"id": 62, "string": "[ ; ] denotes the concatenation of two vectors."}, {"id": 63, "string": "M \u03bb \u2208 R 2d\u00d72d is the matrix representation for AMR relation \u03bb. V u is the composition representation of tuple u, which consists of two updated vector representations V w 1 , V w 2 for w 1 and w 2 by incorporating the semantics of \u03bb."}, {"id": 64, "string": "Event Type Structure For each tuple u = y, r in an event type structure, where y denotes the event type and r denotes an argument role, following Socher et al."}, {"id": 65, "string": "(2013b) , we assume an implicit relation exists between any pair of type and argument, and use a single and powerful tensor to represent the implicit relation: V u = [V y ; V r ] = f ([V y ; V r ] T \u00b7 U [1:2d] \u00b7 [V y ; V r ]) where V y and V r are vector representations for y and r. U [1:2d] \u2208 R 2d\u00d72d\u00d72d is a 3-order tensor."}, {"id": 66, "string": "V u is the composition representation of tuple u , which consists of two updated vector representations V y , V r for y and r by incorporating the semantics of their implicit relation U [1:2d] ."}, {"id": 67, "string": "Trigger and Argument Classification Trigger Classification for Seen Types Both event mention and event type structures are relatively simple and can be represented with a set of tuples."}, {"id": 68, "string": "CNNs have been demonstrated effective at capturing sentence level information by aggregating compositional n-gram representations."}, {"id": 69, "string": "In order to generate structure-level representations, we use CNN to learn to aggregate all edge and tuple representations."}, {"id": 70, "string": "Input layer is a sequence of tuples, where the order of tuples is from top to bottom in the structure."}, {"id": 71, "string": "Each tuple is represented by a d \u00d7 2 dimensional vector, thus each mention structure and each type structure are represented as a feature map of dimensionality d \u00d7 2h * and d \u00d7 2p * respectively, where h * and p * are the maximal number of tuples for event mention and type structures."}, {"id": 72, "string": "We use zero-padding to the right to make the volume of all input structures consistent."}, {"id": 73, "string": "Convolution layer Take S t with h * tuples: u 1 , u 2 , ..., u h * as an example."}, {"id": 74, "string": "The input matrix of S t is a feature map of dimensionality d \u00d7 2h * ."}, {"id": 75, "string": "We make c i as the concatenated embeddings of n continuous columns from the feature map, where n is the filter width and 0 < i < 2h * + n. A convolution operation involves a filter W \u2208 R nd , which is applied to each sliding window c i : c i = tanh(W \u00b7 c i + b) where c i is the new feature representation, and b \u2208 R d is a biased vector."}, {"id": 76, "string": "We set filter width as 2 and stride as 2 to make the convolution function operate on each tuple with two input columns."}, {"id": 77, "string": "Max-Pooling: All tuple representations c i are used to generate the representation of the input sequence by max-pooling."}, {"id": 78, "string": "Learning: For each event mention t, we name the correct type as positive and all the other types in the target event ontology as negative."}, {"id": 79, "string": "To train the composition functions and CNN, we first consider the following hinge ranking loss: L 1 (t, y) = j\u2208Y, j =y max{0, m \u2212 C t,y + C t,j } C t,y = cos([V t ; V St ], [V y ; V Sy ]) where y is the positive event type for t. Y is the type set of the target event ontology."}, {"id": 80, "string": "[V t ; V St ] denotes the concatenation of representations of t and S t ."}, {"id": 81, "string": "j is a negative event type for t from Y ."}, {"id": 82, "string": "m is a margin."}, {"id": 83, "string": "C t,y denotes the cosine similarity between t and y."}, {"id": 84, "string": "The hinge loss is commonly used in zero-shot visual object classification task."}, {"id": 85, "string": "However, it tends to overfit the seen types in our experiments."}, {"id": 86, "string": "While clever data augmentation can help alleviate overfitting, we design two strategies: (1) we add \"negative\" event mentions into the training process."}, {"id": 87, "string": "Here a \"negative\" event mention means that the mention has no positive event type among all seen types, namely it belongs to Other."}, {"id": 88, "string": "(2) we design a new loss function as follows: where Y is the type set of the event ontology."}, {"id": 89, "string": "Y is the seen type set."}, {"id": 90, "string": "y is the annotated type."}, {"id": 91, "string": "y is the type which ranks the highest among all event types for event mention t, while t belongs to Other."}, {"id": 92, "string": "By minimizing L d 1 , we can learn the optimized model which can compose structure representations and map both event mention and types into a shared semantic space, where the positive type ranks the highest for each mention."}, {"id": 93, "string": "Argument Classification for Seen Types For each mention, we map each candidate argument to a specific role based on the semantic similarity of the argument path."}, {"id": 94, "string": "Take E1 as an example."}, {"id": 95, "string": "China is matched to Agent based on the semantic similarity between dispatch-01\u2192 :ARG0\u2192 China and Transport-Person\u2192Agent."}, {"id": 96, "string": "Given a trigger t and a candidate argument a, we first extract a path S a = (u 1 , u 2 , ..., u p ), which connects t and a and consists of p tuples."}, {"id": 97, "string": "Each predefined role r is also represented as a structure by incorporating the event type, S r = y, r ."}, {"id": 98, "string": "We apply the same framework to take the sequence of tuples contained in S a and S r into a weightsharing CNN to rank all possible roles for a. where R y and R Y are the set of argument roles which are predefined for trigger type y and all seen types Y ."}, {"id": 99, "string": "r is the annotated role and r is the argument role which ranks the highest for a when a or y is annotated as Other."}, {"id": 100, "string": "In our experiments, we sample various size of \"negative\" training data for trigger and argument labeling respectively."}, {"id": 101, "string": "In the following section, we describe how the negative training instances are generated."}, {"id": 102, "string": "Zero-Shot Classification for Unseen Types During test, given a new event mention t , we compute its mention structure representation for S t and all event type structure representations for S Y = {S y 1 , S y 2 , ..., S yn } using the same parameters trained from seen types."}, {"id": 103, "string": "Then we rank all event types based on their similarity scores with mention t ."}, {"id": 104, "string": "The top ranked prediction for t from the event type set, denoted as y(t , 1), is given by: y(t , 1) = arg max y\u2208Y cos([V t ; V S t ], [V y ; V Sy ]) Moreover, y(t , k) denotes the k th most probable event type predicted for t ."}, {"id": 105, "string": "We will investigate the event extraction performance based on the topk predicted event types."}, {"id": 106, "string": "After determining the type y for mention t , for each candidate argument, we adopt the same ranking function to find the most appropriate role from the role set defined for y ."}, {"id": 107, "string": "Experiments Hyper-Parameters We used the English Wikipedia dump to learn trigger sense and argument embeddings based on the Continuous Skip-gram model ."}, {"id": 108, "string": "Table 2 We first used the ACE event schema 4 as our target event ontology and assumed the boundaries of triggers and arguments as given."}, {"id": 109, "string": "Of the 33 ACE event types, we selected the top-N most popular event types from ACE05 data as \"seen\" types, and used 90% event annotations of these for training and 10% for development."}, {"id": 110, "string": "We set N as 1, 3, 5, 10 respectively."}, {"id": 111, "string": "We tested the zero-shot classification performance on the annotations for the remaining 23 unseen types."}, {"id": 112, "string": "Table 3 shows the types that we selected for training in each experiment setting."}, {"id": 113, "string": "The negative event mentions and arguments that belong to Other were sampled from the output of the system developed by  based on ACE05 training sentences, which groups all candidate triggers and arguments into clusters based on semantic representations and assigns a type/role name to each cluster."}, {"id": 114, "string": "We sampled the negative event mentions from the clusters (e.g., Build, Threaten) which do not map to ACE event types."}, {"id": 115, "string": "We sampled the negative arguments from the arguments of negative event mentions."}, {"id": 116, "string": "Table 4 shows the statistics of the training, development and testing data sets."}, {"id": 117, "string": "To show the effectiveness of structural similarity in our approach, we designed a baseline, WSD-4 ACE event schema specification is at: https://www.ldc.upenn.edu/sites/www.ldc.upenn.edu/files/englishevents-guidelines-v5.4.3.pdf Embedding, which directly maps event mentions and arguments to their candidate types and roles using our pre-trained word sense embeddings."}, {"id": 118, "string": "Table 5 makes the contrast clear: structural similarity (our approach) is much more effective than lexical similarity (baseline) for both trigger and argument classification."}, {"id": 119, "string": "Also, as the number of seen types in training increases, the performance of the transfer model improves."}, {"id": 120, "string": "We further evaluated the performance of our transfer approach on similar and distinct unseen types."}, {"id": 121, "string": "The 33 subtypes defined in ACE fall within 8 coarse-grained main types, such as Life and Justice."}, {"id": 122, "string": "Each subtype belongs to one main type."}, {"id": 123, "string": "Subtypes that belong to the same main type tend to have similar structures."}, {"id": 124, "string": "For example, Trial-Hearing and Charge-Indict have the same set of argument roles."}, {"id": 125, "string": "For training our transfer model, we selected 4 subtypes of Justice: Arrest-Jail, Convict, Charge-Indict, Execute."}, {"id": 126, "string": "For testing, we selected 3 other subtypes of Justice: Sentence, Appeal, Release-Parole."}, {"id": 127, "string": "Additionally, we selected one subtype from each of the other seven main types for comparison."}, {"id": 128, "string": "Table 6 shows that, when testing on a new unseen type, the more similar it is to the seen types, the better performance is achieved."}, {"id": 129, "string": "ACE Event Identification & Classification The ACE2005 corpus includes the richest event annotations currently available for 33 types."}, {"id": 130, "string": "However, in real-world scenarios, there may be thousands of event types of interest."}, {"id": 131, "string": "To enrich the target event ontology and assess our transferable neural architecture on a large number of unseen types, when trained on limited annotations of seen types, we manually constructed a new event ontology which combined 33 ACE event types and argument roles, and 1,161 frames from FrameNet, except for the most generic frames such as Entity and Locale."}, {"id": 132, "string": "Some ACE event types were easily aligned to frames, e.g., Die aligned to Death."}, {"id": 133, "string": "Some frames were instead more accurately treated as inheritors of ACE types, such as Suicide-Attack, which inherits from Attack."}, {"id": 134, "string": "We manually mapped the selected frames to ACE types."}, {"id": 135, "string": "We then compared our approach with the following state-of-the-art supervised methods: \u2022 LSTM: A long short-term memory neural network (Hochreiter and Schmidhuber, 1997) based on distributed semantic features, similar     ."}, {"id": 136, "string": "\u2022 Joint: A structured perceptron model based on symbolic semantic features (Li et al., 2013) ."}, {"id": 137, "string": "For our approach, we followed the experiment setting D in the previous section, using the same training and development data sets for the 10 seen types, but targeted all 1,194 event types in our new event ontology, instead of just the 33 ACE event types."}, {"id": 138, "string": "For evaluation, we sampled 150 sentences from the remaining ACE05 data, including 129 annotated event mentions for the 23 unseen types."}, {"id": 139, "string": "For both LSTM and Joint approaches, we used the entire ACE05 annotated data for 33 ACE event types for training except for the held-out 150 evaluation sentences."}, {"id": 140, "string": "We first identified the candidate triggers and arguments, then mapped each of these to the target event ontology."}, {"id": 141, "string": "We evaluated our model on their extracting of event mentions which were classified into 23 testing ACE types."}, {"id": 142, "string": "Table 7 shows the per-formance."}, {"id": 143, "string": "To further demonstrate the effectiveness of zero-shot learning in our framework and its impact in saving human annotation effort, we used the supervised LSTM approach for comparison."}, {"id": 144, "string": "The training data of LSTM contained 3,464 sentences with 905 annotated event mentions for the 23 unseen event types."}, {"id": 145, "string": "We divided these event annotations into 10 subsets and successively added one subset at a time (10% of annotations) into the training data of LSTM."}, {"id": 146, "string": "Figure 4 shows the LSTM learning curve."}, {"id": 147, "string": "By contrast, without any annotated mentions on the 23 unseen test event types in its training set, our transfer learning approach achieved performance comparable to that of the LSTM, which was trained on 3,000 sentences 5 with 500 annotated event mentions."}, {"id": 148, "string": "Table 7 : Event Trigger and Argument Extraction Performance (%) on Unseen ACE Types."}, {"id": 149, "string": "Impact of AMR Recall that we used AMR parsing output to identify triggers and arguments in constructing event structures."}, {"id": 150, "string": "To assess the impact of the AMR parser (Wang et al., 2015a) on event extraction, we chose a subset of the ERE (Entity, Relation, Event) corpus (Song et al., 2015) which has ground-truth AMR annotations."}, {"id": 151, "string": "This subset contains 304 documents with 1,022 annotated event mentions of 40 types."}, {"id": 152, "string": "We selected the top-6 most popular event types (Arrest-Jail, Execute, Die, Meet, Sentence, Charge-Indict) with manual annotations of 548 event mentions as seen types."}, {"id": 153, "string": "We sampled 500 negative event mentions from distinct types of clusters generated from the system  based on ERE training sentences."}, {"id": 154, "string": "We combined the annotated events for seen types and the negative event mentions, and used 90% for training and 10% for development."}, {"id": 155, "string": "For evaluation, we selected 200 sentences from the remaining ERE subset, which contains 128 Attack event mentions and 40 Convict event mentions."}, {"id": 156, "string": "Table 8 shows the event extraction performances based on groundtruth AMR and system AMR respectively."}, {"id": 157, "string": "We also compared AMR analyses with Semantic Role Labeling (SRL) output (Palmer et al., 2010) by keeping only the core roles (e.g., :ARG0, :ARG1) from AMR annotations."}, {"id": 158, "string": "As Table 8 shows, comparing the full AMR (top row) to this SRL proxy (middle row), the fine-grained AMR semantic relations such as :location, :instrument appear to be more informative for inferring event argument role labeling."}, {"id": 159, "string": "Method Trigger Labeling Related Work Most previous event extraction methods have been based on supervised learning, using either symbolic features (Ji and Grishman, 2008; Miwa et al., 2009; Liao and Grishman, 2010; Liu et al., 2010; Hong et al., 2011; McClosky et al., 2011; Riedel and McCallum, 2011; Li et al., 2013; or distributional features (Chen et al., 2015; Nguyen and Grishman, 2015; Nguyen et al., 2016) derived from a large amount of training data, and treating event types and argument role labels as symbols."}, {"id": 160, "string": "These approaches can achieve high quality for known event types, but cannot be applied to new types without additional annotation effort."}, {"id": 161, "string": "In contrast, we provide a new angle on event extraction, modeling it as a generic grounding task by taking advantage of rich semantics of event types."}, {"id": 162, "string": "Some other IE paradigms such as Open IE (Etzioni et al., 2005; Banko et al., 2007 Banko et al., , 2008 Etzioni et al., 2011; Ritter et al., 2012) , Preemptive IE (Shinyama and Sekine, 2006) , Ondemand IE (Sekine, 2006) , Liberal IE (Huang et al., , 2017 , and semantic frame-based event discovery (Kim et al., 2013) can discover many events without pre-defined event schema."}, {"id": 163, "string": "These paradigms however rely on information redundancy, and so they are not effective when the input data only consists of a few sentences."}, {"id": 164, "string": "Our work can discover events from any size of input corpus and can also be complementary with these paradigms."}, {"id": 165, "string": "Our event extraction paradigm is similar to the task of entity linking (Ji and Grishman, 2011) in semantic mapping."}, {"id": 166, "string": "However, entity linking aims to map entity mentions to the same concept, while our framework maps each event mention to a specific category."}, {"id": 167, "string": "In addition, Bronstein et al."}, {"id": 168, "string": "(2015) and Peng et al."}, {"id": 169, "string": "(2016) employ an eventindependent similarity-based function for event trigger detection, which follows few-shot learning setting and requires some trigger examples as seeds."}, {"id": 170, "string": "Lu and Roth (2012) design a structure pref-erence modeling framework, which can automatically predict argument roles without any annotated data, but it relies on manually constructed patterns."}, {"id": 171, "string": "Zero-Shot learning has been widely applied in visual object classification Norouzi et al., 2013; Socher et al., 2013a; Chen et al., 2017; Li et al., 2017; Xian et al., 2017; Changpinyo et al., 2017) , fine-grained name tagging (Ma et al., 2016; Qu et al., 2016) , relation extraction (Verga et al., 2016; Levy et al., 2017) , semantic parsing (Bapna et al., 2017) and domain adaptation (Romera-Paredes and Torr, 2015; Kodirov et al., 2015; Peng et al., 2017) ."}, {"id": 172, "string": "In contrast to these tasks, for our case, the number of seen types in event extraction with manual annotations is quite limited."}, {"id": 173, "string": "The most popular event schemas, such as ACE, define 33 event types while most visual object training sets contain more than 1,000 types."}, {"id": 174, "string": "Therefore, methods proposed for zero-shot visual-object classification cannot be directly applied to event extraction due to overfitting."}, {"id": 175, "string": "In this work, we designed a new loss function by creating \"negative\" training instances to avoid overfitting."}, {"id": 176, "string": "Conclusions and Future Work In this work, we take a fresh look at the event extraction task and model it as a generic grounding problem."}, {"id": 177, "string": "We propose a transferable neural architecture, which leverages existing humanconstructed event schemas and manual annotations for a small set of seen types, and transfers the knowledge from the existing types to the extraction of unseen types, to improve the scalability of event extraction as well as to save human effort."}, {"id": 178, "string": "To the best of our knowledge, this work is the first time that zero-shot learning has been applied to event extraction."}, {"id": 179, "string": "Without any annotation, our approach can achieve performance comparable to state-of-the-art supervised models trained on a large amount of labeled data."}, {"id": 180, "string": "In the future, we will extend this framework to other Information Extraction problems."}], "headers": [{"section": "Introduction", "n": "1", "start": 6, "end": 32}, {"section": "Approach Overview", "n": "2", "start": 33, "end": 48}, {"section": "Trigger and Argument Identification", "n": "3", "start": 49, "end": 53}, {"section": "Trigger and Type Structure Composition", "n": "4", "start": 54, "end": 66}, {"section": "Trigger Classification for Seen Types", "n": "5.1", "start": 67, "end": 92}, {"section": "Argument Classification for Seen Types", "n": "5.2", "start": 93, "end": 101}, {"section": "Zero-Shot Classification for Unseen Types", "n": "5.3", "start": 102, "end": 106}, {"section": "Hyper-Parameters", "n": "6.1", "start": 107, "end": 128}, {"section": "ACE Event Identification & Classification", "n": "6.3", "start": 129, "end": 148}, {"section": "Impact of AMR", "n": "6.4", "start": 149, "end": 158}, {"section": "Related Work", "n": "7", "start": 159, "end": 175}, {"section": "Conclusions and Future Work", "n": "8", "start": 176, "end": 180}], "figures": [{"filename": "../figure/image/978-Table2-1.png", "caption": "Table 2: Hyper-parameters.", "page": 5, "bbox": {"x1": 74.88, "x2": 285.12, "y1": 112.8, "y2": 195.35999999999999}}, {"filename": "../figure/image/978-Table3-1.png", "caption": "Table 3: Seen Types in Each Experiment Setting.", "page": 5, "bbox": {"x1": 72.0, "x2": 284.15999999999997, "y1": 265.44, "y2": 337.91999999999996}}, {"filename": "../figure/image/978-Figure1-1.png", "caption": "Figure 1: Event Mention Example: dispatching is the trigger of a Transport-Person event with four arguments: the solid lines show the event annotations for the sentence while the dotted lines show the Abstract Meaning Representation parsing output.", "page": 1, "bbox": {"x1": 85.92, "x2": 509.28, "y1": 61.44, "y2": 144.96}}, {"filename": "../figure/image/978-Figure2-1.png", "caption": "Figure 2: Examples of Event Mention Structures and Type Structures from ACE.", "page": 1, "bbox": {"x1": 76.8, "x2": 282.24, "y1": 480.47999999999996, "y2": 645.12}}, {"filename": "../figure/image/978-Table6-1.png", "caption": "Table 6: Performance on Various Types Using Justice Subtypes for Training", "page": 6, "bbox": {"x1": 72.0, "x2": 301.44, "y1": 299.52, "y2": 421.91999999999996}}, {"filename": "../figure/image/978-Table4-1.png", "caption": "Table 4: Statistics for Positive/Negative Instances in Training, Dev, and Test Sets for Each Experiment.", "page": 6, "bbox": {"x1": 72.0, "x2": 532.3199999999999, "y1": 62.879999999999995, "y2": 133.92}}, {"filename": "../figure/image/978-Figure4-1.png", "caption": "Figure 4: Comparison between Our Approach and Supervised LSTM model on 23 Unseen Event Types.", "page": 6, "bbox": {"x1": 318.71999999999997, "x2": 511.2, "y1": 540.48, "y2": 671.04}}, {"filename": "../figure/image/978-Table5-1.png", "caption": "Table 5: Comparison between Structural Representation (Our Approach) and Word Sense Embedding based Approaches on Hit@K Accuracy (%) for Trigger and Argument Classification.", "page": 6, "bbox": {"x1": 72.0, "x2": 523.1999999999999, "y1": 171.84, "y2": 247.2}}, {"filename": "../figure/image/978-Figure3-1.png", "caption": "Figure 3: Architecture Overview. The blue circles denote event types and event type representations. The dark grey diamonds and circles denote triggers and trigger representations from training set. The light grey diamonds and circles denote triggers and trigger representations from testing set.", "page": 2, "bbox": {"x1": 80.64, "x2": 513.12, "y1": 61.44, "y2": 297.12}}, {"filename": "../figure/image/978-Table7-1.png", "caption": "Table 7: Event Trigger and Argument Extraction Performance (%) on Unseen ACE Types.", "page": 7, "bbox": {"x1": 72.0, "x2": 523.1999999999999, "y1": 62.879999999999995, "y2": 125.28}}, {"filename": "../figure/image/978-Table8-1.png", "caption": "Table 8: Impact of AMR and Semantic Roles on Trigger and Argument Extraction (%).", "page": 7, "bbox": {"x1": 72.0, "x2": 292.32, "y1": 628.8, "y2": 711.36}}, {"filename": "../figure/image/978-Table1-1.png", "caption": "Table 1: Event-Related AMR Relations.", "page": 3, "bbox": {"x1": 72.0, "x2": 292.32, "y1": 129.6, "y2": 192.95999999999998}}]}