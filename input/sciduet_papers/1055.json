{"title": "A Joint Model for Chinese Microblog Sentiment Analysis", "abstract": "Topic-based sentiment analysis for Chinese microblog aims to identify the user attitude on specified topics. In this paper, we propose a joint model by incorporating Support Vector Machines (SVM) and deep neural network to improve the performance of sentiment analysis. Firstly, a SVM Classifier is constructed using N-gram, N-POS and sentiment lexicons features. Meanwhile, a convolutional neural network is applied to learn paragraph representation features as the input of another SVM classifier. The classification results outputted by these two classifiers are merged as the final classification results. The evaluations on the SIGHAN-8 Topic-based Chinese microblog sentiment analysis task show that our proposed approach achieves the second rank on micro average F1 and the fourth rank on macro average F1 among a total of 13 submitted systems.", "text": [{"id": 0, "string": "Introduction With the development of the Internet, microblog has become a popular user-generated content platform where users share the newest events or their personal feelings with each other."}, {"id": 1, "string": "Topic-based microblogs are the most common interactive way for users to share their opinions towards a specified topic."}, {"id": 2, "string": "To identify the opinions of users, sentiment analysis techniques are investigated to classify texts into different categorizations according to their sentiment polarities."}, {"id": 3, "string": "Most existing sentiment classification techniques are based on machine learning algorithms, such as Support Vector Machine, Na\u00efve Bayes and Maximum Entropy."}, {"id": 4, "string": "The machine learning based approach uses feature vectors as the input of classification to predict the classification results."}, {"id": 5, "string": "Thus, feature engineering, a method for extracting effective features from texts, plays an important role."}, {"id": 6, "string": "Some commonly used features in sentiment classification are unigram, bigram and sentiment words."}, {"id": 7, "string": "However, these features cannot work well for cross-domain sentiment classification because of the lack of domain knowledge."}, {"id": 8, "string": "Danushka Bollegala et al."}, {"id": 9, "string": "(2011) used multiple sources to construct a sentiment sensitive thesaurus to overcome the lack of domain knowledge."}, {"id": 10, "string": "New sentiment words expansion is another kind of approach to improve the performance of sentiment analysis."}, {"id": 11, "string": "Strfano Baccianella et al."}, {"id": 12, "string": "(2010) constructed SentiWord-Net by extending WordNet with sentiment information."}, {"id": 13, "string": "It is now widely used in sentiment classification for English."}, {"id": 14, "string": "As for Chinese sentiment analysis, Minlie Huang et al."}, {"id": 15, "string": "(2014) proposed a new word detection method by mining the frequent sentiment word patterns."}, {"id": 16, "string": "This method may discover new sentiment words from a large scale of unlabeled texts."}, {"id": 17, "string": "With the rapid development of pre-trained word embedding and deep neural networks, a new way to represent texts and features is devloped."}, {"id": 18, "string": "Mikolov et al."}, {"id": 19, "string": "(2013) showed that word embedding represents words with meaningful syntactic and semantic information effectively."}, {"id": 20, "string": "Recursive neural network proposed by Socher et al."}, {"id": 21, "string": "(2011a; 2011b; is shown efficient to construct sentence representations based on the word embedding."}, {"id": 22, "string": "Convolutional neural networks (CNN), another deep learn model which achieved success in image recognition field, was applied to nature language processing with word embed-dings."}, {"id": 23, "string": "Yoon Kim (2014) used CNN with pretrained word embedding to achieve state-ofthe-art performances on some sentence classification tasks, including sentiment classification."}, {"id": 24, "string": "Siwei Lai et al."}, {"id": 25, "string": "(2015) incorporated global information in a recurrent convolutional neural network."}, {"id": 26, "string": "It obtained further improvements comparing to other deep learning models."}, {"id": 27, "string": "In this paper, we propose a joint model which incorporates traditional machine learning based method (SVM) and deep learning model."}, {"id": 28, "string": "Two different classifiers are developed."}, {"id": 29, "string": "One is a word feature based SVM classifier which uses word unigram, bigram and sentiment words as features."}, {"id": 30, "string": "Another one is a CNN-based SVM classifier which takes paragraph representations features learned by CNN as input features."}, {"id": 31, "string": "The classification results of these two classifiers are integrated to generate the final classification results."}, {"id": 32, "string": "The evaluations on the SIGHAN-8 Topic-based Chinese microblog sentiment analysis task show that our proposed approach achieves the second rank on micro average F1 and the fourth rank on macro average F1 among a total of 13 submitted systems."}, {"id": 33, "string": "Furthermore, the joint classifier strategy brings further performance improvement on individual classifiers."}, {"id": 34, "string": "The rest of this paper is organized as follows."}, {"id": 35, "string": "Section 2 presents the design and implementation of our proposed joint model."}, {"id": 36, "string": "Section 3 gives the evaluation results and discussions."}, {"id": 37, "string": "Finally, Section 4 gives the conclusion and future research directions."}, {"id": 38, "string": "Our Approach The SIGHAN8 topic-based Chinese polarity classification task aims to is to classify Chinese microblog into three topic-related sentiment classes, namely neutral, positive and negative."}, {"id": 39, "string": "This task may be generally regarded as a three-category classification problem."}, {"id": 40, "string": "The SVM classifier which has been shown effective to document classification is adopted as the core classifier."}, {"id": 41, "string": "Here, two different feature representation models, namely word-based vector space model and CNN-based composition representation, are adopted to generate the classification features for two classifiers, respectively."}, {"id": 42, "string": "The classification outputs of two clas-sifiers are integrated to generate the final output."}, {"id": 43, "string": "Data preprocessing Chinese microblog text is obviously different from formal text."}, {"id": 44, "string": "Many microblogs have noises, including nickname, hashtag, repost or reply symbols, and URL."}, {"id": 45, "string": "Therefore, before the feature representation and extraction, preprocessing is performed to filter out noise text in the microblogs."}, {"id": 46, "string": "Meanwhile, the advertising text and topic-irrelevant microblog are identified as neutral text."}, {"id": 47, "string": "Especially, this task is designed to identify the topic-relevant sentiments."}, {"id": 48, "string": "Therefore, the information coming from the reply, repost and sharing parts should be filtered out to avoid their influences to the sentiment analysis of the microblog author."}, {"id": 49, "string": "Generally speaking, such filtering is based on rules."}, {"id": 50, "string": "The table 1 shows the example data preprocessing rules with illustrations."}, {"id": 51, "string": "Table 2 shows the rules for identifying the advertisement and topic-irrelevant microblogs."}, {"id": 52, "string": "The identified microblogs are labeled as neutral for topic-based sentiment classification."}, {"id": 53, "string": "Word feature based classifier The word feature based classifier is designed based on the vector model."}, {"id": 54, "string": "Firstly, the new sentiment words from unlabeled sentences data are recognized to expand the sentiment lexicon."}, {"id": 55, "string": "The classification features are extracted from the labeled training data and sentiment lexicon resources."}, {"id": 56, "string": "In order to alleviate the influences of unbalanced training data, SMOTE, which is an oversampling algorithm, is applied to training data before classifier training."}, {"id": 57, "string": "Finally, a SVM classifier is trained on the balanced data."}, {"id": 58, "string": "The framework of word feature based classifier is shown in Figure 1."}, {"id": 59, "string": "Feature selection Unigram, Bigram, Uni-Part-of-Speech and Bi-Part-of-Speech features are selected as the basic features."}, {"id": 60, "string": "CHI-test based feature selection is applied to obtain the top 20000 features."}, {"id": 61, "string": "To improve the performance of sentiment classification, additional features based on lexicons including sentiment word lexicons, negation word lexicons, and adverb word lexicons, are incorporated."}, {"id": 62, "string": "Rules Raw Text Processed Text Sharing news with \u597d\u770b\uff1f\u5417\uff1f//\u3010Galaxy S6\uff1a\u4e09\u661f\u8bc1\u660e\u81ea \u597d\u770b\uff1f\u5417\uff1f personal comments \u5df1\u80fd\u505a\u51fa\u597d\u770b\u7684\u624b\u673a\u3011http: //t.cn/ RwHRsIb(\u5206\u4eab\u81ea @ \u4eca\u65e5\u5934\u6761) Removing HashTag # \u4e09\u661f Galaxy S6# \u4e09\u661f GALAXY S6 \u4e09\u661f GALAXY S6\uff0c \uff0c\u633a\u4e2d\u610f [\u9177][\u9177] [\u4f4d\u7f6e] \u8292\u7800\u8def \u633a\u4e2d\u610f [\u9177][\u9177] Removing URL 699 \u6b27\u5143\u8d77\u4f20\u4e09\u661f Galaxy S6/S6 Edge \u552e 699 \u6b27\u5143\u8d77\u4f20\u4e09\u661f Galaxy \u4ef7\u83b7\u8bc1\u5b9e(\u5206\u4eab\u81ea @ \u65b0\u6d6a\u79d1\u6280) S6/S6 Edge \u552e\u4ef7\u83b7\u8bc1\u5b9e http://t.cn/RwTo3on (\u5206\u4eab\u81ea @ \u65b0\u6d6a\u79d1\u6280) Removing nickname \u73bb\u7483\u53d6\u4ee3\u5851\u6599\uff0c\u66f4\u7f8e Galaxy S6 \u7684 5 \u5927 http://t.cn/RwHY6Az \u59a5\u534f http://t.cn/RwHY6Az \u7f57\u6c38\u6d69\u6211\u53bb \u7f57\u6c38\u6d69\u6211\u53bb\u5c0f\u7c73\u548c\u4e09\u661f\u8fd9 \u5c0f\u7c73\u548c\u4e09\u661f\u8fd9\u662f\u8981\u95f9\u54ea\u6837\uff0c \uff0c \uff0c\u8001\u7f57\u3002 \u3002\u4e0d \u662f\u8981\u95f9\u54ea\u6837\uff0c \uff0c \uff0c\u8001\u7f57\u3002 \u3002 \u80fd\u5fcd\u554a\uff0c \uff0c \uff0c \uff0c \uff0c@ \u9524\u5b50\u79d1\u6280\u8425\u9500\u5e10\u53f7 @ \u7f57 \u4e0d\u80fd\u5fcd\u554a\uff0c \uff0c \uff0c \uff0c \uff0c \u6c38\u6d69 Removing \u3010\u89c6\u9891\uff1a\u4e09\u661f S6 \u5bf9\u6bd4\u82f9\u679c iPhone6 \u3010\u89c6\u9891\uff1a\u4e09\u661f S6 \u5bf9\u6bd4\u82f9\u679c information sources MWC2015 @youtube \u79d1\u6280 \u3011 iPhone6 MWC2015 http://t.cn/RwHQzJ8(\u6765\u81ea\u4e8e\u4f18\u9177\u5b89 @youtube \u79d1\u6280 \u3011 \u5353\u5ba2\u6237\u7aef) http://t.cn/RwHQzJ8  Rules Type Including many different Advertisement topic (\"#...#\") tag."}, {"id": 63, "string": "Including many words Advertisement like \"\u5fae\u5546\", \"\u5546\u673a\", \"\u60f3\u8d5a\u94b1\",\"\u9762\u819c\"."}, {"id": 64, "string": "No actual content Topic-irrelevant Table 2 : Microblog text matching rules."}, {"id": 65, "string": "By analyzing the expressions of the microblog text in training data, some special expression features in microblog text are identified."}, {"id": 66, "string": "For example, the continuous punctuations are always used to express a strong feeling and thus, the microblog with continuous punctuations tends to be subjective."}, {"id": 67, "string": "Another adopted feature for microblog text is the use of emoticons."}, {"id": 68, "string": "Sentiment lexicon expansion In microblogs, abundant new or informal sentiment words are widely used."}, {"id": 69, "string": "Normally, these new sentiment words are short but meaningful for expressing a strong feeling."}, {"id": 70, "string": "These new sentiment words play an important role in Chinese microblog sentiment classification."}, {"id": 71, "string": "Therefore, sentiment word identification is performed to recognize new sentiment words as the supplement of sentiment lexicon."}, {"id": 72, "string": "Twenty million microblog text collected from Sina Weibo Platform are used in new sentiment word detection."}, {"id": 73, "string": "Considering that new words normally cannot be correctly segmented by the existing segmentor, identifying new words from preliminary segmentation results together with their POS tags is a feasible method."}, {"id": 74, "string": "Here, potential components for new words are limited to the segmentation tokens shorter than three."}, {"id": 75, "string": "Using word frequency, mutual information and context entropy as the evaluation indicators for words, the most possible new word candidates are obtained."}, {"id": 76, "string": "With the help of word embedding construction model, each word in the corpus can be represented as a low dimension vector together with its context information."}, {"id": 77, "string": "Hence, the distances between the new words and the existed sentiment words corresponding to difference sentiment polarity are estimated."}, {"id": 78, "string": "The new words are then classified into one of the three polarity classes by following voting mechanism."}, {"id": 79, "string": "Classification Two steps are performed to determine the topic-relevant sentiment for input microblogs."}, {"id": 80, "string": "The first step is to distinguish topic relevant messages from topic irrelevant messages."}, {"id": 81, "string": "Sentiment classification is then applied to topic relevant messages in the second step."}, {"id": 82, "string": "Topic relevant words generated by clustering analysis are employed as distinguishable features to filter out topic irrelevant microblogs because normally the topic irrelevant microblogs have few intersections with topic relevant words."}, {"id": 83, "string": "Some advertisement posts consisting of several hot topic hash tags are also filtered out by considering the number of hash tag types in the microblog."}, {"id": 84, "string": "The provided labeled dataset is used to train the SVM classifier with linear kernel."}, {"id": 85, "string": "A new challenge is that the provided training set is imbalanced."}, {"id": 86, "string": "There are about 3973 neutral microblogs, while the numbers of positive and negative microblogs are 394 and 538, respectively."}, {"id": 87, "string": "In order to reduce the influences of imbalanced training dataset, the SMOTE algorithm (Chawla et al., 2002) is applied to oversampling the samples on minority class."}, {"id": 88, "string": "Oversampling ratio is set to 10 and 7.4 for positive class and negative class, respectively."}, {"id": 89, "string": "In this way, the training dataset becomes balanced."}, {"id": 90, "string": "Another classifier is CNN-based SVM classifier."}, {"id": 91, "string": "The classifier framework is shown in Figure 2 ."}, {"id": 92, "string": "Firstly, continuous bog of word (CBOW) model (Mikolov et al., 2013 ) is used to learn word embeddings from Chinese microblog text."}, {"id": 93, "string": "A deep convolutional neural networks (CNN) model is applied to learn distributed paragraph representation features for Chinese microblog training and testing data."}, {"id": 94, "string": "Finally, the distributed paragraph representation features are used in SVM classifier to learn the probability distribution over sentiment labels."}, {"id": 95, "string": "CNN-based SVM classifier Word embedding construction Word embedding, wherein words are projected from a sparse, 1-of-V encoding (here V is the vocabulary size) onto a lower dimensional vector space via a hidden layer, are essentially feature extractors that encode semantic features of words in their dimensions."}, {"id": 96, "string": "Mikolov et al."}, {"id": 97, "string": "(2013) introduced CBOW model to learn vector representations which captures a large number of syntactic and semantic word relationships from unstructured text data."}, {"id": 98, "string": "The main idea of this model is to find word representations which use the surrounding words in a sentence or a document to predict current word."}, {"id": 99, "string": "In this study, we train the CBOW model by using 16GB Chinese microblog text."}, {"id": 100, "string": "Finally, we obtain 200-dimension word embeddings for Chinese microblog text."}, {"id": 101, "string": "CNN-based SVM classifier In the CNN-based SVM classifier, the input is a matrix which is composed of the word embeddings of microblogs."}, {"id": 102, "string": "There are windows with the lengths of three, four and five words, respectively."}, {"id": 103, "string": "A convolution operation involves three filters which are applied to these windows to produce new features."}, {"id": 104, "string": "After convolution operation, a max-over-time pooling operation is applied over these features."}, {"id": 105, "string": "The maximum value is taken as the feature corresponding to this particular filter."}, {"id": 106, "string": "The idea is to capture the most important feature which has the largest value."}, {"id": 107, "string": "Since one feature is extracted from one filter, the model uses multiple filters (with varying window sizes) to obtain multiple features."}, {"id": 108, "string": "These features constitute the distributed paragraph feature representation."}, {"id": 109, "string": "In the last step, a SVM classifier is applied on these distributed paragraph representation features to obtain the probability distributions over labels (positive, negative, and neutral)."}, {"id": 110, "string": "A set of merging rules is designed to incorporate the individual classification results of the two classifiers for generating the final result."}, {"id": 111, "string": "If the two classification outputs are the same, naturally, the final output is the same."}, {"id": 112, "string": "If the two classification outputs are different, the final result is determined from the merge rules shown in Table 3 ."}, {"id": 113, "string": "Simply speaking, if any of two classifiers output neutral category, the final output is neutral."}, {"id": 114, "string": "If two classifiers outputs positive and negative, respectively, the final output is the result of CNN-based clas-sifier."}, {"id": 115, "string": "Such a classification outputs merging strategy is based on the statistical analysis on the individual classifier performances on training dataset."}, {"id": 116, "string": "Outputs Merging Experimental results and analysis Data set In the SIGHAN-8 Chinese sentiment analysis bakeoff dataset, 4905 topic-based Chinese microblog are provided as training data which consists of 394 positive, 538 negative and 3973 neutral microblogs corresponding to 5 topics, namely \"\u592e\u884c\u964d\u606f\", \"\u6cb9\u4ef7\", \"\u65e5\u672c\u9a6c\u6876\", \"\u4e09\u661f S6\"and \"\u96fe\u973e\"."}, {"id": 117, "string": "In the testing data, there are 19,469 microblogs corresponding to 20 topic, such as \"12306 \u9a8c\u8bc1\u7801\", \"\u4e2d\u56fd\u653f \u5e9c\u4e5f\u95e8\u64a4\u4fa8\", \"\u4f55\u4ee5\u7b19\u7bab\u9ed8\", \"\u5218\u7fd4\u9000\u5f79\"."}, {"id": 118, "string": "Metrics Precision, recall and F1-value are used as the evaluation metrics, as shown below: P recision = SystemCorrect SystemOutput (1) Recall = SystemCorrect HumanLabeled (2) F 1 = 2 \u00d7 P recision \u00d7 Recall P recision + Recall (3) Where System.Output refers to the total number of the submitted results, System.Correct refers to the number of correctly classified results in the submitted results, Human.Labeled refers to the total number of manually labeled results in the Gold Standard."}, {"id": 119, "string": "The evaluation metrics corresponding to positive, negative and overall are estimated, respectively."}, {"id": 120, "string": "The corresponding microaverage and macro-average performances are then estimated."}, {"id": 121, "string": "The micro-average estimates the average performance of the three evaluation metrics over the entire dataset."}, {"id": 122, "string": "The macro-average estimates the average performances of the evaluation metrics on positive, negative and neutral, respectively."}, {"id": 123, "string": "Experimental results and analysis There are two subtasks in SIGHAN-8 topicbased Chinese microblog polarity classification Table 6 : Performances by different classifiers in unrestricted resource subtask."}, {"id": 124, "string": "task: restricted resource and unrestricted resource subtasks."}, {"id": 125, "string": "Table 4 gives the performances in restricted resource subtask."}, {"id": 126, "string": "The first column lists the name of participants who achieves higher macro average F1 values while out system is named as HLT_HITSZ."}, {"id": 127, "string": "It is observed that our proposed approach achieves better performance on negative and positive categories, but obviously lower performance on neutral category."}, {"id": 128, "string": "The good performance on the recall of minority classes showed the effectiveness of our consideration on imbalanced dataset training."}, {"id": 129, "string": "The achieved performances in the unrestricted resource subtask are listed in Table 5 ."}, {"id": 130, "string": "Our system achieves about 3% of performance improvement on each category, respectively."}, {"id": 131, "string": "It shows the contributions of extra training corpus and merging rules."}, {"id": 132, "string": "In order to validate the effectiveness of merging rules, the performances of Classifier 1 and Classifier 2 are evaluated, individually."}, {"id": 133, "string": "The achieved performances are given in Table 6."}, {"id": 134, "string": "It is observed that generally speaking, Classifier 1 achieves a higher classification precision because many features are coming from manually compiled sentiment-related lexicons."}, {"id": 135, "string": "However, these features are limited to training data so that Classifier 1 achieved a lower recall."}, {"id": 136, "string": "On the contrary, Classifier 2 may learn the representation features automatically from training data which is better for generalization."}, {"id": 137, "string": "Thus, a good recall is achieved."}, {"id": 138, "string": "Meanwhile, the achieved performances show that our joint model obtains better performances compared to two individual classifiers which indicate the effectiveness of our proposed joint classification strategy."}, {"id": 139, "string": "Conclusion In this work, we propose a joint model for sentiment topic analysis on Chinese microblog messages."}, {"id": 140, "string": "A word feature based SVM classifier and a SVM classifier using CNN-based paragraph representation features are developed, respectively."}, {"id": 141, "string": "To overcome the limitation of each classifier, their classification outputs are merged to generate the final output while the merging rules are based on statistical analy-sis on the performances on training dataset."}, {"id": 142, "string": "Experimental results show that our proposed joint method achieves better sentiment classification performance over individual classifiers which show the effectiveness of the joint classifier strategy."}, {"id": 143, "string": "In future, we intend to study the way to distinguish the subjective messages from objective messages for further improving the sentiment classification performance."}], "headers": [{"section": "Introduction", "n": "1", "start": 0, "end": 37}, {"section": "Our Approach", "n": "2", "start": 38, "end": 42}, {"section": "Data preprocessing", "n": "2.1", "start": 43, "end": 52}, {"section": "Word feature based classifier", "n": "2.2", "start": 53, "end": 58}, {"section": "Feature selection", "n": "2.2.1", "start": 59, "end": 67}, {"section": "Sentiment lexicon expansion", "n": "2.2.2", "start": 68, "end": 78}, {"section": "Classification", "n": "2.2.3", "start": 79, "end": 94}, {"section": "Word embedding construction", "n": "2.3.1", "start": 95, "end": 100}, {"section": "CNN-based SVM classifier", "n": "2.3.2", "start": 101, "end": 114}, {"section": "Data set", "n": "3.1", "start": 115, "end": 117}, {"section": "Metrics", "n": "3.2", "start": 118, "end": 122}, {"section": "Experimental results and analysis", "n": "3.3", "start": 123, "end": 138}, {"section": "Conclusion", "n": "4", "start": 139, "end": 143}], "figures": [{"filename": "../figure/image/1055-Table1-1.png", "caption": "Table 1: Data preprocessing rules with illustrations.", "page": 2, "bbox": {"x1": 72.0, "x2": 525.12, "y1": 62.879999999999995, "y2": 310.08}}, {"filename": "../figure/image/1055-Figure1-1.png", "caption": "Figure 1: Framework of word feature based classifier", "page": 2, "bbox": {"x1": 70.56, "x2": 294.24, "y1": 376.8, "y2": 699.36}}, {"filename": "../figure/image/1055-Table2-1.png", "caption": "Table 2: Microblog text matching rules.", "page": 2, "bbox": {"x1": 307.68, "x2": 525.12, "y1": 354.71999999999997, "y2": 452.15999999999997}}, {"filename": "../figure/image/1055-Table6-1.png", "caption": "Table 6: Performances by different classifiers in unrestricted resource subtask.", "page": 5, "bbox": {"x1": 73.92, "x2": 523.1999999999999, "y1": 303.84, "y2": 375.36}}, {"filename": "../figure/image/1055-Table4-1.png", "caption": "Table 4: Performances in restricted resource subtask.", "page": 5, "bbox": {"x1": 70.56, "x2": 538.0799999999999, "y1": 63.839999999999996, "y2": 148.32}}, {"filename": "../figure/image/1055-Table5-1.png", "caption": "Table 5: Performances in unrestricted resource subtask.", "page": 5, "bbox": {"x1": 70.56, "x2": 531.36, "y1": 183.84, "y2": 269.28}}, {"filename": "../figure/image/1055-Table3-1.png", "caption": "Table 3: Merging rules for two classifiers.", "page": 4, "bbox": {"x1": 70.56, "x2": 292.32, "y1": 467.52, "y2": 564.0}}, {"filename": "../figure/image/1055-Figure2-1.png", "caption": "Figure 2: CNN and SVM joint classifier.", "page": 3, "bbox": {"x1": 306.71999999999997, "x2": 537.12, "y1": 90.72, "y2": 329.28}}]}