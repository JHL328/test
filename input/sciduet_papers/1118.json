{"title": "Document Embedding Enhanced Event Detection with Hierarchical and Supervised Attention", "abstract": "Document-level information is very important for event detection even at sentence level. In this paper, we propose a novel Document Embedding Enhanced Bi-RNN model, called DEEB-RNN, to detect events in sentences. This model first learns event detection oriented embeddings of documents through a hierarchical and supervised attention based RNN, which pays word-level attention to event triggers and sentence-level attention to those sentences containing events. It then uses the learned document embedding to enhance another bidirectional RNN model to identify event triggers and their types in sentences. Through experiments on the ACE-2005 dataset, we demonstrate the effectiveness and merits of the proposed DEEB-RNN model via comparison with state-of-the-art methods.", "text": [{"id": 0, "string": "Introduction Event Detection (ED) is an important subtask of event extraction."}, {"id": 1, "string": "It extracts event triggers from individual sentences and further identifies the type of the corresponding events."}, {"id": 2, "string": "For instance, according to the ACE-2005 annotation guideline, in the sentence \"Jane and John are married\", an ED system should be able to identify the word \"married\" as a trigger of the event \"Marry\"."}, {"id": 3, "string": "However, it may be difficult to identify events from isolated sentences, because the same event trigger might represent different event types in different contexts."}, {"id": 4, "string": "Existing ED methods can mainly be categorized into two classes, namely, feature-based methods (e.g., (McClosky et al., 2011; Hong et al., 2011; Li et al., 2014) ) and representation-based methods (e.g., (Nguyen and Grishman, 2015; Chen et al., 2015; Liu et al., 2016a; )."}, {"id": 5, "string": "The former mainly rely on a set of hand-designed features, while the latter employ distributed representation to capture meaningful semantic information."}, {"id": 6, "string": "In general, most of these existing methods mainly exploit sentence-level contextual information."}, {"id": 7, "string": "However, document-level information is also important for ED, because the sentences in the same document, although they may contain different types of events, are often correlated with respect to the theme of the document."}, {"id": 8, "string": "For example, there are the following sentences in ACE-2005: ..."}, {"id": 9, "string": "I knew it was time to leave."}, {"id": 10, "string": "Isn't that a great argument for term limits?"}, {"id": 11, "string": "..."}, {"id": 12, "string": "If we only examine the first sentence, it is hard to determine whether the trigger \"leave\" indicates a \"Transport\" event meaning that he wants to leave the current place, or an \"End-Position\" event indicating that he will stop working for his current organization."}, {"id": 13, "string": "However, if we can capture the contextual information of this sentence, it is more confident for us to label \"leave\" as the trigger of an \"End-Position\" event."}, {"id": 14, "string": "Upon such observation, there have been some feature-based studies (Ji and Grishman, 2008; Liao and Grishman, 2010; Huang and Riloff, 2012) that construct rules to capture document-level information for improving sentence-level ED."}, {"id": 15, "string": "However, they suffer from two major limitations."}, {"id": 16, "string": "First, the features used therein often need to be manually designed and may involve error propagation due to natural language processing; Second, they discover inter-event information at document level by constructing inference rules, which is time-consuming and is hard to make the rule set as complete as possible."}, {"id": 17, "string": "Besides, a representation-based study has been presented in (Duan et al., 2017) , which employs the PV-DM model to train document embeddings and further uses it in a RNN-based event classifier."}, {"id": 18, "string": "However, as being limited by the unsupervised training process, the document-level representation cannot specifically capture event-related information."}, {"id": 19, "string": "In this paper, we propose a novel Document Embedding Enhanced Bi-RNN model, called DEEB-RNN, for ED at sentence level."}, {"id": 20, "string": "This model first learns ED oriented embeddings of documents through a hierarchical and supervised attention based bidirectional RNN, which pays word-level attention to event triggers and sentence-level attention to those sentences containing events."}, {"id": 21, "string": "It then uses the learned document embeddings to facilitate another bidirectional RNN model to identify event triggers and their types in individual sentences."}, {"id": 22, "string": "This learning process is guided by a general loss function where the loss corresponding to attention at both word and sentence levels and that of event type identification are integrated."}, {"id": 23, "string": "It should be mentioned that although the attention mechanism has recently been applied effectively in various tasks, including machine translation , question answering (Hao et al., 2017) , document summarization (Tan et al., 2017) , etc., this is the first study, to the best of our knowledge, which adopts a hierarchical and supervised attention mechanism to learn ED oriented embeddings of documents."}, {"id": 24, "string": "We evaluate the developed DEEB-RNN model on the benchmark dataset, ACE-2005, and systematically investigate the impacts of different supervised attention strategies on its performance."}, {"id": 25, "string": "Experimental results show that the DEEB-RNN model outperforms both feature-based and representation-based state-of-the-art methods in terms of recall and F1-measure."}, {"id": 26, "string": "The Proposed Model We formalize ED as a multi-class classification problem."}, {"id": 27, "string": "Given a sentence, we treat every word in it as a trigger candidate, and classify each candidate to a certain event type."}, {"id": 28, "string": "In the ACE-2005 dataset, there are 8 event types, further being divided into 33 subtypes, and a \"Not Applicable (NA)\" type."}, {"id": 29, "string": "Without loss of generality, in this paper we regard the 33 subtypes as 33 event types."}, {"id": 30, "string": "Figure 1 presents the schematic diagram of the proposed DEEB-RNN model, which contains two main modules: The ED Oriented Document Embedding Learning (EDODEL) module, which learns the distributed representations of documents from both word and sentence levels via the well-designed hierarchical and supervised attention mechanism."}, {"id": 31, "string": "2."}, {"id": 32, "string": "The Document-level Enhanced Event Detector (DEED) module, which tags each trigger candidate with an event type based on the learned embedding of documents."}, {"id": 33, "string": "The EDODEL Module To learn the ED oriented embedding of a document, we apply the hierarchical and supervised attention network presented in Figure 1 , which consists of a word-level Bi-GRU (Schuster and Paliwal, 2002 ) encoder with attention on event triggers and a sentence-level Bi-GRU encoder with attention on sentences with events."}, {"id": 34, "string": "Given a document with L sentences, DEEB-RNN learns its embedding for detecting events in all sentences."}, {"id": 35, "string": "Word-level embeddings Given a sentence s i (i = 1, 2, ..., L) consisting of words {w it |t = 1, 2, ..., T }."}, {"id": 36, "string": "For each word w it , we first concatenate its embedding w it and its entity type embedding 1 e it (Nguyen and Grishman, 2015) as the input g it of a Bi-GRU and thus obtain the bidirectional hidden state h it : h it = [ \u2212 \u2212\u2212\u2212 \u2192 GRU w (g it ), \u2190 \u2212\u2212\u2212 \u2212 GRU w (g it )]."}, {"id": 37, "string": "(1) We then feed h it to a perceptron with no bias to get u it = tanh(W w h it ) as a hidden representation of h it and also obtain an attention weight \u03b1 it = u T it c w , which should be normalized through a softmax function."}, {"id": 38, "string": "Here, similar to that in (Yang et al., 2016) , c w is a vector representing the wordlevel context of w it , which is initialized at random."}, {"id": 39, "string": "Finally, the embedding of the sentence s i can be obtained by summing up h it with their weights: s i = T \u2211 t=1 \u03b1 it h it ."}, {"id": 40, "string": "(2) To pay more attention to trigger words than other words, we construct the gold word-level attention signals \u03b1 * i for the sentence s i , as illustrated in Figure 2a ."}, {"id": 41, "string": "We can then take the square error as the general loss of the attention at word level to supervise the learning process: E w (\u03b1 * , \u03b1) = L \u2211 i=1 T \u2211 t=1 (\u03b1 * it \u2212 \u03b1 it ) 2 ."}, {"id": 42, "string": "(3) 1 The words in the ACE-2005 dataset are annotated with their entity types (annotated as \"NA\" if they are not an entity)."}, {"id": 43, "string": "Sentence-level embeddings Given the sentence embeddings {s i |i = 1, 2, ..., L}, we first get the hidden state q i via a Bi-GRU: q i = [ \u2212\u2212\u2212\u2192 GRU s (s i ), \u2190\u2212\u2212\u2212 GRU s (s i )]."}, {"id": 44, "string": "(4) Then we feed q i to a perceptron with no bias to get the hidden representation t i = tanh(W s q i ) and also obtain an attention weight \u03b2 i = t T i c s to be normalized via softmax."}, {"id": 45, "string": "Similarly, c s represents the sentence-level context of s i to be randomly initialized."}, {"id": 46, "string": "We eventually obtain the document embedding d as: d = L \u2211 i=1 \u03b2 i s i ."}, {"id": 47, "string": "(5) We also think that the sentences containing event should obtain more attention than other ones."}, {"id": 48, "string": "Therefore, similar to the case at word level, we construct the gold sentence-level attention signals \u03b2 * for the document d, as illustrated in Figure 2b , and further take the square error as the general loss of the attention at sentence level to supervise the learning process: E s (\u03b2 * , \u03b2) = L \u2211 i=1 (\u03b2 * i \u2212 \u03b2 i ) 2 ."}, {"id": 49, "string": "(6) The DEED Module We employ another Bi-GRU encoder and a softmax output layer to model the ED task, which can handle event triggers with multiple words."}, {"id": 50, "string": "Specifically, given a sentence s j (j = 1, 2, ..., L) in document d, for each of its word w jt (t = 1, 2, ..., T ), we concatenate its word embedding w jt and entity type embedding e jt with the corresponding document embedding d as the input r jt of the Bi-GRU and thus obtain the hidden state f jt : f jt = [ \u2212\u2212\u2212\u2192 GRU e (r jt ), \u2190\u2212\u2212\u2212 GRU e (r jt )]."}, {"id": 51, "string": "(7) Finally, we get the probability vector o jt with K dimensions through a softmax layer for w jt , where the k-th element, o jt , of o jt indicates the probability of classifying w jt to the k-th event type."}, {"id": 52, "string": "The loss function , J(y, o) , can thus be defined in terms of the cross-entropy error of the real event type y jt and the predicted probability o (k) jt as follows: J(y, o) = \u2212 L \u2211 j=1 T \u2211 t=1 K \u2211 k=1 I(y jt = k)log o (k) jt , (8) where I(\u00b7) is the indicator function."}, {"id": 53, "string": "Joint Training of the DEEB-RNN model In the DEEB-RNN model, the above two modules are jointly trained."}, {"id": 54, "string": "For this purpose, we define the joint loss function in the training process upon the losses specified for different modules as follows: J(\u03b8) = \u2211 \u2200d\u2208\u03d5 (J(y, o)+\u03bbE w (\u03b1 * , \u03b1)+\u00b5E s (\u03b2 * , \u03b2)), (9) where \u03b8 denotes, as a whole, the parameters used in DEEB-RNN, \u03d5 is the training document set, and \u03bb and \u00b5 are hyper-parameters for striking a balance among J(y, o), E w (\u03b1 * , \u03b1) and E s (\u03b2 * , \u03b2)."}, {"id": 55, "string": "Experiments Datasets and Settings We validate the proposed model through comparison with state-of-the-art methods on the ACE-2005 dataset."}, {"id": 56, "string": "In the experiments, the validation set has 30 documents from different genres, the test set has 40 documents and the training set contains the remaining 529 documents."}, {"id": 57, "string": "All the data preprocessing and evaluation criteria follow those in (Ghaeini et al., 2016) ."}, {"id": 58, "string": "Hyper-parameters are tuned on the validation set."}, {"id": 59, "string": "We set the dimension of the hidden layers corresponding to GRU w , GRU s , and GRU e to 300, 200, and 300, respectively, the output size of W w and W s to 600 and 400, respectively, the dimension of entity type embeddings to 50, the batch size to 25, the dropout rate to 0.5."}, {"id": 60, "string": "In addition, we utilize the pre-trained word embeddings with 300 dimensions from (Mikolov et al., 2013) for initialization."}, {"id": 61, "string": "For entity types, their embeddings are randomly initialized."}, {"id": 62, "string": "We train the model using Stochastic Gradient Descent (SGD) over shuffled mini-batches and using dropout (Krizhevsky et al., 2012) for regularization."}, {"id": 63, "string": "Baseline Models In order to validate the proposed DEEB-RNN model through experimental comparison, we choose the following typical models as the baselines."}, {"id": 64, "string": "Sentence-level is a feature-based model proposed in (Hong et al., 2011) , which regards entitytype consistency as a key feature to predict event mentions."}, {"id": 65, "string": "Joint Local is a feature-based model developed in (Li et al., 2013) , which incorporates such features that explicitly capture the dependency among multiple triggers and arguments."}, {"id": 66, "string": "JRNN is a representation-based model proposed in , which exploits the inter-dependency between event triggers and argument roles via discrete structures."}, {"id": 67, "string": "Skip-CNN is a representation-based model presented in , which proposes a novel convolution to exploit nonconsecutive k-grams for event detection."}, {"id": 68, "string": "ANN-S2 is a representation-based model developed in , which explicitly exploits argument information for event detection via supervised attention mechanisms."}, {"id": 69, "string": "Cross-event is a feature-based model proposed in (Liao and Grishman, 2010) , which learns relations among event types from training corpus and futher helps predict the occurrence of events."}, {"id": 70, "string": "PSL is a feature-based model developed in (Liu et al., 2016b) , which encods global information such as event-event association in the form of logic using the probabilistic soft logic model."}, {"id": 71, "string": "DLRNN is a representation-based model proposed in (Duan et al., 2017) , which automatically extracts cross-sentence clues to improve sentencelevel event detection."}, {"id": 72, "string": "Impacts of Different Attention Strategies In this section, we conduct experiments on the ACE-2005 dataset to demonstrate the effectiveness of different attention strategies."}, {"id": 73, "string": "Bi-GRU is the basic ED model, which does not employ document-level embeddings."}, {"id": 74, "string": "DEEB-RNN uses the document embeddings and computes attentions without supervision, in which hyper-parameters \u03bb and \u00b5 are set to 0."}, {"id": 75, "string": "DEEB-RNN1/2/3 means they uses the gold attention signals as supervision information."}, {"id": 76, "string": "Specifically, DEEB-RNN1 uses only the gold word-level attention signal (\u03bb = 1 and \u00b5 = 0), DEEB-RNN2 uses only the gold sentence-level attention signal (\u03bb = 0 and \u00b5 = 1), whilst DEEB-RNN3 employs the gold attention signals at both word and sen-  tence levels (\u03bb = 1 and \u00b5 = 1)."}, {"id": 77, "string": "Table 1 compares these methods, where we can observe that the methods with document embeddings (i.e., the last four) significantly outperform the pure Bi-GRU method, which suggests that document-level information is very beneficial for ED."}, {"id": 78, "string": "An interesting phenomenon is that, as compared to DEEB-RNN, DEEB-RNN2 changes the precision-recall balance."}, {"id": 79, "string": "This is because of the following reasons."}, {"id": 80, "string": "On one hand, as compared to DEEB-RNN, DEEB-RNN2 uses the gold sentence-level attention signal, indicating that it pays special attention to the sentences containing events with event triggers."}, {"id": 81, "string": "In this way, the Bi-RNN model for learning document embeddings will filter out the sentences containing events but without explicit event triggers."}, {"id": 82, "string": "That means the events detected by DEEB-RNN2 are basically the ones with explicit event triggers."}, {"id": 83, "string": "Therefore, as compared to DEEB-RNN, the precision of DEEB-RNN2 is improved; On the other hand, the above strategy may result in less learning of words, which are event triggers but do not appear in the training dataset."}, {"id": 84, "string": "Therefore, those sentences with such event triggers cannot be detected."}, {"id": 85, "string": "The recall of DEEB-RNN2 is thus lowered, as compared to DEEB-RNN."}, {"id": 86, "string": "Moreover, DEEB-RNN3 shows the best performance, indicating that the gold attention signals at both word and sentence levels are useful for ED."}, {"id": 87, "string": "Table 2 presents the overall performance of all methods on ACE-2005."}, {"id": 88, "string": "We can see that different versions of DEEB-RNN consistently out-perform the existing state-of-the-art methods in terms of both recall and F1-measure, while their precision is comparable to that of others."}, {"id": 89, "string": "The better performance of DEEB-RNN can be explained by the following reasons: (1) Compared with feature-based methods, including Sentencelevel, Joint Local, and representation-based methods, including JRNN, Skip-CNN and ANN-S2, our method exploits document-level information (i.e., the ED oriented document embeddings) from both word and sentence levels in a document by the supervised attention mechanism, which enhance the ability of identifying trigger words; Performance Comparison (2) Compared with feature-based methods using document-level information, such as Cross-event, PSL, our method can automatically capture event types in documents via a end-to-end Bi-RNN based model without manually designed rules; (3) Compared with representation-based methods using document-level information, such as DLRNN, our method can learn event detection oriented embeddings of documents through the hierarchical and supervised attention based Bi-RNN network."}, {"id": 90, "string": "Conclusions and Future Work In this study, we proposed a hierarchical and supervised attention based and document embedding enhanced Bi-RNN method, called DEEB-RNN, for event detection."}, {"id": 91, "string": "We explored different strategies to construct gold word-and sentence-level attentions to focus on event information."}, {"id": 92, "string": "Experiments on the ACE-2005 dataset demonstrate that DEEB-RNN achieves better performance as compared to the state-of-the-art methods in terms of both recall and F1-measure."}, {"id": 93, "string": "In this paper, we can strike a balance between sentence and document embeddings by adjusting their dimensions."}, {"id": 94, "string": "In the future, we may improve the DEEB-RNN model to automatically determine the weights of sentence and document embeddings."}], "headers": [{"section": "Introduction", "n": "1", "start": 0, "end": 25}, {"section": "The Proposed Model", "n": "2", "start": 26, "end": 29}, {"section": "The ED Oriented Document Embedding", "n": "1.", "start": 30, "end": 32}, {"section": "The EDODEL Module", "n": "2.1", "start": 33, "end": 48}, {"section": "The DEED Module", "n": "2.2", "start": 49, "end": 52}, {"section": "Joint Training of the DEEB-RNN model", "n": "2.3", "start": 53, "end": 54}, {"section": "Datasets and Settings", "n": "3.1", "start": 55, "end": 62}, {"section": "Baseline Models", "n": "3.2", "start": 63, "end": 71}, {"section": "Impacts of Different Attention Strategies", "n": "3.3", "start": 72, "end": 88}, {"section": "Performance Comparison", "n": "3.4", "start": 89, "end": 89}, {"section": "Conclusions and Future Work", "n": "4", "start": 90, "end": 94}], "figures": [{"filename": "../figure/image/1118-Figure2-1.png", "caption": "Figure 2: Examples of the gold word- and sentence-level attention without normalization. (a) Word-level attention. \u201cIndicated\u201d is a candidate trigger; (b) Sentence-level attention. The sentences in purple contain trigger words.", "page": 2, "bbox": {"x1": 79.67999999999999, "x2": 282.24, "y1": 61.44, "y2": 158.4}}, {"filename": "../figure/image/1118-Table2-1.png", "caption": "Table 2: Comparison between different methods. \u2020 indicates that the corresponding ED method uses information at both sentence and document levels.", "page": 4, "bbox": {"x1": 93.6, "x2": 268.32, "y1": 62.879999999999995, "y2": 211.2}}, {"filename": "../figure/image/1118-Figure1-1.png", "caption": "Figure 1: The schematic diagram of the DEEB-RNN model for ED at sentence level.", "page": 1, "bbox": {"x1": 74.88, "x2": 523.1999999999999, "y1": 61.44, "y2": 276.0}}, {"filename": "../figure/image/1118-Table1-1.png", "caption": "Table 1: Experimental results with different attention strategies.", "page": 3, "bbox": {"x1": 325.92, "x2": 507.35999999999996, "y1": 62.879999999999995, "y2": 137.28}}]}