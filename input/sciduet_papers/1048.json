{"title": "A Case Study on Neural Headline Generation for Editing Support", "abstract": "There have been many studies on neural headline generation models trained with a lot of (article, headline) pairs. However, there are few situations for putting such models into practical use in the real world since news articles typically already have corresponding headlines. In this paper, we describe a practical use case of neural headline generation in a news aggregator, where dozens of professional editors constantly select important news articles and manually create their headlines, which are much shorter than the original headlines. Specifically, we show how to deploy our model to an editing support tool and report the results of comparing the behavior of the editors before and after the release.", "text": [{"id": 0, "string": "Introduction A news-aggregator is a website or mobile application that aggregates a large amount of web content, e.g., online newspapers provided by different publishers."}, {"id": 1, "string": "The main purpose of such a service is to help users obtain important news out of vast amounts of information quickly and easily."}, {"id": 2, "string": "Therefore, it is critical to consider how to compactly show news, as well as what type of news to select, to improve service quality."}, {"id": 3, "string": "In fact, the news-aggregator of Yahoo!"}, {"id": 4, "string": "JAPAN 1 , the largest Japanese portal site, is supported by dozens of professional editors who constantly select important news articles and manually create their new headlines called short titles, which are much shorter than the original headline, to construct a newstopic list."}, {"id": 5, "string": "Note that we use the term \"title\" to avoid confusion with the original news headline, although they are similar concepts."}, {"id": 6, "string": "* Both authors contributed equally to this work."}, {"id": 7, "string": "1 https://www.yahoo.co.jp/ (a) List of news topics including short titles."}, {"id": 8, "string": "(b) Page of news entry including headline and lead."}, {"id": 9, "string": "Figure 1 shows screenshots of the newsaggregator of Yahoo!"}, {"id": 10, "string": "JAPAN, where the English translations of the short title, headline and lead are listed in Table 1 ."}, {"id": 11, "string": "The left figure (a) shows the list of news topics (important news articles), which includes short titles, and the right figure (b) shows the entry page of the first topic in the list, which consists of a headline and lead."}, {"id": 12, "string": "The lead is a short version of the article and can be used by users to decide whether to read the whole article."}, {"id": 13, "string": "The editors' job is to create a short title from news content including the headline and lead."}, {"id": 14, "string": "A short title has two advantages over a normal headline; one is quick understandability of the content and the other is saving display space by using a single line."}, {"id": 15, "string": "This means that short titles can increase a user's chances of reaching interesting articles."}, {"id": 16, "string": "Since the click-through rate of news articles is directly related to ad revenue, even a small improvement in short titles has a significant impact on business."}, {"id": 17, "string": "We tackle an automatic-generation task of such short titles for a news aggregator to support the Japanese English translation Short title The prime minister cannot say that there is no surmise."}, {"id": 18, "string": "Headline It cannot be said that there is no \"sontaku (surmise)\" with absolute certainty."}, {"id": 19, "string": "The prime minister Abe said about the problem of \"Kake Gakuen (Kake school)\"."}, {"id": 20, "string": "Lead Prime Minister Shinzo Abe said, in an intensive deliberation with the House of Councilors Budget Committee held on the afternoon of the 14th, as an answer to a question about whether bureaucrats surmised to the prime minister regarding the Kake suspicion, \"It is difficult to understand whether there is a sontaku (surmise)\"."}, {"id": 21, "string": "He said \"It cannot be said that there was nothing wrong,\" while explaining that \"I do not need to be obsequious\"."}, {"id": 22, "string": "An answer to Ichiro Tsukada (LDP)."}, {"id": 23, "string": "Table 1 : Short title, headline, and lead in Figure 1 (b) with English versions."}, {"id": 24, "string": "editorial process."}, {"id": 25, "string": "Our task is a variant of newsheadline generation, which has been extensively studied, as described in Section 6."}, {"id": 26, "string": "A clear difference between their task and ours is that we need to generate short titles from news content including headlines."}, {"id": 27, "string": "Thus, we formulate our task as an abstractive summarization from multiple information sources, i.e., headlines and leads, based on an encoder-decoder model (Section 2)."}, {"id": 28, "string": "There are roughly three approaches for handling multiple information sources."}, {"id": 29, "string": "The first approach is to merge all sources with some weights based on the importance of each source, which can be achieved by a weighted average of the context vectors, as in multimodal summarization (Hori et al., 2017) ."}, {"id": 30, "string": "This is the most general approach since the other two can also be regarded as special cases of the weighted average."}, {"id": 31, "string": "The second approach is to use one source as the main source and others as secondary ones."}, {"id": 32, "string": "This is effective when the main source can be clearly determined, such as query-focused summarization (Nema et al., 2017) , where the target document is main and a query is secondary."}, {"id": 33, "string": "The third approach is to find the salient components of the sources."}, {"id": 34, "string": "This is suitable when there are many sources including less informative ones (redundant sources), such as lengthydocument summarization that outputs a multisentence summary (Tan et al., 2017) , where each sentence can be regarded as one source."}, {"id": 35, "string": "We addressed an extension of the weighted average approach and compared our proposed model with a multimodal model (Hori et al., 2017) from the first approach and a query-based model (Nema et al., 2017) from the second approach, as well as the normal encoder-decoder model."}, {"id": 36, "string": "Since we have only two sources (headlines and leads), where the headline source is clearly salient for generating a short title, the third approach can be reduced to the normal encoder-decoder model."}, {"id": 37, "string": "Our contributions are as follows."}, {"id": 38, "string": "\u2022 We report on a case study of short-title generation of news articles for a news aggregator as a real-world application of neural headline generation."}, {"id": 39, "string": "This study supports previous studies based on the encoder-decoder model from a practical standpoint since most real-world news articles basically already have headlines, which means that there has been little direct application of these previous studies."}, {"id": 40, "string": "\u2022 We propose an encoder-decoder model with multiple encoders for separately encoding news headlines and leads (Section 3)."}, {"id": 41, "string": "Our comparative experiments with several baselines involving evaluations done by crowdsourcing workers showed the effectiveness of our model, especially using the \"usefulness\" measure (Section 4)."}, {"id": 42, "string": "\u2022 We describe how to deploy our model to an editing support tool and show the results of comparing the editors' behavior before and after releasing the tool (Section 5), which imply that the editors began to refer to generated titles after the release."}, {"id": 43, "string": "late the following conditional likelihood p(y | x) = T \u22121 \u220f t=1 p(y t+1 | y \u2264t , x) (1) with respect to each pair (x, y) of an input sequence x = x 1 \u00b7 \u00b7 \u00b7 x S and output sequence y = y 1 \u00b7 \u00b7 \u00b7 y T , where y \u2264t = y 1 \u00b7 \u00b7 \u00b7 y t , and maximize its mean."}, {"id": 44, "string": "The model p(y | x) in Eq."}, {"id": 45, "string": "(1) is computed by a combination of two recurrent neural networks (RNNs): an encoder and decoder."}, {"id": 46, "string": "The encoder reads an input sequence x to recognize its content, and the decoder predicts an output sequence y corresponding to the content."}, {"id": 47, "string": "More formally, an encoder calculates a hidden state h s for each element x s in a x by using the state transition function f enc of the encoder: h s = f enc (x s , h s\u22121 )."}, {"id": 48, "string": "In a similar fashion, a decoder calculates a hidden state\u0125 t for each element y t in a y by using the state transition function f dec of the decoder after setting the last hidden state of the encoder as the initial state of the decoder (\u0125 0 = h S ): h t = f dec (y t ,\u0125 t\u22121 )."}, {"id": 49, "string": "Then, a prediction of outputs for each\u0125 t is calculated using the output function g dec with an attention mechanism: p(y t+1 | y \u2264t , x) = g dec (\u0125 t , c t ), (2) where c t is a weighted average of the encoder hidden states {h 1 , \u00b7 \u00b7 \u00b7 , h S }, defined by c t = S \u2211 s=1 a t (s)h s , (3) where a t (s) represents a weight of an encoder hidden state h s with respect to a decoder hidden stat\u00ea h t ."}, {"id": 50, "string": "c t represents a soft alignment (or attention weight) to the source sequence at the target position t, so it is called a context."}, {"id": 51, "string": "Proposed Method We propose an encoder-decoder model with multiple encoders."}, {"id": 52, "string": "For simplicity, we describe our model assuming two encoders for news headlines and leads."}, {"id": 53, "string": "Let d t and d \u2032 t be contexts calculated with Eq."}, {"id": 54, "string": "(3) with the headline encoder and lead encoder, respectively."}, {"id": 55, "string": "Our model combines the two context vectors inspired by a gating mechanism in long-short term memory networks (Hochreiter and Schmidhuber, 1997) as follows: w t = \u03c3(W [d t ; d \u2032 t ;\u0125 t ]), (4) w \u2032 t = \u03c3(W \u2032 [d t ; d \u2032 t ;\u0125 t ]), (5) c t = w t \u2299 d t + w \u2032 t \u2299 d \u2032 t , (6) where function \u03c3 represents the sigmoid function, i.e., \u03c3(x) = 1/(1 + e \u2212x ), and the operator \u2299 represents the element-wise product."}, {"id": 56, "string": "Eq."}, {"id": 57, "string": "(4) calculates a gating weight w t for d t , where W represents a weight matrix for a concatenated vector [d t ; d \u2032 t ;\u0125 t ]."}, {"id": 58, "string": "Similarly, Eq."}, {"id": 59, "string": "(5) calculates a gating weight w \u2032 t for d \u2032 t ."}, {"id": 60, "string": "Eq."}, {"id": 61, "string": "(6) calculates a mixed context c t made from the two contexts, d t and d \u2032 t ."}, {"id": 62, "string": "Finally, the output function in our model is constructed by substituting c t with c t in Eq."}, {"id": 63, "string": "(2)."}, {"id": 64, "string": "Our model can be regarded as an extension of the multimodal fusion model (Hori et al., 2017) , where multiple contexts are mixed using scalar weights, i.e., c t = \u03b1d t + \u03b2d \u2032 t , where \u03b1 and \u03b2 are positive scalar weights calculated using an attention mechanism such as a t (s) in Eq."}, {"id": 65, "string": "(3)."}, {"id": 66, "string": "Our model can obtain a more sophisticated mixed context than their model since that model only takes into account which encoder to weigh at a time step, while our model adjusts weights on the element level."}, {"id": 67, "string": "Experiments Dataset We prepared a dataset extracted from the newsaggregator of Yahoo!"}, {"id": 68, "string": "JAPAN by Web crawling."}, {"id": 69, "string": "The dataset included 263K (headline, lead, short title) triples, and was split into three parts, i.e., for training (90%), validation (5%), and testing (5%)."}, {"id": 70, "string": "We preprocessed them by separating characters for training since our preliminary experiments showed that character-based training clearly performed better than word-based training."}, {"id": 71, "string": "The statistics of our dataset are as follows."}, {"id": 72, "string": "The average lengths of headlines, leads, and short titles are 24.87, 128.49, and 13.05 Japanese characters, respectively."}, {"id": 73, "string": "The dictionary sizes (for characters) of headlines, leads, and short titles are 3618, 4226, and 3156, respectively."}, {"id": 74, "string": "Each news article has only one short title created by a professional editor."}, {"id": 75, "string": "The percentage of short titles equal to their headlines is only 0.13%, while the percentage of extractively solvable instances, in which the characters in each short title are completely matched by those in the corresponding headline, was about 20%."}, {"id": 76, "string": "However, the average edit distance (Levenshtein, 1966 ) between short titles and headlines was 23.74."}, {"id": 77, "string": "This means that short titles cannot be easily created from headlines."}, {"id": 78, "string": "Training We implemented our model on the OpenNMT 2 toolkit."}, {"id": 79, "string": "We used a convolutional neural network (CNN) (Kim, 2014) , instead of an RNN, to construct the lead encoder since leads are longer than headlines and require much more computational time."}, {"id": 80, "string": "Since the CNN encoder outputs all hidden states for an input sequence in the same format as the RNN encoder, we can easily apply these states to Eq."}, {"id": 81, "string": "(3)."}, {"id": 82, "string": "Our headline encoder still remains as an RNN (i.e., bidirectional LSTM) for fair comparison with the default implementation."}, {"id": 83, "string": "We used a stochastic gradient descent algorithm with Nesterov momentum (Nesterov, 1983) as an optimizer, after initializing parameters by uniform sampling on (\u22120.1, 0.1)."}, {"id": 84, "string": "Table 2 lists the details of the hyper-parameter settings in our experiment."}, {"id": 85, "string": "Other settings were basically the same as the default implementation of OpenNMT."}, {"id": 86, "string": "Evaluation We conducted two crowdsourcing tasks to separately measure readability and usefulness."}, {"id": 87, "string": "The readability task asked ten workers how readable each short title was on a four-point scale (higher is better), while the usefulness task asked them how useful the short title was compared to the corresponding article."}, {"id": 88, "string": "The score of each generated short title was calculated by averaging the scores collected from the ten workers."}, {"id": 89, "string": "Compared Models We prepared four models, our model GateFusion and three baselines MultiModal, QueryBased, and OpenNMT, listed below."}, {"id": 90, "string": "We implemented the fusion mechanisms of MultiModal and 2 https://github.com/OpenNMT/OpenNMT-py Table 3 : Mean scores of readability (r), usefulness (u), and their average r+u 2 based on crowdsourcing."}, {"id": 91, "string": "The \" \u2020\" mark shows a statistical significance from all three baselines OpenNMT, MultiModal, and QueryBased on a one-tailed, paired t-test (p < 0.01)."}, {"id": 92, "string": "QueryBased on OpenNMT using an RNN encoder for headlines and CNN encoder for leads (see Appendix A for detailed definitions)."}, {"id": 93, "string": "\u2022 GateFusion: Our model with a gating mechanism described in Section 3."}, {"id": 94, "string": "This is a fusion based on vector weights."}, {"id": 95, "string": "\u2022 MultiModal: A multimodal model proposed by (Hori et al., 2017) , which can handle multimodal information such as image and audio as well as text by using separate encoders."}, {"id": 96, "string": "The model combines contexts obtained from the encoders via an attention mechanism such as a t (s) in Eq."}, {"id": 97, "string": "(3)."}, {"id": 98, "string": "This is a fusion based on scalar weights."}, {"id": 99, "string": "\u2022 QueryBased: A query-based model proposed by (Nema et al., 2017) , which can finetune the attention on a document by using a query for query-focused summarization."}, {"id": 100, "string": "We regard a headline as a document and a lead as a query since the headline is more similar to its short title."}, {"id": 101, "string": "Specifically, the model finetunes an attention weight a t (s) for calculating a headline context d t by using a pre-computed lead context d \u2032 t ."}, {"id": 102, "string": "This is a fusion based on cascade connection."}, {"id": 103, "string": "\u2022 OpenNMT: An encoder-decoder model with a single encoder implemented in OpenNMT, whose input is a headline only, because a variant using a lead did not perform better than this setting."}, {"id": 104, "string": "Table 3 lists the results from the crowdsourcing tasks for readability and usefulness (see Appendix B for the details of these scores)."}, {"id": 105, "string": "Editor and Prefix in the top block of rows show the results of correct short titles created by editors and a naive model using the first 13.5 Japanese characters 3 , respectively."}, {"id": 106, "string": "The middle and bottom blocks represent the three baselines and our models, respectively."}, {"id": 107, "string": "We explain our hybrid model HybridFusion later."}, {"id": 108, "string": "Each model was prepared as an ensemble of ten models by random initialization, aiming for robust performance."}, {"id": 109, "string": "Our GateFusion clearly performed better than the three baselines regarding usefulness and interestingly outperformed even Editor."}, {"id": 110, "string": "This implies that GateFusion tends to aggressively copy elements from source sequences."}, {"id": 111, "string": "However, this seemed to result in complicated expressions; thus, GateFusion performed the worst with respect to readability."}, {"id": 112, "string": "To overcome this weakness, we developed a hybrid model HybridFusion that consists of GateFusion and another fusion model QueryBased, which performed relatively well in terms of readability."}, {"id": 113, "string": "The results indicate that HybridFusion performed the best regarding readability and usefulness."}, {"id": 114, "string": "It can be considered that QueryBased helps GateFusion generate headline-style outputs since QueryBased mainly uses the headline source."}, {"id": 115, "string": "Table 4 lists output examples generated by the best model OpenNMT from the three baselines and our best model HybridFusion (see Appendix C for more examples)."}, {"id": 116, "string": "In this case, the difference between OpenNMT and HybridFusion is easily comprehensible."}, {"id": 117, "string": "The former selected \" (evolution)\", and the latter selected \" (Darvish)\" from the headline."}, {"id": 118, "string": "In Japanese headlines, the last word tends to be important, so using the last word is basically a good strategy."}, {"id": 119, "string": "However, the lead indicates that \"Darvish\" is more important than \"evolution\" (actually, there is no word \"evolution\" in the lead); thus, HybridFusion was able to correctly select the long name \"Darvish\" and abbreviate it to \" (Dar)\"."}, {"id": 120, "string": "In addition, it forcibly changed the style to the short title's style by putting the name into the forefront to easily get users' attention."}, {"id": 121, "string": "This suggests that our neural-headline-generation model HybridFusion can successfully work even in this real-world application."}, {"id": 122, "string": "Results Deployment to Editing Support Tool We deployed our short-title-generation model to an editing support tool in collaboration with the 3 13.5 is the limit in the news-aggregator, where space, numbers, and alphabet characters are counted as 0.5."}, {"id": 123, "string": "Figure 2 : Screenshot of editing support tool displaying generated candidates for creating a short title."}, {"id": 124, "string": "news service, as shown in Figure 2 ."}, {"id": 125, "string": "In the tool, when an editor enters the URL of an article, the tool can automatically fetch the headline and lead of the article and display up to five candidates next to the edit form of a short title, as shown in the dotted box in the figure."}, {"id": 126, "string": "These candidates are hypotheses (with high probabilities) generated by the beam search based on the model."}, {"id": 127, "string": "Then, the editor can effectively create a short title by referring to the generated candidates."}, {"id": 128, "string": "This supporting feature is expected to be useful especially for inexperienced editors since the quality of short titles is heavily dependent on editors' experience."}, {"id": 129, "string": "From now on, we briefly describe three features of the tool to improve its usability when displaying candidates: cutoff of unpromising candidates, skipping redundant candidates, and highlighting unknown characters."}, {"id": 130, "string": "After that, we discuss the effect of the deployment analyzing user behavior before and after releasing the tool."}, {"id": 131, "string": "Cutoff of Unpromising Candidates The quality of displayed candidates is one of the main factors that affect the usability of the tool."}, {"id": 132, "string": "If the tool frequently displays unpromising candidates, editors will gradually start ignoring them."}, {"id": 133, "string": "Therefore, we cutoff unpromising candidates whose perplexity scores are higher than a certain threshold, where the perplexity score of a candidate is calculated by the inverse of the geometric mean of the generation probabilities for all characters in the candidate."}, {"id": 134, "string": "We set the threshold considering the results of the editors' manual evaluation, where they checked if each candidate was acceptable or not."}, {"id": 135, "string": "Specifically, we used 1.47 (=1/0.68) as the threshold, which means that the (geometric) mean character likelihood in the candidate should be higher than 0.68."}, {"id": 136, "string": "If all candidates are judged as unpromising, the tool displays a message like \"No promising candidates.\""}, {"id": 137, "string": "Skipping Redundant Candidates The purpose of the tool is to give editors some new ideas for creating short titles, so it is not useful to display redundant candidates similar to others."}, {"id": 138, "string": "Therefore, we skip candidates whose edit distance (Levenshtein, 1966) to the other candidates is lower than a threshold when selecting hypotheses in descending order of probability."}, {"id": 139, "string": "Formally, the edit distance between two texts is defined as the minimum number of single-character edits (insertions, deletions, or substitutions) required to change one text into the other."}, {"id": 140, "string": "We set the threshold to 2 so as to restrict variations of Japanese particles as there are many particles with a similar meaning in Japanese 4 , e.g., \" (ha)\" and \" (ga)\"."}, {"id": 141, "string": "Although we used a unit cost for the edit distance, we can adjust the cost of each edit operation so that the tool can ignore variations of prepositions if we want to use English texts."}, {"id": 142, "string": "Highlighting Unknown Characters One difficulty of neural models is that there is a possibility of generating incorrect or fake titles, which do not correspond to the article."}, {"id": 143, "string": "This is a serious issue for news editing support since displayed candidates can mislead editors."}, {"id": 144, "string": "For example, if the tool displays \" (Fujinami)\" for the news about \" (Fujinami)\", where they are different names with the same pronunciation, editors might choose the incorrect one."}, {"id": 145, "string": "As a simple solution, we highlighted unknown characters that do not appear in both headline and lead in red."}, {"id": 146, "string": "In Figure 2 , two phrases (\"B\" and \" \") are highlighted since they do not appear in the headline and lead."}, {"id": 147, "string": "When a candidate includes highlighted characters, editors can carefully check if the candidate is semantically correct."}, {"id": 148, "string": "Note that we did not exclude candidates with unknown characters so that the model can aggressively generate paraphrases and abbreviations."}, {"id": 149, "string": "For example, the tool ROUGE-L (\u00b1 SE) # articles Before 52.71% (\u00b1 0.56) 1773 After 57.65% (\u00b1 0.53) 1959 Table 5 : Sequence matching rates (ROUGE-L) of editors' titles and generated titles, which are averaged over articles over three weeks before/after releasing tool."}, {"id": 150, "string": "suggests \" B(Soft B.)\""}, {"id": 151, "string": "as an abbreviation of \" (Softbank)\" in the figure."}, {"id": 152, "string": "Effect of Deployment To investigate the effect of the deployment, we compared the sequence matching rates between editors' correct titles and generated candidates before and after releasing the tool."}, {"id": 153, "string": "The sequence matching rate is basically calculated by ROUGE-L (Lin, 2004) , which is defined as the rate of the length of the longest common subsequence between two sequences, i.e., a correct title and a generated candidate."}, {"id": 154, "string": "Because we have multiple candidates for each article, we calculate the sequence matching rate as the maximum of their ROUGE-L scores, assuming that editors may refer to the most promising candidate."}, {"id": 155, "string": "Note that the candidates were filtered by the aforementioned features, so we omitted a few articles without candidates."}, {"id": 156, "string": "Table 5 shows the results of the sequence matching rates averaged over the articles over three weeks before and after releasing the tool."}, {"id": 157, "string": "The results indicate that the ROUGE-L score increased by about 5 percentage points after the release."}, {"id": 158, "string": "This implies that editors created their titles by referring to the displayed candidates to some extent."}, {"id": 159, "string": "In fact, the ratio of the exact matched titles (ROUGE-L = 100%) in all articles (before/after the release) increased after the release by a factor of 1.62(i.e., from 3.78% to 6.13%)."}, {"id": 160, "string": "Similarly, the ratio of the 80% matched titles (ROUGE-L \u2265 80%) also increased by a factor of 1.32 (i.e., from 14.04% to 18.53%)."}, {"id": 161, "string": "This suggests that professional editors obtained new ideas from generated titles of the tool."}, {"id": 162, "string": "Related Work We briefly review related studies from three aspects: news headline generation, editing support, and application of headline generation."}, {"id": 163, "string": "In summary, our work is the first attempt to deploy a neural news-headline-generation model to a realworld application, i.e., news editing support tool."}, {"id": 164, "string": "News-headline-generation tasks have been extensively studied since early times (Wang et al., 2005; Soricut and Marcu, 2006; Woodsend et al., 2010; Alfonseca et al., 2013; Sun et al., 2015; Colmenares et al., 2015) ."}, {"id": 165, "string": "In this line of research, Rush et al."}, {"id": 166, "string": "(2015) proposed a neural model to generate news headlines and released a benchmark dataset for their task, and consequently this task has recently received increasing attention (Chopra et al., 2016; Takase et al., 2016; Kiyono et al., 2017; Zhou et al., 2017; Ayana et al., 2017; Raffel et al., 2017; Cao et al., 2018; Kobayashi, 2018) ."}, {"id": 167, "string": "However, their approaches were basically based on the encoderdecoder model, which is trained with a lot of (article, headline) pairs."}, {"id": 168, "string": "This means that there are few situations for putting their models into the real world because news articles typically already have corresponding headlines, and most editors create a headline before its content (according to a senior journalist)."}, {"id": 169, "string": "Therefore, our work can strongly support their approaches from a practical perspective."}, {"id": 170, "string": "Considering technologies used for editing support, there have been many studies for various purposes, such as spelling error correction (Farra et al., 2014; Hasan et al., 2015; Etoori et al., 2018) , grammatical error correction (Dahlmeier and Ng, 2012; Susanto et al., 2014; Choshen and Abend, 2018) , fact checking (Baly et al., 2018; Thorne and Vlachos, 2018; Lee et al., 2018) , fluency evaluation (Vadlapudi and Katragadda, 2010; Heilman et al., 2014; Kann et al., 2018) , and so on."}, {"id": 171, "string": "However, when we consider their studies on our task, they are only used after editing (writing a draft)."}, {"id": 172, "string": "On the other hand, the purpose of our tool is different from theirs since our tool can support editors before or during editing."}, {"id": 173, "string": "The usage of (interactive) machine translation systems (Denkowski et al., 2014; Gonz\u00e1lez-Rubio et al., 2016; Wuebker et al., 2016; Ye et al., 2016; Takeno et al., 2017) for supporting manual post-editing are similar to our purpose, but their task is completely different from ours."}, {"id": 174, "string": "In other words, their task is a translation without information loss, whereas our task is a summarization that requires information compression."}, {"id": 175, "string": "We believe that a case study on summarization is still important for the summarization community."}, {"id": 176, "string": "There have been several studies reporting case studies on headline generation for different real services: (a) question headlines on question answering service (Higurashi et al., 2018) , (b) product headlines on e-commerce service (Wang et al., 2018) , and (c) headlines for product curation pages Camargo de Souza et al., 2018) ."}, {"id": 177, "string": "The first two (a) and (b) are extractive approaches, and the last one (c) is an abstractive approach, where the input is a set of slot/value pairs, such as \"color/white.\""}, {"id": 178, "string": "That is, our task is more difficult to use in the real-world."}, {"id": 179, "string": "In addition, application to news services tends to be sensitive since news articles contain serious contents such as incidents, accidents, and disasters."}, {"id": 180, "string": "Thus, our work should be valuable as a rare case study applying a neural model to such a news service."}, {"id": 181, "string": "Conclusion We addressed short-title generation from news articles for a news aggregator to support the editorial process."}, {"id": 182, "string": "We proposed an encoder-decoder model with multiple encoders for separately encoding multiple information sources, i.e., news headlines and leads."}, {"id": 183, "string": "Comparative experiments using crowdsourcing showed that our hybrid model performed better than the baselines, especially using the usefulness measure."}, {"id": 184, "string": "We deployed our model to an editing support tool and empirically confirmed that professional editors began to refer to the generated titles after the release."}, {"id": 185, "string": "Future research will include verifying how much our headline generation model can affect practical performance indicators, such as click-through rate."}, {"id": 186, "string": "In this case, we need to develop a much safer model since our model sometimes yields erroneous outputs or fake news titles, which cannot be directly used in the commercial service."}], "headers": [{"section": "Introduction", "n": "1", "start": 0, "end": 50}, {"section": "Proposed Method", "n": "3", "start": 51, "end": 66}, {"section": "Dataset", "n": "4.1", "start": 67, "end": 77}, {"section": "Training", "n": "4.2", "start": 78, "end": 85}, {"section": "Evaluation", "n": "4.3", "start": 86, "end": 88}, {"section": "Compared Models", "n": "4.4", "start": 89, "end": 121}, {"section": "Deployment to Editing Support Tool", "n": "5", "start": 122, "end": 130}, {"section": "Cutoff of Unpromising Candidates", "n": "5.1", "start": 131, "end": 136}, {"section": "Skipping Redundant Candidates", "n": "5.2", "start": 137, "end": 141}, {"section": "Highlighting Unknown Characters", "n": "5.3", "start": 142, "end": 151}, {"section": "Effect of Deployment", "n": "5.4", "start": 152, "end": 161}, {"section": "Related Work", "n": "6", "start": 162, "end": 180}, {"section": "Conclusion", "n": "7", "start": 181, "end": 186}], "figures": []}