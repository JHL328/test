{"title": "Confidence Modeling for Neural Semantic Parsing", "abstract": "In this work we focus on confidence modeling for neural semantic parsers which are built upon sequence-to-sequence models. We outline three major causes of uncertainty, and design various metrics to quantify these factors. These metrics are then used to estimate confidence scores that indicate whether model predictions are likely to be correct. Beyond confidence estimation, we identify which parts of the input contribute to uncertain predictions allowing users to interpret their model, and verify or refine its input. Experimental results show that our confidence model significantly outperforms a widely used method that relies on posterior probability, and improves the quality of interpretation compared to simply relying on attention scores.", "text": [{"id": 0, "string": "Introduction Semantic parsing aims to map natural language text to a formal meaning representation (e.g., logical forms or SQL queries)."}, {"id": 1, "string": "The neural sequenceto-sequence architecture Bahdanau et al., 2015) has been widely adopted in a variety of natural language processing tasks, and semantic parsing is no exception."}, {"id": 2, "string": "However, despite achieving promising results (Dong and Lapata, 2016; Jia and Liang, 2016; , neural semantic parsers remain difficult to interpret, acting in most cases as a black box, not providing any information about what made them arrive at a particular decision."}, {"id": 3, "string": "In this work, we explore ways to estimate and interpret the * Work carried out during an internship at Microsoft Research."}, {"id": 4, "string": "model's confidence in its predictions, which we argue can provide users with immediate and meaningful feedback regarding uncertain outputs."}, {"id": 5, "string": "An explicit framework for confidence modeling would benefit the development cycle of neural semantic parsers which, contrary to more traditional methods, do not make use of lexicons or templates and as a result the sources of errors and inconsistencies are difficult to trace."}, {"id": 6, "string": "Moreover, from the perspective of application, semantic parsing is often used to build natural language interfaces, such as dialogue systems."}, {"id": 7, "string": "In this case it is important to know whether the system understands the input queries with high confidence in order to make decisions more reliably."}, {"id": 8, "string": "For example, knowing that some of the predictions are uncertain would allow the system to generate clarification questions, prompting users to verify the results before triggering unwanted actions."}, {"id": 9, "string": "In addition, the training data used for semantic parsing can be small and noisy, and as a result, models do indeed produce uncertain outputs, which we would like our framework to identify."}, {"id": 10, "string": "A widely-used confidence scoring method is based on posterior probabilities p (y|x) where x is the input and y the model's prediction."}, {"id": 11, "string": "For a linear model, this method makes sense: as more positive evidence is gathered, the score becomes larger."}, {"id": 12, "string": "Neural models, in contrast, learn a complicated function that often overfits the training data."}, {"id": 13, "string": "Posterior probability is effective when making decisions about model output, but is no longer a good indicator of confidence due in part to the nonlinearity of neural networks (Johansen and Socher, 2017) ."}, {"id": 14, "string": "This observation motivates us to develop a confidence modeling framework for sequenceto-sequence models."}, {"id": 15, "string": "We categorize the causes of uncertainty into three types, namely model uncertainty, data uncertainty, and input uncertainty and design different metrics to characterize them."}, {"id": 16, "string": "We compute these confidence metrics for a given prediction and use them as features in a regression model which is trained on held-out data to fit prediction F1 scores."}, {"id": 17, "string": "At test time, the regression model's outputs are used as confidence scores."}, {"id": 18, "string": "Our approach does not interfere with the training of the model, and can be thus applied to various architectures, without sacrificing test accuracy."}, {"id": 19, "string": "Furthermore, we propose a method based on backpropagation which allows to interpret model behavior by identifying which parts of the input contribute to uncertain predictions."}, {"id": 20, "string": "Experimental results on two semantic parsing datasets (IFTTT, Quirk et al."}, {"id": 21, "string": "2015; and DJANGO, Oda et al."}, {"id": 22, "string": "2015) show that our model is superior to a method based on posterior probability."}, {"id": 23, "string": "We also demonstrate that thresholding confidence scores achieves a good trade-off between coverage and accuracy."}, {"id": 24, "string": "Moreover, the proposed uncertainty backpropagation method yields results which are qualitatively more interpretable compared to those based on attention scores."}, {"id": 25, "string": "Related Work Confidence Estimation Confidence estimation has been studied in the context of a few NLP tasks, such as statistical machine translation (Blatz et al., 2004; Ueffing and Ney, 2005; Soricut and Echihabi, 2010) , and question answering (Gondek et al., 2012) ."}, {"id": 26, "string": "To the best of our knowledge, confidence modeling for semantic parsing remains largely unexplored."}, {"id": 27, "string": "A common scheme for modeling uncertainty in neural networks is to place distributions over the network's weights (Denker and Lecun, 1991; MacKay, 1992; Neal, 1996; Blundell et al., 2015; Gan et al., 2017) ."}, {"id": 28, "string": "But the resulting models often contain more parameters, and the training process has to be accordingly changed, which makes these approaches difficult to work with."}, {"id": 29, "string": "Gal and Ghahramani (2016) develop a theoretical framework which shows that the use of dropout in neural networks can be interpreted as a Bayesian approximation of Gaussian Process."}, {"id": 30, "string": "We adapt their framework so as to represent uncertainty in the encoder-decoder architectures, and extend it by adding Gaussian noise to weights."}, {"id": 31, "string": "Semantic Parsing Various methods have been developed to learn a semantic parser from natural language descriptions paired with meaning representations (Tang and Mooney, 2000; Zettlemoyer and Collins, 2007; Lu et al., 2008; Kwiatkowski et al., 2011; Andreas et al., 2013; Zhao and Huang, 2015) ."}, {"id": 32, "string": "More recently, a few sequence-to-sequence models have been proposed for semantic parsing (Dong and Lapata, 2016; Jia and Liang, 2016; and shown to perform competitively whilst eschewing the use of templates or manually designed features."}, {"id": 33, "string": "There have been several efforts to improve these models including the use of a tree decoder (Dong and Lapata, 2016) , data augmentation (Jia and Liang, 2016; , the use of a grammar model (Xiao et al., 2016; Rabinovich et al., 2017; Yin and Neubig, 2017; , coarse-tofine decoding (Dong and Lapata, 2018) , network sharing (Susanto and Lu, 2017; Herzig and Berant, 2017) , user feedback (Iyer et al., 2017) , and transfer learning (Fan et al., 2017) ."}, {"id": 34, "string": "Current semantic parsers will by default generate some output for a given input even if this is just a random guess."}, {"id": 35, "string": "System results can thus be somewhat unexpected inadvertently affecting user experience."}, {"id": 36, "string": "Our goal is to mitigate these issues with a confidence scoring model that can estimate how likely the prediction is correct."}, {"id": 37, "string": "Neural Semantic Parsing Model In the following section we describe the neural semantic parsing model (Dong and Lapata, 2016; Jia and Liang, 2016; we assume throughout this paper."}, {"id": 38, "string": "The model is built upon the sequence-to-sequence architecture and is illustrated in Figure 1 ."}, {"id": 39, "string": "An encoder is used to encode natural language input q = q 1 \u00b7 \u00b7 \u00b7 q |q| into a vector representation, and a decoder learns to generate a logical form representation of its meaning a = a 1 \u00b7 \u00b7 \u00b7 a |a| conditioned on the encoding vectors."}, {"id": 40, "string": "The encoder and decoder are two different recurrent neural networks with long short-term memory units (LSTMs; Hochreiter and Schmidhuber 1997) which process tokens sequentially."}, {"id": 41, "string": "The probability of generating the whole sequence p (a|q) is factorized as: p (a|q) = |a| t=1 p (a t |a <t , q) (1) where a <t = a 1 \u00b7 \u00b7 \u00b7 a t\u22121 ."}, {"id": 42, "string": "Let e t \u2208 R n denote the hidden vector of the encoder at time step t. It is computed via e t = f LSTM (e t\u22121 , q t ), where f LSTM refers to the LSTM unit, and q t \u2208 R n is the word embedding \u2026 \u2026 \u2026 <s> \u2026 \u2026 \u2026 i) iii) i) ii) iv) Figure 1: We use dropout as approximate Bayesian inference to obtain model uncertainty."}, {"id": 43, "string": "The dropout layers are applied to i) token vectors; ii) the encoder's output vectors; iii) bridge vectors; and iv) decoding vectors."}, {"id": 44, "string": "of q t ."}, {"id": 45, "string": "Once the tokens of the input sequence are encoded into vectors, e |q| is used to initialize the hidden states of the first time step in the decoder."}, {"id": 46, "string": "Similarly, the hidden vector of the decoder at time step t is computed by d t = f LSTM (d t\u22121 , a t\u22121 ), where a t\u22121 \u2208 R n is the word vector of the previously predicted token."}, {"id": 47, "string": "Additionally, we use an attention mechanism (Luong et al., 2015a) to utilize relevant encoder-side context."}, {"id": 48, "string": "For the current time step t of the decoder, we compute its attention score with the k-th hidden state in the encoder as: r t,k \u221d exp{d t \u00b7 e k } (2) where |q| j=1 r t,j = 1."}, {"id": 49, "string": "The probability of generating a t is computed via: c t = |q| k=1 r t,k e k (3) d att t = tanh (W 1 d t + W 2 c t ) (4) p (a t |a <t , q) = softmax at W o d att t (5) where W 1 , W 2 \u2208 R n\u00d7n and W o \u2208 R |Va|\u00d7n are three parameter matrices."}, {"id": 50, "string": "The training objective is to maximize the likelihood of the generated meaning representation a given input q, i.e., maximize (q,a)\u2208D log p (a|q), where D represents training pairs."}, {"id": 51, "string": "At test time, the model's prediction for input q is obtained vi\u00e2 a = arg max a p (a |q), where a represents candidate outputs."}, {"id": 52, "string": "Because p (a|q) is factorized as shown in Equation (1), we can use beam search to generate tokens one by one rather than iterating over all possible results."}, {"id": 53, "string": "Confidence Estimation Given input q and its predicted meaning representation a, the confidence model estimates Algorithm 1 Dropout Perturbation Input: q, a: Input and its prediction M: Model parameters 1: for i \u2190 1, \u00b7 \u00b7 \u00b7 , F do 2:M i \u2190 Apply dropout layers to M Figure 1 3: Run forward pass and computep(a|q;M i ) 4: Compute variance of {p(a|q;M i )} F i=1 Equation (6) score s (q, a) \u2208 (0, 1)."}, {"id": 54, "string": "A large score indicates the model is confident that its prediction is correct."}, {"id": 55, "string": "In order to gauge confidence, we need to estimate \"what we do not know\"."}, {"id": 56, "string": "To this end, we identify three causes of uncertainty, and design various metrics characterizing each one of them."}, {"id": 57, "string": "We then feed these metrics into a regression model in order to predict s (q, a)."}, {"id": 58, "string": "Model Uncertainty The model's parameters or structures contain uncertainty, which makes the model less confident about the values of p (a|q)."}, {"id": 59, "string": "For example, noise in the training data and the stochastic learning algorithm itself can result in model uncertainty."}, {"id": 60, "string": "We describe metrics for capturing uncertainty below: Dropout Perturbation Our first metric uses dropout (Srivastava et al., 2014) as approximate Bayesian inference to estimate model uncertainty (Gal and Ghahramani, 2016) ."}, {"id": 61, "string": "Dropout is a widely used regularization technique during training, which relieves overfitting by randomly masking some input neurons to zero according to a Bernoulli distribution."}, {"id": 62, "string": "In our work, we use dropout at test time, instead."}, {"id": 63, "string": "As shown in Algorithm 1, we perform F forward passes through the network, and collect the results {p(a|q; M i )} F i=1 whereM i represents the perturbed parameters."}, {"id": 64, "string": "Then, the uncertainty metric is computed by the variance of results."}, {"id": 65, "string": "We define the metric on the sequence level as: var{p(a|q;M i )} F i=1 ."}, {"id": 66, "string": "(6) In addition, we compute uncertainty u at at the token-level a t via: u at = var{p(a t |a <t , q;M i )} F i=1 (7) wherep(a t |a <t , q;M i ) is the probability of generating token a t (Equation (5) ) using perturbed modelM i ."}, {"id": 67, "string": "We operationalize tokenlevel uncertainty in two ways, as the average score avg{u at } |a| t=1 and the maximum score max{u at } |a| t=1 (since the uncertainty of a sequence is often determined by the most uncertain token)."}, {"id": 68, "string": "As shown in Figure 1 , we add dropout layers in i) the word vectors of the encoder and decoder q t , a t ; ii) the output vectors of the encoder e t ; iii) bridge vectors e |q| used to initialize the hidden states of the first time step in the decoder; and iv) decoding vectors d att t (Equation (4) )."}, {"id": 69, "string": "Gaussian Noise Standard dropout can be viewed as applying noise sampled from a Bernoulli distribution to the network parameters."}, {"id": 70, "string": "We instead use Gaussian noise, and apply the metrics in the same way discussed above."}, {"id": 71, "string": "Let v denote a vector perturbed by noise, and g a vector sampled from the Gaussian distribution N (0, \u03c3 2 )."}, {"id": 72, "string": "We usev = v + g andv = v + v g as two noise injection methods."}, {"id": 73, "string": "Intuitively, if the model is more confident in an example, it should be more robust to perturbations."}, {"id": 74, "string": "Posterior Probability Our last class of metrics is based on posterior probability."}, {"id": 75, "string": "We use the log probability log p(a|q) as a sequence-level metric."}, {"id": 76, "string": "The token-level metric min{p(a t |a <t , q)} |a| t=1 can identify the most uncertain predicted token."}, {"id": 77, "string": "The perplexity per token \u2212 1 |a| |a| t=1 log p (a t |a <t , q) is also employed."}, {"id": 78, "string": "Data Uncertainty The coverage of training data also affects the uncertainty of predictions."}, {"id": 79, "string": "If the input q does not match the training distribution or contains unknown words, it is difficult to predict p (a|q) reliably."}, {"id": 80, "string": "We define two metrics: Probability of Input We train a language model on the training data, and use it to estimate the probability of input p(q|D) where D represents the training data."}, {"id": 81, "string": "Number of Unknown Tokens Tokens that do not appear in the training data harm robustness, and lead to uncertainty."}, {"id": 82, "string": "So, we use the number of unknown tokens in the input q as a metric."}, {"id": 83, "string": "Input Uncertainty Even if the model can estimate p (a|q) reliably, the input itself may be ambiguous."}, {"id": 84, "string": "For instance, the input the flight is at 9 o'clock can be interpreted as either flight time(9am) or flight time(9pm)."}, {"id": 85, "string": "Selecting between these predictions is difficult, especially if they are both highly likely."}, {"id": 86, "string": "We use the following metrics to measure uncertainty caused by ambiguous inputs."}, {"id": 87, "string": "Variance of Top Candidates We use the variance of the probability of the top candidates to indicate whether these are similar."}, {"id": 88, "string": "The sequencelevel metric is computed by: var{p(a i |q)} K i=1 where a 1 ."}, {"id": 89, "string": "."}, {"id": 90, "string": "."}, {"id": 91, "string": "a K are the K-best predictions obtained by the beam search during inference (Section 3)."}, {"id": 92, "string": "Entropy of Decoding The sequence-level entropy of the decoding process is computed via: H[a|q] = \u2212 a p(a |q) log p(a |q) which we approximate by Monte Carlo sampling rather than iterating over all candidate predictions."}, {"id": 93, "string": "The token-level metrics of decoding entropy are computed by avg{H[a t |a <t , q]} |a| t=1 and max{H[a t |a <t , q]} |a| t=1 ."}, {"id": 94, "string": "Confidence Scoring The sentence-and token-level confidence metrics defined in Section 4 are fed into a gradient tree boosting model (Chen and Guestrin, 2016) in order to predict the overall confidence score s (q, a)."}, {"id": 95, "string": "The model is wrapped with a logistic function so that confidence scores are in the range of (0, 1)."}, {"id": 96, "string": "Because the confidence score indicates whether the prediction is likely to be correct, we can use the prediction's F1 (see Section 6.2) as target value."}, {"id": 97, "string": "The training loss is defined as: (q,a)\u2208D ln(1+e \u2212\u015d(q,a) ) yq,a + ln(1+e\u015d (q,a) ) (1\u2212yq,a) where D represents the data, y q,a is the target F1 score, and\u015d(q, a) the predicted confidence score."}, {"id": 98, "string": "We refer readers to Chen and Guestrin (2016) for mathematical details of how the gradient tree boosting model is trained."}, {"id": 99, "string": "Notice that we learn the confidence scoring model on the held-out set (rather than on the training data of the semantic parser) to avoid overfitting."}, {"id": 100, "string": "Uncertainty Interpretation Confidence scores are useful in so far they can be traced back to the inputs causing the uncertainty in the first place."}, {"id": 101, "string": "For semantic parsing, identifying = v c 1 m u c 1 + v c 2 m u c 2 ."}, {"id": 102, "string": "The score u m is then redistributed to its parent neurons p 1 and p 2 , which satisfies v m p 1 + v m p 2 = 1. which input words contribute to uncertainty would be of value, e.g., these could be treated explicitly as special cases or refined if they represent noise."}, {"id": 103, "string": "In this section, we introduce an algorithm that backpropagates token-level uncertainty scores (see Equation (7) ) from predictions to input tokens, following the ideas of Bach et al."}, {"id": 104, "string": "(2015) and Zhang et al."}, {"id": 105, "string": "(2016) ."}, {"id": 106, "string": "Let u m denote neuron m's uncertainty score, which indicates the degree to which it contributes to uncertainty."}, {"id": 107, "string": "As shown in Figure 2 , u m is computed by the summation of the scores backpropagated from its child neurons: u m = c\u2208Child(m) v c m u c where Child(m) is the set of m's child neurons, and the non-negative contribution ratio v c m indicates how much we backpropagate u c to neuron m. Intuitively, if neuron m contributes more to c's value, ratio v c m should be larger."}, {"id": 108, "string": "After obtaining score u m , we redistribute it to its parent neurons in the same way."}, {"id": 109, "string": "Contribution ratios from m to its parent neurons are normalized to 1: p\u2208Parent(m) v m p = 1 where Parent(m) is the set of m's parent neurons."}, {"id": 110, "string": "Given the above constraints, we now define different backpropagation rules for the operators used in neural networks."}, {"id": 111, "string": "We first describe the rules used for fully-connected layers."}, {"id": 112, "string": "Let x denote the input."}, {"id": 113, "string": "The output is computed by z = \u03c3(Wx+b), where \u03c3 is a nonlinear function, W \u2208 R |z| * |x| is the weight matrix, b \u2208 R |z| is the bias, and neuron z i is computed via z i = \u03c3( |x| j=1 W i,j x j + b i )."}, {"id": 114, "string": "Neuron x k 's uncertainty score u x k is gath-Algorithm 2 Uncertainty Interpretation Input: q, a: Input and its prediction Output: {\u00fbq t } |q| t=1 : Interpretation scores for input tokens Function: TokenUnc: Get token-level uncertainty 1: Get token-level uncertainty for predicted tokens 2: {ua t } |a| t=1 \u2190 TokenUnc(q, a) 3: Initialize uncertainty scores for backpropagation 4: for t \u2190 1, \u00b7 \u00b7 \u00b7 , |a| do 5: Decoder classifier's output neuron \u2190 ua t 6: Run backpropagation 7: for m \u2190 neuron in backward topological order do 8: Gather scores from child neurons 9: um \u2190 c\u2208Child(m) v c m uc 10: Summarize scores for input words 11: for t \u2190 1, \u00b7 \u00b7 \u00b7 , |q| do 12: uq t \u2190 c\u2208q t uc 13: {\u00fbq t } |q| t=1 \u2190 normalize {uq t } |q| t=1 ered from the next layer: u x k = |z| i=1 v z i x k u z i = |z| i=1 |W i,k x k | |x| j=1 |W i,j x j | u z i ignoring the nonlinear function \u03c3 and the bias b."}, {"id": 115, "string": "The ratio v z i x k is proportional to the contribution of x k to the value of z i ."}, {"id": 116, "string": "We define backpropagation rules for elementwise vector operators."}, {"id": 117, "string": "For z = x \u00b1 y, these are: u x k = |x k | |x k |+|y k | u z k u y k = |y k | |x k |+|y k | u z k where the contribution ratios v z k x k and v z k y k are determined by |x k | and |y k |."}, {"id": 118, "string": "For multiplication, the contribution of two elements in 1 3 * 3 should be the same."}, {"id": 119, "string": "So, the propagation rules for z = x y are: u x k = | log |x k || | log |x k ||+| log |y k || u z k u y k = | log |y k || | log |x k ||+| log |y k || u z k where the contribution ratios are determined by | log |x k || and | log |y k ||."}, {"id": 120, "string": "For scalar multiplication, z = \u03bbx where \u03bb denotes a constant."}, {"id": 121, "string": "We directly assign z's uncertainty scores to x and the backpropagation rule is u x k = u z k ."}, {"id": 122, "string": "As shown in Algorithm 2, we first initialize uncertainty backpropagation in the decoder (lines 1-5)."}, {"id": 123, "string": "For each predicted token a t , we compute its uncertainty score u at as in Equation (7) ."}, {"id": 124, "string": "Next, we find the dimension of a t in the decoder's softmax classifier (Equation (5) ), and initialize the neuron with the uncertainty score u at ."}, {"id": 125, "string": "We then backpropagate these uncertainty scores through Dataset Example IFTTT turn android phone to full volume at 7am monday to friday date time\u2212every day of the week at\u2212((time of day (07)(:)(00)) (days of the week (1)(2)(3)(4)(5))) THEN android device\u2212set ringtone volume\u2212(volume ({' volume level':1.0,'name':'100%'})) DJANGO for every key in sorted list of user settings for key in sorted(user settings): the network (lines 6-9), and finally into the neurons of the input words."}, {"id": 126, "string": "We summarize them and compute the token-level scores for interpreting the results (line 10-13)."}, {"id": 127, "string": "For input word vector q t , we use the summation of its neuron-level scores as the token-level score:\u00fb qt \u221d c\u2208qt u c where c \u2208 q t represents the neurons of word vector q t , and |q| t=1\u00fb qt = 1."}, {"id": 128, "string": "We use the normalized score\u00fb qt to indicate token q t 's contribution to prediction uncertainty."}, {"id": 129, "string": "Experiments In this section we describe the datasets used in our experiments and various details concerning our models."}, {"id": 130, "string": "We present our experimental results and analysis of model behavior."}, {"id": 131, "string": "Our code is publicly available at https://github.com/ donglixp/confidence."}, {"id": 132, "string": "Datasets We trained the neural semantic parser introduced in Section 3 on two datasets covering different domains and meaning representations."}, {"id": 133, "string": "Examples are shown in Table 1 ."}, {"id": 134, "string": "IFTTT This dataset (Quirk et al., 2015) contains a large number of if-this-then-that programs crawled from the IFTTT website."}, {"id": 135, "string": "The programs are written for various applications, such as home security (e.g., \"email me if the window opens\"), and task automation (e.g., \"save instagram photos to dropbox\")."}, {"id": 136, "string": "Whenever a program's trigger is satisfied, an action is performed."}, {"id": 137, "string": "Triggers and actions represent functions with arguments; they are selected from different channels (160 in total) representing various services (e.g., Android)."}, {"id": 138, "string": "There are 552 trigger functions and 229 action functions."}, {"id": 139, "string": "The original split contains 77, 495 training, 5, 171 development, and 4, 294 test instances."}, {"id": 140, "string": "The subset that removes non-English descriptions was used in our experiments."}, {"id": 141, "string": "DJANGO This dataset (Oda et al., 2015) is built upon the code of the Django web framework."}, {"id": 142, "string": "Each line of Python code has a manually annotated natural language description."}, {"id": 143, "string": "Our goal is to map the English pseudo-code to Python statements."}, {"id": 144, "string": "This dataset contains diverse use cases, such as iteration, exception handling, and string manipulation."}, {"id": 145, "string": "The original split has 16, 000 training, 1, 000 development, and 1, 805 test examples."}, {"id": 146, "string": "Settings We followed the data preprocessing used in previous work (Dong and Lapata, 2016; Yin and Neubig, 2017) ."}, {"id": 147, "string": "Input sentences were tokenized using NLTK (Bird et al., 2009) and lowercased."}, {"id": 148, "string": "We filtered words that appeared less than four times in the training set."}, {"id": 149, "string": "Numbers and URLs in IFTTT and quoted strings in DJANGO were replaced with place holders."}, {"id": 150, "string": "Hyperparameters of the semantic parsers were validated on the development set."}, {"id": 151, "string": "The learning rate and the smoothing constant of RMSProp (Tieleman and Hinton, 2012) were 0.002 and 0.95, respectively."}, {"id": 152, "string": "The dropout rate was 0.25."}, {"id": 153, "string": "A two-layer LSTM was used for IFTTT, while a one-layer LSTM was employed for DJANGO."}, {"id": 154, "string": "Dimensions for the word embedding and hidden vector were selected from {150, 250}."}, {"id": 155, "string": "The beam size during decoding was 5."}, {"id": 156, "string": "For IFTTT, we view the predicted trees as a set of productions, and use balanced F1 as evaluation metric (Quirk et al., 2015) ."}, {"id": 157, "string": "We do not measure accuracy because the dataset is very noisy and there rarely is an exact match between the predicted output and the gold standard."}, {"id": 158, "string": "The F1 score of our neural semantic parser is 50.1%, which is comparable to Dong and Lapata (2016) ."}, {"id": 159, "string": "For DJANGO, we measure the fraction of exact matches, where F1 score is equal to accuracy."}, {"id": 160, "string": "Because there are unseen variable names at test time, we use attention scores as alignments to replace unknown to- Table 2 : Spearman \u03c1 correlation between confidence scores and F1."}, {"id": 161, "string": "Best results are shown in bold."}, {"id": 162, "string": "All correlations are significant at p < 0.01. kens in the prediction with the input words they align to (Luong et al., 2015b) ."}, {"id": 163, "string": "The accuracy of our parser is 53.7%, which is better than the result (45.1%) of the sequence-to-sequence model reported in Yin and Neubig (2017) ."}, {"id": 164, "string": "To estimate model uncertainty, we set dropout rate to 0.1, and performed 30 inference passes."}, {"id": 165, "string": "The standard deviation of Gaussian noise was 0.05."}, {"id": 166, "string": "The language model was estimated using KenLM (Heafield et al., 2013) ."}, {"id": 167, "string": "For input uncertainty, we computed variance for the 10-best candidates."}, {"id": 168, "string": "The confidence metrics were implemented in batch mode, to take full advantage of GPUs."}, {"id": 169, "string": "Hyperparameters of the confidence scoring model were cross-validated."}, {"id": 170, "string": "The number of boosted trees was selected from {20, 50}."}, {"id": 171, "string": "The maximum tree depth was selected from {3, 4, 5}."}, {"id": 172, "string": "We set the subsample ratio to 0.8."}, {"id": 173, "string": "All other hyperparameters in XGBoost (Chen and Guestrin, 2016) were left with their default values."}, {"id": 174, "string": "Results Confidence Estimation We compare our approach (CONF) against confidence scores based on posterior probability p(a|q) (POSTERIOR)."}, {"id": 175, "string": "We also report the results of three ablation variants (\u2212MODEL, \u2212DATA, \u2212INPUT) by removing each group of confidence metrics described in Section 4."}, {"id": 176, "string": "We measure the relationship between confidence scores and F1 using Spearman's \u03c1 correlation coefficient which varies between \u22121 and 1 (0 implies there is no correlation)."}, {"id": 177, "string": "High \u03c1 indicates that the confidence scores are high for correct predictions and low otherwise."}, {"id": 178, "string": "As shown in Table 2 , our method CONF outperforms POSTERIOR by a large margin."}, {"id": 179, "string": "The ablation results indicate that model uncertainty plays the most important role among the confidence metrics."}, {"id": 180, "string": "In contrast, removing the metrics of data uncertainty affects performance less, because most examples in the datasets are in-domain."}, {"id": 181, "string": "Improve- Table 3 ."}, {"id": 182, "string": "ments for each group of metrics are significant with p < 0.05 according to bootstrap hypothesis testing (Efron and Tibshirani, 1994) ."}, {"id": 183, "string": "Tables 3 and 4 show the correlation matrix for F1 and individual confidence metrics on the IFTTT and DJANGO datasets, respectively."}, {"id": 184, "string": "As can be seen, metrics representing model uncertainty and input uncertainty are more correlated to each other compared with metrics capturing data uncertainty."}, {"id": 185, "string": "Perhaps unsurprisingly metrics of the same group are highly inter-correlated since they model the same type of uncertainty."}, {"id": 186, "string": "Table 5 shows the relative importance of individual metrics in the regression model."}, {"id": 187, "string": "As importance score we use the average gain (i.e., loss reduction) brought by the confidence metric once added as feature to the branch of the decision tree (Chen and Guestrin, 2016) ."}, {"id": 188, "string": "The results indicate that model uncertainty (Noise/Dropout/Posterior/Perplexity) plays Table 5 : Importance scores of confidence metrics (normalized by maximum value on each dataset)."}, {"id": 189, "string": "Best results are shown in bold."}, {"id": 190, "string": "Same shorthands apply as in Table 3. the most important role."}, {"id": 191, "string": "On IFTTT, the number of unknown tokens (#UNK) and the variance of top candidates (var(K-best)) are also very helpful because this dataset is relatively noisy and contains many ambiguous inputs."}, {"id": 192, "string": "Finally, in real-world applications, confidence scores are often used as a threshold to trade-off precision for coverage."}, {"id": 193, "string": "Figure 3 shows how F1 score varies as we increase the confidence threshold, i.e., reduce the proportion of examples that we return answers for."}, {"id": 194, "string": "F1 score improves monotonically for POSTERIOR and our method, which, however, achieves better performance when coverage is the same."}, {"id": 195, "string": "Uncertainty Interpretation We next evaluate how our backpropagation method (see Section 5) allows us to identify input tokens contributing to uncertainty."}, {"id": 196, "string": "We compare against a method that interprets uncertainty based on the attention mechanism (ATTENTION)."}, {"id": 197, "string": "As shown in Equation (2) , attention scores r t,k can be used as soft alignments between the time step t of the decoder and the k-th input token."}, {"id": 198, "string": "We compute the normalized uncertainty score\u00fb qt for a token q t via: u qt \u221d |a| t=1 r t,k u at (8) where u at is the uncertainty score of the predicted token a t (Equation (7) ), and |q| t=1\u00fb qt = 1."}, {"id": 199, "string": "Unfortunately, the evaluation of uncertainty interpretation methods is problematic."}, {"id": 200, "string": "For our semantic parsing task, we do not a priori know which tokens in the natural language input contribute to uncertainty and these may vary depending on the architecture used, model parameters, and so on."}, {"id": 201, "string": "We work around this problem by creating a proxy gold standard."}, {"id": 202, "string": "We inject noise to the vectors representing tokens in the encoder (see Section 4.1) and then estimate the uncertainty caused by each token q t (Equation (6) addition of noise should only affect genuinely uncertain tokens."}, {"id": 203, "string": "Notice that here we inject noise to one token at a time 1 instead of all parameters (see Figure 1 )."}, {"id": 204, "string": "Tokens identified as uncertain by the above procedure are considered gold standard and compared to those identified by our method."}, {"id": 205, "string": "We use Gaussian noise to perturb vectors in our experiments (dropout obtained similar results)."}, {"id": 206, "string": "We define an evaluation metric based on the overlap (overlap@K) among tokens identified as uncertain by the model and the gold standard."}, {"id": 207, "string": "Given an example, we first compute the interpretation scores of the input tokens according to our method, and obtain a list \u03c4 1 of K tokens with highest scores."}, {"id": 208, "string": "We also obtain a list \u03c4 2 of K tokens with highest ground-truth scores and measure the degree of overlap between these two lists: overlap@K = |\u03c4 1 \u2229 \u03c4 2 | K Method IFTTT DJANGO @2 @4 @2 @4 ATTENTION 0.525 0.737 0.637 0.684 BACKPROP 0.608 0.791 0.770 0.788 Table 6 : Uncertainty interpretation against inferred ground truth; we compute the overlap between tokens identified as contributing to uncertainty by our method and those found in the gold standard."}, {"id": 209, "string": "Overlap is shown for top 2 and 4 tokens."}, {"id": 210, "string": "Best results are in bold."}, {"id": 211, "string": "google calendar\u2212any event starts THEN facebook \u2212create a status message\u2212(status message ({description})) ATT post calendar event to facebook BP post calendar event to facebook feed\u2212new feed item\u2212(feed url( url sports.espn.go.com)) THEN ... ATT espn mlb headline to readability BP espn mlb headline to readability weather\u2212tomorrow's low drops below\u2212(( temperature(0)) (degrees in(c))) THEN ... ATT warn me when it's going to be freezing tomorrow BP warn me when it's going to be freezing tomorrow if str number[0] == ' STR ': ATT if first element of str number equals a string STR ."}, {"id": 212, "string": "BP if first element of str number equals a string STR ."}, {"id": 213, "string": "start = 0 ATT start is an integer 0 ."}, {"id": 214, "string": "BP start is an integer 0 ."}, {"id": 215, "string": "if name.startswith(' STR '): ATT if name starts with an string STR , BP if name starts with an string STR , Table 7 : Uncertainty interpretation for ATTEN-TION (ATT) and BACKPROP (BP) ."}, {"id": 216, "string": "The first line in each group is the model prediction."}, {"id": 217, "string": "Predicted tokens and input words with large scores are shown in red and blue, respectively."}, {"id": 218, "string": "where K \u2208 {2, 4} in our experiments."}, {"id": 219, "string": "For example, the overlap@4 metric of the lists \u03c4 1 = [q 7 , q 8 , q 2 , q 3 ] and \u03c4 2 = [q 7 , q 8 , q 3 , q 4 ] is 3/4, because there are three overlapping tokens."}, {"id": 220, "string": "Table 6 reports results with overlap@2 and overlap@4."}, {"id": 221, "string": "Overall, BACKPROP achieves better interpretation quality than the attention mechanism."}, {"id": 222, "string": "On both datasets, about 80% of the top-4 tokens identified as uncertain agree with the ground truth."}, {"id": 223, "string": "Table 7 shows examples where our method has identified input tokens contributing to the uncertainty of the output."}, {"id": 224, "string": "We highlight token a t if its uncertainty score u at is greater than 0.5 * avg{u a t } |a| t =1 ."}, {"id": 225, "string": "The results illustrate that the parser tends to be uncertain about tokens which are function arguments (e.g., URLs, and message content), and ambiguous inputs."}, {"id": 226, "string": "The examples show that BACKPROP is qualitatively better compared to ATTENTION; attention scores often produce inaccurate alignments while BACKPROP can utilize information flowing through the LSTMs rather than only relying on the attention mechanism."}, {"id": 227, "string": "Conclusions In this paper we presented a confidence estimation model and an uncertainty interpretation method for neural semantic parsing."}, {"id": 228, "string": "Experimental results show that our method achieves better performance than competitive baselines on two datasets."}, {"id": 229, "string": "Directions for future work are many and varied."}, {"id": 230, "string": "The proposed framework could be applied to a variety of tasks (Bahdanau et al., 2015; Schmaltz et al., 2017) employing sequence-to-sequence architectures."}, {"id": 231, "string": "We could also utilize the confidence estimation model within an active learning framework for neural semantic parsing."}], "headers": [{"section": "Introduction", "n": "1", "start": 0, "end": 24}, {"section": "Related Work", "n": "2", "start": 25, "end": 36}, {"section": "Neural Semantic Parsing Model", "n": "3", "start": 37, "end": 52}, {"section": "Confidence Estimation", "n": "4", "start": 53, "end": 57}, {"section": "Model Uncertainty", "n": "4.1", "start": 58, "end": 76}, {"section": "Data Uncertainty", "n": "4.2", "start": 77, "end": 82}, {"section": "Input Uncertainty", "n": "4.3", "start": 83, "end": 93}, {"section": "Confidence Scoring", "n": "4.4", "start": 94, "end": 99}, {"section": "Uncertainty Interpretation", "n": "5", "start": 100, "end": 128}, {"section": "Experiments", "n": "6", "start": 129, "end": 131}, {"section": "Datasets", "n": "6.1", "start": 132, "end": 145}, {"section": "Settings", "n": "6.2", "start": 146, "end": 226}, {"section": "Conclusions", "n": "7", "start": 227, "end": 231}], "figures": [{"filename": "../figure/image/1307-Table1-1.png", "caption": "Table 1: Natural language descriptions and their meaning representations from IFTTT and DJANGO.", "page": 5, "bbox": {"x1": 77.75999999999999, "x2": 517.4399999999999, "y1": 67.67999999999999, "y2": 150.23999999999998}}, {"filename": "../figure/image/1307-Table4-1.png", "caption": "Table 4: Correlation matrix for F1 and individual confidence metrics on the DJANGO dataset. All correlations are significant at p < 0.01. Best predictors are shown in bold. Same shorthands apply as in Table 3.", "page": 6, "bbox": {"x1": 313.92, "x2": 518.4, "y1": 307.68, "y2": 410.4}}, {"filename": "../figure/image/1307-Table2-1.png", "caption": "Table 2: Spearman \u03c1 correlation between confidence scores and F1. Best results are shown in bold. All correlations are significant at p < 0.01.", "page": 6, "bbox": {"x1": 109.92, "x2": 250.07999999999998, "y1": 62.879999999999995, "y2": 140.16}}, {"filename": "../figure/image/1307-Table3-1.png", "caption": "Table 3: Correlation matrix for F1 and individual confidence metrics on the IFTTT dataset. All correlations are significant at p < 0.01. Best predictors are shown in bold. Dout is short for dropout, PR for posterior probability, PPL for perplexity, LM for probability based on a language model, #UNK for number of unknown tokens, Var for variance of top candidates, and Ent for Entropy.", "page": 6, "bbox": {"x1": 313.92, "x2": 518.4, "y1": 62.879999999999995, "y2": 165.12}}, {"filename": "../figure/image/1307-Figure1-1.png", "caption": "Figure 1: We use dropout as approximate Bayesian inference to obtain model uncertainty. The dropout layers are applied to i) token vectors; ii) the encoder\u2019s output vectors; iii) bridge vectors; and iv) decoding vectors.", "page": 2, "bbox": {"x1": 74.88, "x2": 284.15999999999997, "y1": 61.44, "y2": 139.2}}, {"filename": "../figure/image/1307-Figure3-1.png", "caption": "Figure 3: Confidence scores are used as threshold to filter out uncertain test examples. As the threshold increases, performance improves. The horizontal axis shows the proportion of examples beyond the threshold.", "page": 7, "bbox": {"x1": 331.2, "x2": 499.2, "y1": 64.8, "y2": 348.47999999999996}}, {"filename": "../figure/image/1307-Table5-1.png", "caption": "Table 5: Importance scores of confidence metrics (normalized by maximum value on each dataset). Best results are shown in bold. Same shorthands apply as in Table 3.", "page": 7, "bbox": {"x1": 73.92, "x2": 288.0, "y1": 62.879999999999995, "y2": 105.11999999999999}}, {"filename": "../figure/image/1307-Table6-1.png", "caption": "Table 6: Uncertainty interpretation against inferred ground truth; we compute the overlap between tokens identified as contributing to uncertainty by our method and those found in the gold standard. Overlap is shown for top 2 and 4 tokens. Best results are in bold.", "page": 8, "bbox": {"x1": 86.88, "x2": 273.12, "y1": 62.879999999999995, "y2": 120.47999999999999}}, {"filename": "../figure/image/1307-Table7-1.png", "caption": "Table 7: Uncertainty interpretation for ATTENTION (ATT) and BACKPROP (BP) . The first line in each group is the model prediction. Predicted tokens and input words with large scores are shown", "page": 8, "bbox": {"x1": 70.56, "x2": 299.03999999999996, "y1": 222.72, "y2": 452.15999999999997}}, {"filename": "../figure/image/1307-Figure2-1.png", "caption": "Figure 2: Uncertainty backpropagation at the neuron level. Neuron m\u2019s score um is collected from child neurons c1 and c2 by um = vc1muc1 + v c2 muc2 . The score um is then redistributed to its parent neurons p1 and p2, which satisfies vmp1 + v m p2 = 1.", "page": 4, "bbox": {"x1": 73.44, "x2": 277.44, "y1": 62.879999999999995, "y2": 142.07999999999998}}]}