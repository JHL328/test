{"title": "Bleaching Text: Abstract Features for Cross-lingual Gender Prediction", "abstract": "Gender prediction has typically focused on lexical and social network features, yielding good performance, but making systems highly language-, topic-, and platformdependent. Cross-lingual embeddings circumvent some of these limitations, but capture gender-specific style less. We propose an alternative: bleaching text, i.e., transforming lexical strings into more abstract features. This study provides evidence that such features allow for better transfer across languages. Moreover, we present a first study on the ability of humans to perform cross-lingual gender prediction. We find that human predictive power proves similar to that of our bleached models, and both perform better than lexical models.", "text": [{"id": 0, "string": "Introduction Author profiling is the task of discovering latent user attributes disclosed through text, such as gender, age, personality, income, location and occupation (Rao et al., 2010; Burger et al., 2011; Feng et al., 2012; Jurgens, 2013; Bamman et al., 2014; Plank and Hovy, 2015; Flekova et al., 2016) ."}, {"id": 1, "string": "It is of interest to several applications including personalized machine translation, forensics, and marketing (Mirkin et al., 2015; Rangel et al., 2015) ."}, {"id": 2, "string": "Early approaches to gender prediction (Koppel et al., 2002; Schler et al., 2006, e.g.)"}, {"id": 3, "string": "are inspired by pioneering work on authorship attribution (Mosteller and Wallace, 1964) ."}, {"id": 4, "string": "Such stylometric models typically rely on carefully handselected sets of content-independent features to capture style beyond topic."}, {"id": 5, "string": "Recently, open vocabulary approaches (Schwartz et al., 2013) , where the entire linguistic production of an author is used, yielded substantial performance gains in on-line user-attribute prediction (Nguyen et al., 2014; Preo\u0163iuc-Pietro et al., 2015; Emmery et al., 2017) ."}, {"id": 6, "string": "Indeed, the best performing gender prediction models exploit chiefly lexical information (Rangel et al., 2017; Basile et al., 2017) ."}, {"id": 7, "string": "Relying heavily on the lexicon though has its limitations, as it results in models with limited portability."}, {"id": 8, "string": "Moreover, performance might be overly optimistic due to topic bias (Sarawgi et al., 2011) ."}, {"id": 9, "string": "Recent work on cross-lingual author profiling has proposed the use of solely language-independent features (Ljube\u0161i\u0107 et al., 2017) , e.g., specific textual elements (percentage of emojis, URLs, etc) and users' meta-data/network (number of followers, etc), but this information is not always available."}, {"id": 10, "string": "We propose a novel approach where the actual text is still used, but bleached out and transformed into more abstract, and potentially better transferable features."}, {"id": 11, "string": "One could view this as a method in between the open vocabulary strategy and the stylometric approach."}, {"id": 12, "string": "It has the advantage of fading out content in favor of more shallow patterns still based on the original text, without introducing additional processing such as part-of-speech tagging."}, {"id": 13, "string": "In particular, we investigate to what extent gender prediction can rely on generic non-lexical features (RQ1), and how predictive such models are when transferred to other languages (RQ2)."}, {"id": 14, "string": "We also glean insights from human judgments, and investigate how well people can perform cross-lingual gender prediction (RQ3)."}, {"id": 15, "string": "We focus on gender prediction for Twitter, motivated by data availability."}, {"id": 16, "string": "Contributions In this work i) we are the first to study cross-lingual gender prediction without relying on users' meta-data; ii) we propose a novel simple abstract feature representation which is surprisingly effective; and iii) we gauge human ability to perform cross-lingual gender detection, an angle of analysis which has not been studied thus far."}, {"id": 17, "string": "Profiling with Abstract Features Can we recover the gender of an author from bleached text, i.e., transformed text were the raw lexical strings are converted into abstract features?"}, {"id": 18, "string": "We investigate this question by building a series of predictive models to infer the gender of a Twitter user, in absence of additional user-specific metadata."}, {"id": 19, "string": "Our approach can be seen as taking advantage of elements from a data-driven open-vocabulary approach, while trying to capture gender-specific style in text beyond topic."}, {"id": 20, "string": "To represent utterances in a more language agnostic way, we propose to simply transform the text into alternative textual representations, which deviate from the lexical form to allow for abstraction."}, {"id": 21, "string": "We propose the following transformations, exemplified in Table 1 ."}, {"id": 22, "string": "They are mostly motivated by intuition and inspired by prior work, like the use of shape features from NER and parsing (Petrov and Klein, 2007; Schnabel and Sch\u00fctze, 2014; Limsopatham and Collier, 2016) : \u2022 Frequency Each word is presented as its binned frequency in the training data; bins are sized by orders of magnitude."}, {"id": 23, "string": "\u2022 Length Number of characters (prefixed by 0 to avoid collision with the next transformation)."}, {"id": 24, "string": "\u2022 PunctC Merges all consecutive alphanumeric characters to one 'W' and leaves all other characters as they are (C for conservative)."}, {"id": 25, "string": "\u2022 PunctA Generalization of PunctC (A for aggressive), converting different types of punctuation to classes: emoticons 1 to 'E' and emojis 2 to 'J', other punctuation to 'P'."}, {"id": 26, "string": "\u2022 Shape Transforms uppercase characters to 'U', lowercase characters to 'L', digits to 'D' and all other characters to 'X'."}, {"id": 27, "string": "Repetitions of transformed characters are condensed to a maximum of 2 for greater generalization."}, {"id": 28, "string": "\u2022 Vowel-Consonant To approximate vowels, while being able to generalize over (Indo-European) languages, we convert any of the 'aeiou' characters to 'V', other alphabetic character to 'C', and all other characters to 'O'."}, {"id": 29, "string": "\u2022 AllAbs A combination (concatenation) of all previously described features."}, {"id": 30, "string": "Experiments In order to test whether abstract features are effective and transfer across languages, we set up experiments for gender prediction comparing lexicalized and bleached models for both in-and cross-language experiments."}, {"id": 31, "string": "We compare them to a model using multilingual embeddings (Ruder, 2017) ."}, {"id": 32, "string": "Finally, we elicit human judgments both within language and across language."}, {"id": 33, "string": "The latter is to check whether a person with no prior knowledge of (the lexicon of) a given language can predict the gender of a user, and how that compares to an in-language setup and the machine."}, {"id": 34, "string": "If humans can predict gender cross-lingually, they are likely to rely on aspects beyond lexical information."}, {"id": 35, "string": "Data We obtain data from the TWISTY corpus , a multi-lingual collection of Twitter users, for the languages with 500+ users, namely Dutch, French, Portuguese, and Spanish."}, {"id": 36, "string": "We complement them with English, using data from a predecessor of TWISTY (Plank and Hovy, 2015) ."}, {"id": 37, "string": "All datasets contain manually annotated gender information."}, {"id": 38, "string": "To simplify interpretation for the cross-language experiments, we balance gender in all datasets by downsampling to the minority class."}, {"id": 39, "string": "The datasets' final sizes are given in Table 2 ."}, {"id": 40, "string": "We use 200 tweets per user, as done by previous work ."}, {"id": 41, "string": "We leave the data untokenized to exclude any languagedependent processing, because original tokenization could preserve some signal."}, {"id": 42, "string": "Apart from mapping usernames to 'USER' and urls to 'URL' we do not perform any further data pre-processing."}, {"id": 43, "string": "Lexical vs Bleached Models We use the scikit-learn (Pedregosa et al., 2011) implementation of a linear SVM with default parameters (e.g., L2 regularization)."}, {"id": 44, "string": "We use 10-fold cross validation for all in-language experiments."}, {"id": 45, "string": "For the cross-lingual experiments, we train on all available source language data and test on all target language data."}, {"id": 46, "string": "For the lexicalized experiments, we adopt the features from the best performing system at the latest PAN evaluation campaign 3 (Basile et al., 2017) (word 1-2 grams and character 3-6 grams)."}, {"id": 47, "string": "For the multilingual embeddings model we use the mean embedding representation from the system of (Plank, 2017) and add max, std and coverage features."}, {"id": 48, "string": "We create multilingual embeddings by projecting monolingual embeddings to a single multilingual space for all five languages using a recently proposed SVD-based projection method with a pseudo-dictionary (Smith et al., 2017) ."}, {"id": 49, "string": "The monolingual embeddings are trained on large amounts of in-house Twitter data (as much data as we had access to, i.e., ranging from 30M tweets for French to 1,500M tweets in Dutch, with a word type coverage between 63 and 77%)."}, {"id": 50, "string": "This results in an embedding space with a vocabulary size of 16M word types."}, {"id": 51, "string": "All code is available at https:// github.com/bplank/bleaching-text."}, {"id": 52, "string": "For the bleached experiments, we ran models with each feature set separately."}, {"id": 53, "string": "In this paper, we report results for the model where all features are combined, as it proved to be the most robust across languages."}, {"id": 54, "string": "We tuned the n-gram size of this model through in-language cross-validation, finding that n = 5 performs best."}, {"id": 55, "string": "When testing across languages, we report accuracy for two setups: average accuracy over each single-language model (AVG), and accuracy obtained when training on the concatenation of all languages but the target one (ALL)."}, {"id": 56, "string": "The latter setting is also used for the embeddings model."}, {"id": 57, "string": "We report accuracy for all experiments."}, {"id": 58, "string": "Results and Analysis Table 2 shows results for both the cross-language and in-language experiments in the lexical and abstract-feature setting."}, {"id": 59, "string": "Within language, the lexical features unsurprisingly work the best, achieving an average accuracy of 80.5% over all languages."}, {"id": 60, "string": "The abstract features lose some information and score on average 11.8% lower, still beating the majority baseline (50%) by a large margin (68.7%)."}, {"id": 61, "string": "If we go across language, the lexical approaches break down (overall to 53.7% for LEX AVG/56.3% for ALL), except for Portuguese and Spanish, thanks to their similarities (see Table 3 for pair-wise results)."}, {"id": 62, "string": "The closelyrelated-language effect is also observed when training on all languages, as scores go up when the classifier has access to the related language."}, {"id": 63, "string": "The same holds for the multilingual embeddings model."}, {"id": 64, "string": "On average it reaches an accuracy of 59.8%."}, {"id": 65, "string": "The closeness effect for Portuguese and Spanish can also be observed in language-to-language experiments, where scores for ES \u2192PT and PT \u2192ES are the highest."}, {"id": 66, "string": "Results for the lexical models are generally lower on English, which might be due to smaller amounts of data (see first column in Table 2 providing number of users per language)."}, {"id": 67, "string": "The abstract features fare surprisingly well and Table 4 , we can see that the use of an emoji (like ) and shape-based features are predictive of female users."}, {"id": 68, "string": "Quotes, question marks and length features, for example, appear to be more predictive of male users."}, {"id": 69, "string": "Human Evaluation We experimented with three different conditions, one within language and two across language."}, {"id": 70, "string": "For the latter, we set up an experiment where native speakers of Dutch were presented with tweets written in Portuguese and were asked to guess the poster's gender."}, {"id": 71, "string": "In the other experiment, we asked speakers of French to identify the gender of the writer when reading Dutch tweets."}, {"id": 72, "string": "In both cases, the participants declared to have no prior knowledge of the target language."}, {"id": 73, "string": "For the in-language experiment, we asked Dutch speakers to identify the gender of a user writing Dutch tweets."}, {"id": 74, "string": "The  Dutch speakers who participated in the two experiments are distinct individuals."}, {"id": 75, "string": "Participants were informed of the experiment's goal."}, {"id": 76, "string": "Their identity is anonymized in the data."}, {"id": 77, "string": "We selected a random sample of 200 users from the Dutch and Portuguese data, preserving a 50/50 gender distribution."}, {"id": 78, "string": "Each user was represented by twenty tweets."}, {"id": 79, "string": "The answer key (F/M) order was randomized."}, {"id": 80, "string": "For each of the three experiments we had six judges, balanced for gender, and obtained three annotations per target user."}, {"id": 81, "string": "Results and Analysis Inter-annotator agreement for the tasks was measured via Fleiss kappa (n = 3, N = 200), and was higher for the in-language experiment (K = 0.40) than for the cross-language tasks (NL \u2192PT: K = 0.25; FR \u2192NL: K = 0.28)."}, {"id": 82, "string": "Table 5 shows accuracy against the gold labels, comparing humans (average accuracy over three annotators) to lexical and bleached models on the exact same subset of 200 users."}, {"id": 83, "string": "Systems were tested under two different conditions regarding the number of tweets per user for the target language: machine and human saw the exact same twenty tweets, or the full set of tweets (200) per user, as done during training (Section 3.1)."}, {"id": 84, "string": "First of all, our results indicate that in-language performance of humans is 70.5%, which is quite in line with the findings of Flekova et al."}, {"id": 85, "string": "(2016) , who report an accuracy of 75% on English."}, {"id": 86, "string": "Within language, lexicalized models are superior to humans if exposed to enough information (200 tweets setup)."}, {"id": 87, "string": "One explanation for this might lie in an observation by Flekova et al."}, {"id": 88, "string": "(2016) , according to which people tend to rely too much on stereotypical lexical indicators when assigning gender to the poster of a tweet, while machines model less evident patterns."}, {"id": 89, "string": "Lexicalized models are also superior to the bleached ones, as already seen on the full datasets (Table 2) ."}, {"id": 90, "string": "We can also observe that the amount of information available to represent a user influences system's performance."}, {"id": 91, "string": "Training on 200 tweets per user, but testing on 20 tweets only, decreases performance by 12 percentage points."}, {"id": 92, "string": "This is likely due to the fact that inputs are sparser, especially since the bleached model is trained on 5-grams."}, {"id": 93, "string": "5 The bleached model, when given 200 tweets per user, yields a performance that is slightly higher than human accuracy."}, {"id": 94, "string": "In the cross-language setting, the picture is very different."}, {"id": 95, "string": "Here, human performance is superior to the lexicalized models, independently of the amount of tweets per user at testing time."}, {"id": 96, "string": "This seems to indicate that if humans cannot rely on the lexicon, they might be exploiting some other signal when guessing the gender of a user who tweets in a language unknown to them."}, {"id": 97, "string": "Interestingly, the bleached models, which rely on non-lexical features, not only outperform the lexicalized ones in the cross-language experiments, but also neatly match the human scores."}, {"id": 98, "string": "Related Work Most existing work on gender prediction exploits shallow lexical information based on the linguistic production of the users."}, {"id": 99, "string": "Few studies investigate deeper syntactic information (Koppel et al., 2002; Feng et al., 2012) or non-linguistic input, e.g., language-independent clues such as visual (Alowibdi et al., 2013) or network information (Jurgens, 2013; Plank and Hovy, 2015; Ljube\u0161i\u0107 et al., 2017) ."}, {"id": 100, "string": "A related angle is cross-genre profiling."}, {"id": 101, "string": "In both settings lexical models have limited portability due to their bias towards the language/genre they have been trained on (Rangel et al., 2016; Busger op Vollenbroek et al., 2016; ."}, {"id": 102, "string": "Lexical bias has been shown to affect inlanguage human gender prediction, too."}, {"id": 103, "string": "Flekova et al."}, {"id": 104, "string": "(2016) found that people tend to rely too much on stereotypical lexical indicators, while Nguyen et al."}, {"id": 105, "string": "(2014) show that more than 10% of the Twitter users do actually not employ words that the crowd associates with their biological sex."}, {"id": 106, "string": "Our features abstract away from such lexical cues while retaining predictive signal."}, {"id": 107, "string": "Conclusions Bleaching text into abstract features is surprisingly effective for predicting gender, though lexical infor- 5 We experimented with training on 20 tweets rather than 200, and with different n-gram sizes (e.g., 1-4)."}, {"id": 108, "string": "Despite slightly better results, we decided to use the trained models as they were to employ the same settings across all experiments (200 tweets per users, n = 5), with no further tuning."}, {"id": 109, "string": "mation is still more useful within language (RQ1)."}, {"id": 110, "string": "However, models based on lexical clues fail when transferred to other languages, or require large amounts of unlabeled data from a similar domain as our experiments with the multilingual embedding model indicate."}, {"id": 111, "string": "Instead, our bleached models clearly capture some signal beyond the lexicon, and perform well in a cross-lingual setting (RQ2)."}, {"id": 112, "string": "We are well aware that we are testing our crosslanguage bleached models in the context of closely related languages."}, {"id": 113, "string": "While some features (such as PunctA, or Frequency) might carry over to genetically more distant languages, other features (such as Vowels and Shape) would probably be meaningless."}, {"id": 114, "string": "Future work on this will require a sensible setting from a language typology perspective for choosing and testing adequate features."}, {"id": 115, "string": "In our novel study on human proficiency for cross-lingual gender prediction, we discovered that people are also abstracting away from the lexicon."}, {"id": 116, "string": "Indeed, we observe that they are able to detect gender by looking at tweets in a language they do not know (RQ3) with an accuracy of 60% on average."}], "headers": [{"section": "Introduction", "n": "1", "start": 0, "end": 16}, {"section": "Profiling with Abstract Features", "n": "2", "start": 17, "end": 29}, {"section": "Experiments", "n": "3", "start": 30, "end": 42}, {"section": "Lexical vs Bleached Models", "n": "3.1", "start": 43, "end": 68}, {"section": "Human Evaluation", "n": "3.2", "start": 69, "end": 97}, {"section": "Related Work", "n": "4", "start": 98, "end": 106}, {"section": "Conclusions", "n": "5", "start": 107, "end": 116}], "figures": [{"filename": "../figure/image/1090-Table2-1.png", "caption": "Table 2: Number of users per language and results for gender prediction (accuracy). IN-LANGUAGE: 10-fold cross-validation. CROSS-LANGUAGE: Testing on all test data in two setups: averages over single source models (AVG) or training a single model on all languages except the target (ALL). Comparison of lexical n-gram models (LEX), bleached models (ABS) and multilingual embeddings model (EMBEDS).", "page": 2, "bbox": {"x1": 72.0, "x2": 523.1999999999999, "y1": 65.75999999999999, "y2": 168.0}}, {"filename": "../figure/image/1090-Table3-1.png", "caption": "Table 3: Pair-wise results for lexicalized models.", "page": 2, "bbox": {"x1": 306.71999999999997, "x2": 523.1999999999999, "y1": 251.51999999999998, "y2": 363.36}}, {"filename": "../figure/image/1090-Table1-1.png", "caption": "Table 1: Abstract features example transformation.", "page": 1, "bbox": {"x1": 72.0, "x2": 289.44, "y1": 62.879999999999995, "y2": 126.24}}, {"filename": "../figure/image/1090-Table4-1.png", "caption": "Table 4: Ten most predictive features of the ABS model across all five languages. Features are ranked by how often they were in the top-ranked features for each language. Those prefixed with 0 (line 9) are length features. The prefix is used to avoid clashes with the frequency features.", "page": 3, "bbox": {"x1": 77.75999999999999, "x2": 284.15999999999997, "y1": 62.4, "y2": 224.16}}, {"filename": "../figure/image/1090-Table5-1.png", "caption": "Table 5: Accuracy human versus machine.", "page": 3, "bbox": {"x1": 308.64, "x2": 524.16, "y1": 62.4, "y2": 136.32}}]}