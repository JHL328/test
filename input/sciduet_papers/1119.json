{"title": "Jointly Predicting Predicates and Arguments in Neural Semantic Role Labeling", "abstract": "Recent BIO-tagging-based neural semantic role labeling models are very high performing, but assume gold predicates as part of the input and cannot incorporate span-level features. We propose an endto-end approach for jointly predicting all predicates, arguments spans, and the relations between them. The model makes independent decisions about what relationship, if any, holds between every possible word-span pair, and learns contextualized span representations that provide rich, shared input features for each decision. Experiments demonstrate that this approach sets a new state of the art on PropBank SRL without gold predicates. 1", "text": [{"id": 0, "string": "Introduction Semantic role labeling (SRL) captures predicateargument relations, such as \"who did what to whom.\""}, {"id": 1, "string": "Recent high-performing SRL models Marcheggiani et al., 2017; Tan et al., 2018) are BIO-taggers, labeling argument spans for a single predicate at a time (as shown in Figure 1) ."}, {"id": 2, "string": "They are typically only evaluated with gold predicates, and must be pipelined with error-prone predicate identification models for deployment."}, {"id": 3, "string": "We propose an end-to-end approach for predicting all the predicates and their argument spans in one forward pass."}, {"id": 4, "string": "Our model builds on a recent coreference resolution model , by making central use of learned, contextualized span representations."}, {"id": 5, "string": "We use these representations to predict SRL graphs directly over text spans."}, {"id": 6, "string": "Each edge is identified by independently predicting which role, if any, holds between every possible pair of text spans, while using aggressive beam 1 Code and models: https://github.com/luheng/lsgn pruning for efficiency."}, {"id": 7, "string": "The final graph is simply the union of predicted SRL roles (edges) and their associated text spans (nodes)."}, {"id": 8, "string": "Our span-graph formulation overcomes a key limitation of semi-markov and BIO-based models (Kong et al., 2016; Zhou and Xu, 2015; Yang and Mitchell, 2017; Tan et al., 2018) : it can model overlapping spans across different predicates in the same output structure (see Figure 1 )."}, {"id": 9, "string": "The span representations also generalize the token-level representations in BIObased models, letting the model dynamically decide which spans and roles to include, without using previously standard syntactic features (Punyakanok et al., 2008; FitzGerald et al., 2015) ."}, {"id": 10, "string": "To the best of our knowledge, this is the first span-based SRL model that does not assume that predicates are given."}, {"id": 11, "string": "In this more realistic setting, where the predicate must be predicted, our model achieves state-of-the-art performance on PropBank."}, {"id": 12, "string": "It also reinforces the strong performance of similar span embedding methods for coreference , suggesting that this style of models could be used for other span-span relation tasks, such as syntactic parsing (Stern et al., 2017) , relation extraction (Miwa and Bansal, 2016) , and QA-SRL (FitzGerald et al., 2018) ."}, {"id": 13, "string": "Model We consider the space of possible predicates to be all the tokens in the input sentence, and the space of arguments to be all continuous spans."}, {"id": 14, "string": "Our model decides what relation exists between each predicate-argument pair (including no relation)."}, {"id": 15, "string": "Formally, given a sequence X = w 1 , ."}, {"id": 16, "string": "."}, {"id": 17, "string": "."}, {"id": 18, "string": ", w n , we wish to predict a set of labeled predicateargument relations Y \u2286 P \u00d7 A \u00d7 L, where P = {w 1 , ."}, {"id": 19, "string": "."}, {"id": 20, "string": "."}, {"id": 21, "string": ", w n } is the set of all tokens (predicates), A = {(w i , ."}, {"id": 22, "string": "."}, {"id": 23, "string": "."}, {"id": 24, "string": ", w j ) | 1 \u2264 i \u2264 j \u2264 n} contains all the spans (arguments), and L is the space of semantic role labels, including a null label indicating no relation."}, {"id": 25, "string": "The final SRL output would be all the non-empty relations {(p, a, l) \u2208 Y | l = }."}, {"id": 26, "string": "We then define a set of random variables, where each random variable y p,a corresponds to a predicate p \u2208 P and an argument a \u2208 A, taking value from the discrete label space L. The random variables y p,a are conditionally independent of each other given the input X: P (Y | X) = p\u2208P,a\u2208A P (y p,a | X) (1) P (y p,a = l | X) = exp(\u03c6(p, a, l)) l \u2208L exp(\u03c6(p, a, l )) (2) Where \u03c6(p, a, l) is a scoring function for a possible (predicate, argument, label) combination."}, {"id": 27, "string": "\u03c6 is decomposed into two unary scores on the predicate and the argument (defined in Section 3), as well as a label-specific score for the relation: \u03c6(p, a, l) = \u03a6 a (a) + \u03a6 p (p) + \u03a6 (l) rel (a, p) (3) The score for the null label is set to a constant: \u03c6(p, a, ) = 0, similar to logistic regression."}, {"id": 28, "string": "Learning For each input X, we minimize the negative log likelihood of the gold structure Y * : J (X) = \u2212 log P (Y * | X) (4) Beam pruning As our model deals with O(n 2 ) possible argument spans and O(n) possible predicates, it needs to consider O(n 3 |L|) possible relations, which is computationally impractical."}, {"id": 29, "string": "To overcome this issue, we define two beams B a and B p for storing the candidate arguments and predicates, respectively."}, {"id": 30, "string": "The candidates in each beam are ranked by their unary score (\u03a6 a or \u03a6 p )."}, {"id": 31, "string": "The sizes of the beams are limited by \u03bb a n and \u03bb p n. Elements that fall out of the beam do not participate in computing the edge factors \u03a6 (l) rel , reducing the overall number of relational factors evaluated by the model to O(n 2 |L|)."}, {"id": 32, "string": "We also limit the maximum width of spans to a fixed number W (e.g."}, {"id": 33, "string": "W = 30), further reducing the number of computed unary factors to O(n)."}, {"id": 34, "string": "Neural Architecture Our model builds contextualized representations for argument spans a and predicate words p based on BiLSTM outputs ( Figure 2 ) and uses feedforward networks to compute the factor scores in \u03c6(p, a, l) described in Section 2 ( Figure 3 )."}, {"id": 35, "string": "Word-level contexts The bottom layer consists of pre-trained word embeddings concatenated with character-based representations, i.e."}, {"id": 36, "string": "for each token w i , we have x i = [WORDEMB(w i ); CHARCNN(w i )]."}, {"id": 37, "string": "We then contextualize each x i using an m-layered bidirectional LSTM with highway connections (Zhang et al., 2016) , which we denote asx i ."}, {"id": 38, "string": "Argument and predicate representation We build contextualized representations for all candidate arguments a \u2208 A and predicates p \u2208 P. The argument representation contains the following: end points from the BiLSTM outputs (x START(a) ,x END(a) ), a soft head word x h (a), and embedded span width features f (a), similar to ."}, {"id": 39, "string": "The predicate representation is simply the BiLSTM output at the position INDEX(p)."}, {"id": 40, "string": "g(a) =[x START(a) ;x END(a) ; x h (a); f (a)] (5) g(p) =x INDEX(p) (6) The soft head representation x h (a) is an attention mechanism over word inputs x in the argument span, where the weights e(a) are computed via a linear layer over the BiLSTM outputsx."}, {"id": 41, "string": "x h (a) = x START(a):END(a) e(s) (7) e(a) = SOFTMAX(w exSTART(a):END(a) ) (8) x START(a):END(a) is a shorthand for stacking a list of vectors x t , where START(a) \u2264 t \u2264 END(a)."}, {"id": 42, "string": "Scoring The scoring functions \u03a6 are implemented with feed-forward networks based on the predicate and argument representations g: \u03a6 a (a) =w a MLP a (g(a)) (9) \u03a6 p (p) =w p MLP p (g(p)) (10) \u03a6 (l) rel (a, p) =w (l) r MLP r ([g(a); g(p)]) (11) Experiments We experiment on the CoNLL 2005 (Carreras and M\u00e0rquez, 2005) and CoNLL 2012 (OntoNotes 5.0, (Pradhan et al., 2013)) benchmarks, using two SRL setups: end-to-end and gold predicates."}, {"id": 43, "string": "In the end-to-end setup, a system takes a tokenized sentence as input, and predicts all the predicates and their arguments."}, {"id": 44, "string": "Systems are evaluated on the micro-averaged F1 for correctly predicting (predicate, argument span, label) tuples."}, {"id": 45, "string": "For comparison with previous systems, we also report results with gold predicates, in which the complete set of predicates in the input sentence is given as well."}, {"id": 46, "string": "Other experimental setups and hyperparameteres are listed in Appendix A.1."}, {"id": 47, "string": "ELMo embeddings To further improve performance, we also add ELMo word representations (Peters et al., 2018) to the BiLSTM input (in the +ELMo rows)."}, {"id": 48, "string": "Since the contextualized representations ELMo provides can be applied to most previous neural systems, the improvement is orthogonal to our contribution."}, {"id": 49, "string": "In Table 1 and 2, we organize all the results into two categories: the comparable single model systems, and the mod-els augmented with ELMo or ensembling (in the PoE rows)."}, {"id": 50, "string": "End-to-end results As shown in Table 1 , 2 our joint model outperforms the previous best pipeline system  by an F1 difference of anywhere between 1.3 and 6.0 in every setting."}, {"id": 51, "string": "The improvement is larger on the Brown test set, which is out-of-domain, and the CoNLL 2012 test set, which contains nominal predicates."}, {"id": 52, "string": "On all datasets, our model is able to predict over 40% of the sentences completely correctly."}, {"id": 53, "string": "Results with gold predicates To compare with additional previous systems, we also conduct experiments with gold predicates by constraining our predicate beam to be gold predicates only."}, {"id": 54, "string": "As shown in Analysis Our model's architecture differs significantly from previous BIO systems in terms of both input and decision space."}, {"id": 55, "string": "To better understand our model's strengths and weaknesses, we perform three analyses following  and , studying (1) the effectiveness of beam Figure 4 shows the predicate and argument spans kept in the beam, sorted with their unary scores."}, {"id": 56, "string": "Our model efficiently prunes unlikely argument spans and predicates, significantly reduces the number of edges it needs to consider."}, {"id": 57, "string": "Figure 5 shows the recall of predicate words on the CoNLL 2012 development set."}, {"id": 58, "string": "By retaining \u03bb p = 0.4 predicates per word, we are able to keep over 99.7% argument-bearing predicates."}, {"id": 59, "string": "Compared to having a part-of-speech tagger (POS:X in Figure 5 ), our joint beam pruning allowing the model to have a soft trade-off between efficiency and recall."}, {"id": 60, "string": "4 Effectiveness of beam pruning Long-distance dependencies Figure 6 shows the performance breakdown by binned distance between arguments to the given predicates."}, {"id": 61, "string": "Our model is better at accurately predicting arguments that are farther away from the predicates, even  compared to an ensemble model  that has a higher overall F1."}, {"id": 62, "string": "This is very likely due to architectural differences; in a BIO tagger, predicate information passes through many LSTM timesteps before reaching a long-distance argument, whereas our architecture enables direct connections between all predicates-arguments pairs."}, {"id": 63, "string": "Agreement with syntax As mentioned in , their BIO-based SRL system has good agreement with gold syntactic span boundaries (94.3%) but falls short of previous syntaxbased systems (Punyakanok et al., 2004) ."}, {"id": 64, "string": "By directly modeling span information, our model achieves comparable syntactic agreement (95.0%) to Punyakanok et al."}, {"id": 65, "string": "(2004) Figure 5 : Recall of gold argument-bearing predicates on the CoNLL 2012 development data as we increase the number of predicates kept per word."}, {"id": 66, "string": "POS:X shows the gold predicate recall from using certain pos-tags identified by the NLTK part-ofspeech tagger (Bird, 2006) ."}, {"id": 67, "string": "tions of global structural constraints 5 compared to previous systems."}, {"id": 68, "string": "Our model made more constraint violations compared to previous systems."}, {"id": 69, "string": "For example, our model predicts duplicate core arguments 6 (shown in the U column in Table 3 ) more often than previous work."}, {"id": 70, "string": "This is due to the fact that our model uses independent classifiers to label each predicate-argument pair, making it difficult for them to implicitly track the decisions made for several arguments with the same predicate."}, {"id": 71, "string": "The Ours+decode row in Table 3 shows SRL performance after enforcing the U-constraint using dynamic programming  at decoding time."}, {"id": 72, "string": "Constrained decoding at test time is effective at eliminating all the core-role inconsistencies (shown in the U-column), but did not bring significant gain on the end result (shown 5 Punyakanok et al."}, {"id": 73, "string": "(2008) described a list of global constraints for SRL systems, e.g., there can be at most one core argument of each type for each predicate."}, {"id": 74, "string": "6 Arguments with labels ARG0,ARG1,."}, {"id": 75, "string": "."}, {"id": 76, "string": "."}, {"id": 77, "string": ",ARG5 and AA."}, {"id": 78, "string": "in SRL F1), which only evaluates the piece-wise predicate-argument structures."}, {"id": 79, "string": "Conclusion and Future Work We proposed a new SRL model that is able to jointly predict all predicates and argument spans, generalized from a recent coreference system ."}, {"id": 80, "string": "Compared to previous BIO systems, our new model supports joint predicate identification and is able to incorporate span-level features."}, {"id": 81, "string": "Empirically, the model does better at longrange dependencies and agreement with syntactic boundaries, but is weaker at global consistency, due to our strong independence assumption."}, {"id": 82, "string": "In the future, we could incorporate higher-order inference methods  to relax this assumption."}, {"id": 83, "string": "It would also be interesting to combine our span-based architecture with the selfattention layers (Tan et al., 2018; Strubell et al., 2018) for more effective contextualization."}], "headers": [{"section": "Introduction", "n": "1", "start": 0, "end": 12}, {"section": "Model", "n": "2", "start": 13, "end": 31}, {"section": "Neural Architecture", "n": "3", "start": 32, "end": 41}, {"section": "Experiments", "n": "4", "start": 42, "end": 53}, {"section": "Analysis", "n": "5", "start": 54, "end": 78}, {"section": "Conclusion and Future Work", "n": "6", "start": 79, "end": 83}], "figures": [{"filename": "../figure/image/1119-Figure3-1.png", "caption": "Figure 3: The span-pair classifier takes in predicate and argument representations as inputs, and computes a softmax over the label space L.", "page": 2, "bbox": {"x1": 88.8, "x2": 290.4, "y1": 246.72, "y2": 360.47999999999996}}, {"filename": "../figure/image/1119-Figure2-1.png", "caption": "Figure 2: Building the argument span representations g(a) from BiLSTM outputs. For clarity, we only show one BiLSTM layer and a small subset of the arguments.", "page": 2, "bbox": {"x1": 102.72, "x2": 494.88, "y1": 65.28, "y2": 184.79999999999998}}, {"filename": "../figure/image/1119-Figure5-1.png", "caption": "Figure 5: Recall of gold argument-bearing predicates on the CoNLL 2012 development data as we increase the number of predicates kept per word. POS:X shows the gold predicate recall from using certain pos-tags identified by the NLTK part-ofspeech tagger (Bird, 2006).", "page": 4, "bbox": {"x1": 89.28, "x2": 277.44, "y1": 73.44, "y2": 186.72}}, {"filename": "../figure/image/1119-Table3-1.png", "caption": "Table 3: Comparison on the CoNLL 05 development set against previous systems in terms of unlabeled agreement with gold constituency (Syn%) and each type of SRL-constraints violations (Unique core roles, Continuation roles and Reference roles).", "page": 4, "bbox": {"x1": 306.71999999999997, "x2": 526.0799999999999, "y1": 62.4, "y2": 165.12}}, {"filename": "../figure/image/1119-Figure6-1.png", "caption": "Figure 6: F1 by surface distance between predicates and arguments, showing degrading performance on long-range arguments.", "page": 4, "bbox": {"x1": 90.72, "x2": 276.0, "y1": 306.71999999999997, "y2": 408.0}}, {"filename": "../figure/image/1119-Table1-1.png", "caption": "Table 1: End-to-end SRL results for CoNLL 2005 and CoNLL 2012, compared to previous systems. CoNLL 05 contains two test sets: WSJ (in-domain) and Brown (out-of-domain).", "page": 3, "bbox": {"x1": 72.0, "x2": 526.0799999999999, "y1": 67.2, "y2": 144.96}}, {"filename": "../figure/image/1119-Table2-1.png", "caption": "Table 2: Experiment results with gold predicates.", "page": 3, "bbox": {"x1": 72.0, "x2": 296.15999999999997, "y1": 198.72, "y2": 327.36}}, {"filename": "../figure/image/1119-Figure4-1.png", "caption": "Figure 4: Top: The candidate arguments and predicates in the argument beam Ba and predicate beam Bp after pruning, along with their unary scores. Bottom: Predicted SRL relations with two identified predicates and their arguments.", "page": 3, "bbox": {"x1": 306.71999999999997, "x2": 526.0799999999999, "y1": 198.72, "y2": 357.12}}, {"filename": "../figure/image/1119-Figure1-1.png", "caption": "Figure 1: A comparison of our span-graph structure (top) versus BIO-based SRL (bottom).", "page": 0, "bbox": {"x1": 319.68, "x2": 510.24, "y1": 226.07999999999998, "y2": 357.12}}]}