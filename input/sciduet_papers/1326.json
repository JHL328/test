{"title": "Weak Semi-Markov CRFs for NP Chunking in Informal Text", "abstract": "This paper introduces a new annotated corpus based on an existing informal text corpus: the NUS SMS Corpus (Chen and Kan, 2013). The new corpus includes 76,490 noun phrases from 26,500 SMS messages, annotated by university students. We then explored several graphical models, including a novel variant of the semi-Markov conditional random fields (semi-CRF) for the task of noun phrase chunking. We demonstrated through empirical evaluations on the new dataset that the new variant yielded similar accuracy but ran in significantly lower running time compared to the conventional semi-CRF.", "text": [{"id": 0, "string": "Introduction Processing user-generated text data is getting more popular recently as a way to gather information, such as collecting facts about certain events (Ritter et al., 2015) , gathering and identifying user profiles (Layton et al., 2010; Li et al., 2014; Spitters et al., 2015) , or extracting information in open domain (Ritter et al., 2012; ."}, {"id": 1, "string": "Most recent work focus on the texts generated through Twitter, which, due to the design of Twitter, contain a lot of announcement-like messages mostly intended for general public."}, {"id": 2, "string": "In contrast, SMS was designed as a way to communicate short personal messages to a known person, and hence SMS messages tend to be more conversational and more informal compared to tweets."}, {"id": 3, "string": "As conversational texts, SMS data often contains references to named entities such as people and locations relevant to certain events."}, {"id": 4, "string": "Recognizing those Hmm Dr teh says the research presentation should still prepare, butshe's not to sure whether they'd time to present Figure 1 : Sample SMS, with NPs underlined references will be useful for further NLP tasks."}, {"id": 5, "string": "One way to recognize those named entities is to first create a list of candidates, which can be further filtered to get the desired named entities."}, {"id": 6, "string": "Nadeau (Nadeau and Sekine, 2007) lists several methods that work upon candidates for NER."}, {"id": 7, "string": "As all named entities are nouns, recognizing noun phrases (NP) is therefore a task that can be potentially useful for further steps in the NLP pipeline to build upon."}, {"id": 8, "string": "Figure 1 shows an example SMS message within which noun phrases are highlighted."}, {"id": 9, "string": "As can be seen from this example, recognizing the NP information on such a dataset presents some additional challenges over conventional NP recognition tasks."}, {"id": 10, "string": "Specifically, the texts are highly informal and noisy, with misspelling errors and without grammatical structures."}, {"id": 11, "string": "The correct casing and punctuation information is often missing."}, {"id": 12, "string": "The lack of spaces between adjacent words makes the detection of NP boundaries more challenging."}, {"id": 13, "string": "Furthermore, the lack of available annotated data for such informal datasets prevents researchers from understanding what effective models can be used to resolve the above issues."}, {"id": 14, "string": "In this work, we focus on tackling these issues while making the following two main contributions: \u2022 We build a new corpus of SMS data that is fully annotated with noun phrase information."}, {"id": 15, "string": "\u2022 We propose and build a new variant of semi-Markov CRF (Sarawagi and Cohen, 2004) for the task of NP chunking on our corpus, which is faster and yields a performance similar to the conventional semi-Markov CRF models."}, {"id": 16, "string": "NP-annotated SMS Corpus Our text corpus comes from the NUS SMS Corpus (Chen and Kan, 2013) , containing 55,835 SMS messages from university students, mostly in English."}, {"id": 17, "string": "We used the 2011 version of the corpus, containing 45,718 messages, as it is more relevant to modern phone models using full keyboard layout."}, {"id": 18, "string": "We note that there are a small portion of the messages written in non-English language, such as Tamil and Chinese."}, {"id": 19, "string": "As we are focusing on English, we excluded messages written by non-native English speakers based on the metadata (21.3% of all messages)."}, {"id": 20, "string": "We also excluded messages which contain only one word (6.1%) and we remove duplicate messages (8.1%)."}, {"id": 21, "string": "1 We assigned the remaining 27,700 messages to 64 university students who conduct annotations, each annotating 500 with 100 messages co-annotated by two other annotators."}, {"id": 22, "string": "After manual verification we excluded annotations with low quality from 3 students."}, {"id": 23, "string": "We used the resulting 26,500 messages as our dataset."}, {"id": 24, "string": "The students were asked to annotate the toplevel noun phrases found in each message using the BRAT rapid annotation tool 2 , where they were instructed to highlight character spans to be marked as noun phrases."}, {"id": 25, "string": "The number of noun phrases per message can be found in Table 1 ."}, {"id": 26, "string": "Due to the noisy nature of SMS messages, there may not be proper capitalization or punctuation, and in some cases there might be missing spaces between words."}, {"id": 27, "string": "Figure 1 shows a sample SMS message taken from the corpus."}, {"id": 28, "string": "We can see that \"Dr teh\" is not properly capitalized and \"she\" in \"butshe's\" is missing spaces around it."}, {"id": 29, "string": "NPs which do not have clear boundaries (improper NPs) constitutes 4.0% of all NPs."}, {"id": 30, "string": "We then use this dataset to evaluate some models on base NP chunking task, where, given a text, the system should return a list of character spans denoting the noun phrases found in the text."}, {"id": 31, "string": "Models In this paper, we will build our models based on a class of discriminative graphical models, namely conditional random fields (CRFs) (Lafferty et al., 2001) , for extracting NPs."}, {"id": 32, "string": "The edges in the graph represents the dependencies between states and the features are defined over each edge in the graph."}, {"id": 33, "string": "Though CRFs are undirected graphical models, we can use directed acyclic graphs with a root, a leaf, and some inner nodes to represent label sequences 3 ."}, {"id": 34, "string": "A path in the graph from the root to the leaf represents one possible label assignment to the input."}, {"id": 35, "string": "In the labeled instance, there will be only one single path from the root to the leaf, while for the unlabeled instance, the graph will compactly encode all possible label assignments."}, {"id": 36, "string": "The learning procedure is essentially the process that tries to tune the feature weights such that the true structures get assigned higher weights as compared to all other alternative structures in the graph."}, {"id": 37, "string": "In general, a CRF tries to maximize the following objective function: L(T ) = (x,y)\u2208T \uf8ee \uf8f0 e\u2208E(x,y) w T f (e) \u2212 log Z w (x) \uf8f9 \uf8fb \u2212 \u03bb||w|| 2 (1) where T is the training set, (x, y) is a training instance consisting of the sentence x and the label sequence y \u2208 Y n for a label set Y, w is the feature weight vector, E(x, y) is the set of edges which defines the input x labeled with the label sequence y, f (e) is the feature vector of the edge e, Z w (x) is the normalization term which sums over all possible paths from the root to the leaf node, and \u03bb is the regularization parameter."}, {"id": 38, "string": "The set of edges and features defined in each model affects the feature expectation and the normalization term."}, {"id": 39, "string": "Computation of the normalization term, being the highest in time complexity, will determine the overall complexity of training the model."}, {"id": 40, "string": "The set of edges and the normalization term in each model will be described in the following sections."}, {"id": 41, "string": "Linear CRF A linear-chain CRF, or linear CRF is a standard version of CRF which was introduced in Lafferty et al."}, {"id": 42, "string": "(2001) , where each word in the sentence is given a set of nodes representing the possible labels, and edges are present between any two nodes from adjacent words, forming a trellis graph."}, {"id": 43, "string": "Here we consider only the first-order linear CRF."}, {"id": 44, "string": "The normalization term Z w (x) is calculated as: Z w (x) = y exp (y ,y,i)\u2208E(x,y) w T f x (y , y, i) (2) where f x (y , y, i) represents the feature vector on the edge connecting state y at position i \u2212 1 to state y at position i."}, {"id": 45, "string": "The time complexity of the inference procedure for this model is O(n |Y| 2 )."}, {"id": 46, "string": "Semi-CRF In semi-CRF (Sarawagi and Cohen, 2004) , in addition to the edges defined in linear CRF, there are additional edges from a node to all nodes up to L next words away, representing a segment within which the words will be labeled with a single label."}, {"id": 47, "string": "The normalization term Z w (x) is calculated as: Z w (x) = y\u2208Y n exp (y ,y,i\u2212k,i)\u2208E(x,y) w T g x (y , y, i \u2212 k, i) (3) where g x (y , y, i\u2212k, i) represents the feature vector on the edge connecting state y at position i \u2212 k to state y at position i."}, {"id": 48, "string": "The time complexity for this model is O(nL |Y| 2 )."}, {"id": 49, "string": "Weak Semi-CRF Note that in semi-CRF, each node is connected to L \u00d7 |Y| next nodes."}, {"id": 50, "string": "Intuitively, the model tries to decide the next segment length and type at the same time."}, {"id": 51, "string": "We now propose a weaker variant that makes the two decisions separately by restricting each node to connect to either only the nodes of the same label up to L next words away, or to all the nodes only in the next word."}, {"id": 52, "string": "We call this variant Weak Semi-CRF."}, {"id": 53, "string": "To implement this, we need to split the original nodes into Begin and End nodes, representing the start and end of a segment."}, {"id": 54, "string": "The End nodes connect only to the very next Begin nodes of any label, while the Begin nodes connect only to the End nodes of same label up to next L words."}, {"id": 55, "string": "We denote the set of the earlier edges as E A (x, y) and the latter edges as E J (x, y)."}, {"id": 56, "string": "The normalization term Z w (x) is then: Z w (x) = y\u2208Y n exp (y ,y,i)\u2208E A (x,y) w T f x (y , y, i) + (y,i\u2212k,i)\u2208E J (x,y) w T g x (y, i \u2212 k, i) (4) where g x (y, i \u2212 k, i) represents the feature vector on the edge connecting the Begin node with state y at position i \u2212 k to the End node with the same state y at position i."}, {"id": 57, "string": "Note that, different from the g x function defined in Equation (3) , this new g x function is defined over a single (current) y label only, making the time complexity O(n |Y| 2 + nL |Y|)."}, {"id": 58, "string": "Theoretically this model is slightly more efficient than the conventional semi-CRF model."}, {"id": 59, "string": "Unlike conventional (first-order) semi-Markov CRF, this new model does not allow us to capture the dependencies between one segment and its adjacent segment's label information."}, {"id": 60, "string": "We argue that, however, such dependencies can be less crucial for our task."}, {"id": 61, "string": "We will empirically assess this aspect through experiments."}, {"id": 62, "string": "Figure 2 illustrates the differences among the three models."}, {"id": 63, "string": "Features In linear CRF, the baseline feature set considers the previous word, current word, and the tag transition."}, {"id": 64, "string": "In semi-CRF, following Sarawagi and Cohen (2004) we put each word which is not part of a noun phrase in its own segment, and put each noun phrase in one segment, possibly spanning over multiple words."}, {"id": 65, "string": "Here we set L = 6 and ignored NPs with more than six words during training, which is less than 0.5% of all NPs."}, {"id": 66, "string": "For each segment, we defined the following features as the baseline: (1) Linear CRF Semi CRF Weak Semi-CRF indexed words inside current segment, running from the start and from the end of the segment, (2) the word before and after current segment, and (3) the labels of previous segment and current segment."}, {"id": 67, "string": "In weak semi-CRF we use the same feature set as semi-CRF, adjusting the features accordingly where segment-specific features (1) are defined only in the Begin-End edges, and transition features (3) are defined only in the End-Begin edges."}, {"id": 68, "string": "For each model we then add the character prefixes and suffixes up to length 3 for each word (+a), Brown cluster (Brown et al., 1992) for current word (+b), and word shapes (+s)."}, {"id": 69, "string": "For Brown cluster features we used 100 clusters trained on the whole NUS SMS Corpus."}, {"id": 70, "string": "The cluster information is then used directly as a feature."}, {"id": 71, "string": "Word shapes can be considered a generic representation of words that retains only the \"shape\" information, such as whether it starts with capital letter or whether it contains digits."}, {"id": 72, "string": "The Brown clusters and word shapes features are applied to each of the word features described in each model."}, {"id": 73, "string": "Experiments All models were built by us using Java, and were optimized with L-BFGS."}, {"id": 74, "string": "Models are all tuned in the development set for optimal \u03bb."}, {"id": 75, "string": "The optimal \u03bb values are noted in Table 2 ."}, {"id": 76, "string": "Since the models that we consider are all wordbased 4 , we tokenize the corpus using a regex-based tokenizer similar to the wordpunct_tokenize function in Python NLTK package."}, {"id": 77, "string": "We also included some rules to consider special anonymization tokens in the SMS dataset (Chen and Kan, 2013) ."}, {"id": 78, "string": "The gold character spans are converted into word labels in BIO format, reducing or extending the character spans as necessary to the closest word boundaries."}, {"id": 79, "string": "The converted annotations are regarded as gold word spans."}, {"id": 80, "string": "Note that this conversion is lossy due to the presence of improper NPs, which makes it impossible for the converted format to represent the original gold standard."}, {"id": 81, "string": "We evaluated the models in the original characterlevel spans and also in the converted word-level spans, to see the impact of the lossy conversion on the scores."}, {"id": 82, "string": "In character-level evaluation, the system output is converted back into character boundaries and compared with the original gold standard, while in the word-level evaluation, the system output is compared directly with the gold word spans."}, {"id": 83, "string": "For this reason, we anticipate that the scores in word-level evaluation will be higher than in the character-level evaluation."}, {"id": 84, "string": "The results are shown in Table 3 ."}, {"id": 85, "string": "The scores for \"Gold\" in the character-level evaluation mark the upperbound of word-based models due to the presence of improper NPs."}, {"id": 86, "string": "The average time per training iteration on the base models is 1.311s, 2.072s, and 1.811s respectively for Linear CRF, Semi-CRF, and Weak Semi-CRF."}, {"id": 87, "string": "Discussion First, we see that the two variants of semi-CRF models perform better compared to the baseline linear CRF model, showing the benefit of using segment features over only single word features."}, {"id": 88, "string": "It is also interesting that, while being a weaker version of the semi-CRF, the weak semi-CRF can actually perform in the same level within 95% confidence interval as the conventional semi-CRF."}, {"id": 89, "string": "This shows that some of the dependencies in the conventional semi-CRF do not really contribute to the strength of semi-CRF over standard linear CRF."}, {"id": 90, "string": "As noted in Section 3.3, weak semi-CRF makes the decision on the segment type and length separately."}, {"id": 91, "string": "This means there is enough information in the local features to decide the segment type and length separately, and so we can remove some combined features while retaining the same performance."}, {"id": 92, "string": "This result, coupled with the fact that the weak semi-CRF requires 12.5% less time than the conventional semi-CRF (1.811s vs 2.072s), shows the po-tentials of using this weak semi-CRF as an alternative of the conventional semi-CRF."}, {"id": 93, "string": "With more label types (here only two), the difference will be larger, since the weak semi-CRF is linear in number of label types, while conventional semi-CRF is quadratic."}, {"id": 94, "string": "6 Related Work Ritter et al."}, {"id": 95, "string": "(2011) previously showed that off-theshelf NP-chunker performs worse on informal text."}, {"id": 96, "string": "Then they trained a linear-CRF model on additional in-domain data, reducing the error up to 22%."}, {"id": 97, "string": "However no results on semi-CRF was given."}, {"id": 98, "string": "Semi-CRF has proven effective in chunking tasks."}, {"id": 99, "string": "Other variants of semi-CRF models also exist."}, {"id": 100, "string": "Nguyen et al."}, {"id": 101, "string": "(2014) explored the use of higherorder dependencies to improve the performance of semi-CRF models on synthetic data and on handwriting recognition."}, {"id": 102, "string": "They exploited the sparsity of label sequence in order to make the training efficient."}, {"id": 103, "string": "It is also known that feature selection is an important aspect when trying to use semi-CRF models to improve on the linear CRF."}, {"id": 104, "string": "Andrew (2006) reported an error reduction of up to 25% when using features that are best exploited by semi-CRF."}, {"id": 105, "string": "Conclusion and Future Work In this paper we present a new NP-annotated SMS corpus, together with a novel variant of the semi-CRF model, which runs in significantly lower time while maintaining similar accuracy on the NP chunking task on the new dataset."}, {"id": 106, "string": "Future work includes the application of the weak semi-CRF model to other structured prediction problems, as well as performing investigations on handling other types of informal or noisy texts such as speech transcripts."}, {"id": 107, "string": "We make the code and data available for download at http://statnlp.org/research/ie/."}], "headers": [{"section": "Introduction", "n": "1", "start": 0, "end": 15}, {"section": "NP-annotated SMS Corpus", "n": "2", "start": 16, "end": 30}, {"section": "Models", "n": "3", "start": 31, "end": 40}, {"section": "Linear CRF", "n": "3.1", "start": 41, "end": 45}, {"section": "Semi-CRF", "n": "3.2", "start": 46, "end": 48}, {"section": "Weak Semi-CRF", "n": "3.3", "start": 49, "end": 62}, {"section": "Features", "n": "4", "start": 63, "end": 72}, {"section": "Experiments", "n": "5", "start": 73, "end": 86}, {"section": "Discussion", "n": "5.1", "start": 87, "end": 103}, {"section": "Conclusion and Future Work", "n": "7", "start": 104, "end": 107}], "figures": [{"filename": "../figure/image/1326-Table3-1.png", "caption": "Table 3: Scores on test set (both character-level and word-level evaluation) using optimal \u03bb. +a, +b, and +s refer to the affix, Brown cluster, and word shape features respectively. Best F1 scores are underlined, and values which are not significantly different in 95% confidence interval are in bold", "page": 4, "bbox": {"x1": 72.96, "x2": 298.08, "y1": 72.0, "y2": 362.4}}, {"filename": "../figure/image/1326-Table1-1.png", "caption": "Table 1: Number of messages, NPs, number of improper NPs (as percentage in brackets), which are NPs that do not have clear boundaries, and number of tokens.", "page": 1, "bbox": {"x1": 330.71999999999997, "x2": 522.24, "y1": 72.0, "y2": 133.44}}, {"filename": "../figure/image/1326-Table2-1.png", "caption": "Table 2: Tuned regularization parameter \u03bb from the set {0.125, 0.25, 0.5, 1.0, 2.0} for various feature sets. +a, +b, and +s refer to the affix, Brown cluster, and word shape features respectively.", "page": 3, "bbox": {"x1": 312.96, "x2": 537.12, "y1": 221.76, "y2": 322.08}}, {"filename": "../figure/image/1326-Figure2-1.png", "caption": "Figure 2: Graphical illustrations of the differences between three models. The bold arrows represent the path in each model to label \u201cDr Teh\u201d as a noun phrase. For Linear CRF, this is a simplified diagram; in the implementation we used the \u201cBIO\u201d approach to represent text chunks. The underlined nodes in Weak Semi-CRF are the Begin nodes.", "page": 3, "bbox": {"x1": 96.0, "x2": 523.1999999999999, "y1": 84.0, "y2": 160.32}}]}