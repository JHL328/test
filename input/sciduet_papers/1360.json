{"title": "Pre-training on High-Resource Speech Recognition Improves Low-Resource Speech-to-Text Translation", "abstract": "We present a simple approach to improve direct speech-to-text translation (ST) when the source language is low-resource: we pre-train the model on a high-resource automatic speech recognition (ASR) task, and then fine-tune its parameters for ST. We demonstrate that our approach is effective by pre-training on 300 hours of English ASR data to improve Spanish-English ST from 10.8 to 20.2 BLEU when only 20 hours of Spanish-English ST training data are available. Through an ablation study, we find that the pre-trained encoder (acoustic model) accounts for most of the improvement, despite the fact that the shared language in these tasks is the target language text, not the source language audio. Applying this insight, we show that pre-training on ASR helps ST even when the ASR language differs from both source and target ST languages: pre-training on French ASR also improves Spanish-English ST. Finally, we show that the approach improves performance on a true low-resource task: pre-training on a combination of English ASR and French ASR improves Mboshi-French ST, where only 4 hours of data are available, from 3.5 to 7.1 BLEU.", "text": [{"id": 0, "string": "Introduction Speech-to-text Translation (ST) has many potential applications for low-resource languages: for example in language documentation, where the source language is often unwritten or endangered (Besacier et al., 2006; Martin et al., 2015; Adams et al., 2016a,b; Anastasopoulos and Chiang, 2017) ; or in crisis relief, where emergency workers might need to respond to calls or requests in a foreign language (Munro, 2010) ."}, {"id": 1, "string": "Traditional ST is a pipeline of automatic speech recognition (ASR) and machine translation (MT), and thus requires transcribed source audio to train ASR and parallel text to train MT."}, {"id": 2, "string": "These resources are often unavailable for low-resource languages, but for our potential applications, there may be some source language audio paired with target language text translations."}, {"id": 3, "string": "In these scenarios, end-to-end ST is appealing."}, {"id": 4, "string": "Recently, Weiss et al."}, {"id": 5, "string": "(2017) showed that endto-end ST can be very effective, achieving an impressive BLEU score of 47.3 on Spanish-English ST."}, {"id": 6, "string": "But this result required over 150 hours of translated audio for training, still a substantial resource requirement."}, {"id": 7, "string": "By comparison, a similar system trained on only 20 hours of data for the same task achieved a BLEU score of 5.3 (Bansal et al., 2018) ."}, {"id": 8, "string": "Other low-resource systems have similarly low accuracies (Anastasopoulos and Chiang, 2018; B\u00e9rard et al., 2018) ."}, {"id": 9, "string": "To improve end-to-end ST in low-resource settings, we can try to leverage other data resources."}, {"id": 10, "string": "For example, if we have transcribed audio in the source language, we can use multi-task learning to improve ST (Anastasopoulos and Chiang, 2018; Weiss et al., 2017; B\u00e9rard et al., 2018) ."}, {"id": 11, "string": "But source language transcriptions are unlikely to be available in our scenarios of interest."}, {"id": 12, "string": "Could we improve low-resource ST by leveraging data from a high-resource language?"}, {"id": 13, "string": "For ASR, training a single model on multiple languages can be effective for all of them (Toshniwal et al., 2018b; Deng et al., 2013) ."}, {"id": 14, "string": "For MT, transfer learning (Thrun, 1995) has been very effective: pretraining a model for a high-resource language pair and transferring its parameters to a low-resource language pair when the target language is shared (Zoph et al., 2016; Johnson et al., 2017) ."}, {"id": 15, "string": "Inspired by these successes, we show that low-resource ST can leverage transcribed audio in a high-resource target language, or even a different language altogether, simply by pre-training a model for the high-resource ASR task, and then transferring and fine-tuning some or all of the model's parameters for low-resource ST. We first test our approach using Spanish as the source language and English as the target."}, {"id": 16, "string": "After training an ASR system on 300 hours of English, fine-tuning on 20 hours of Spanish-English yields a BLEU score of 20.2, compared to only 10.8 for an ST model without ASR pre-training."}, {"id": 17, "string": "Analyzing this result, we discover that the main benefit of pre-training arises from the transfer of the encoder parameters, which model the input acoustic signal."}, {"id": 18, "string": "In fact, this effect is so strong that we also obtain improvements by pre-training on a language that differs from both the source and the target: pre-training on French and fine-tuning on Spanish-English."}, {"id": 19, "string": "We hypothesize that pre-training the encoder parameters, even on a different language, allows the model to better learn about linguistically meaningful phonetic variation while normalizing over acoustic variability such as speaker and channel differences."}, {"id": 20, "string": "We conclude that the acousticphonetic learning problem, rather than translation itself, is one of the main difficulties in low-resource ST. A final set of experiments confirm that ASR pretraining also helps on another language pair where the input is truly low-resource: Mboshi-French."}, {"id": 21, "string": "Method For both ASR and ST, we use an encoder-decoder model with attention adapted from Weiss et al."}, {"id": 22, "string": "(2017), B\u00e9rard et al."}, {"id": 23, "string": "(2018) and Bansal et al."}, {"id": 24, "string": "(2018) , as shown in Figure 1 ."}, {"id": 25, "string": "We use the same model architecture for all our models, allowing us to conveniently transfer parameters between them."}, {"id": 26, "string": "We also constrain the hyper-parameter search to fit a model into a single Titan X GPU, allowing us to maximize available compute resources."}, {"id": 27, "string": "We use a pre-trained English ASR model to initialize training of Spanish-English ST models, and a pre-trained French ASR model to initialize training of Mboshi-French ST models."}, {"id": 28, "string": "During ST training, all model parameters are updated."}, {"id": 29, "string": "In these configurations, the decoder shares the same vocabulary across the ASR and ST tasks."}, {"id": 30, "string": "This is practical for settings where the target text language is highresource with ASR data available."}, {"id": 31, "string": "In settings where both ST languages are lowresource, ASR data may only be available in a third language."}, {"id": 32, "string": "To test whether transfer learning will help in this setting, we use a pre-trained French ASR model to train Spanish-English ST models; and English ASR for Mboshi-French models."}, {"id": 33, "string": "In these cases, the ST languages are different from the ASR language, so we can only transfer the encoder parameters of the ASR model, since the dimensions of the decoder's output softmax layer are indexed by the vocabulary, which is not shared."}, {"id": 34, "string": "1 Sharing only the speech encoder parameters is much easier, since the speech input can be preprocessed in the same manner for all languages."}, {"id": 35, "string": "This form of transfer learning is more flexible, as there are no constraints on the ASR language used."}, {"id": 36, "string": "3 Experimental Setup 3.1 Data sets English ASR."}, {"id": 37, "string": "We use the Switchboard Telephone speech corpus (Godfrey and Holliman, 1993) , which consists of around 300 hours of English speech and transcripts, split into 260k utterances."}, {"id": 38, "string": "The development set consists of 5 hours that we removed from the training set, split into 4k utterances."}, {"id": 39, "string": "French ASR."}, {"id": 40, "string": "We use the French speech corpus from the GlobalPhone collection (Schultz, 2002) , which consists of around 20 hours of high quality read speech and transcripts, split into 9k utterances."}, {"id": 41, "string": "The development set consists of 2 hours, split into 800 utterances."}, {"id": 42, "string": "Spanish-English ST. We use the Fisher Spanish speech corpus (Graff et al., 2010) , which consists of 160 hours of telephone speech in a variety of Spanish dialects, split into 140K utterances."}, {"id": 43, "string": "To simulate low-resource conditions, we construct smaller train-ing corpora consisting of 50, 20, 10, 5, or 2.5 hours of data, selected at random from the full training data."}, {"id": 44, "string": "The development and test sets each consist of around 4.5 hours of speech, split into 4K utterances."}, {"id": 45, "string": "We do not use the corresponding Spanish transcripts; our target text consists of English translations that were collected through crowdsourcing (Post et al., 2013 (Post et al., , 2014 ."}, {"id": 46, "string": "Mboshi-French ST. Mboshi is a Bantu language spoken in the Republic of Congo, with around 160,000 speakers."}, {"id": 47, "string": "2 We use the Mboshi-French parallel corpus (Godard et al., 2018) , which consists of around 4 hours of Mboshi speech, split into a training set of 5K utterances and a development set of 500 utterances."}, {"id": 48, "string": "Since this corpus does not include a designated test set, we randomly sampled and removed 200 utterances from training to use as a development set, and use the designated development data as a test set."}, {"id": 49, "string": "Preprocessing Speech."}, {"id": 50, "string": "We convert raw speech input to 13dimensional MFCCs using Kaldi (Povey et al., 2011) ."}, {"id": 51, "string": "3 We also perform speaker-level mean and variance normalization."}, {"id": 52, "string": "Text."}, {"id": 53, "string": "The target text of the Spanish-English data set contains 1.5M word tokens and 17K word types."}, {"id": 54, "string": "If we model text as sequences of words, our model cannot produce any of the unseen word types in the test data and is penalized for this, but it can be trained very quickly (Bansal et al., 2018) ."}, {"id": 55, "string": "If we instead model text as sequences of characters as done by Weiss et al."}, {"id": 56, "string": "(2017) , we would have 7M tokens and 100 types, resulting in a model that is open-vocabulary, but very slow to train (Bansal et al., 2018) ."}, {"id": 57, "string": "As an effective middle ground, we use byte pair encoding (BPE; Sennrich et al., 2016) to segment each word into subwords, each of which is a character or a high-frequency sequence of characters-we use 1000 of these high-frequency sequences."}, {"id": 58, "string": "Since the set of subwords includes the full set of characters, the model is still open vocabulary; but it results in a text with only 1.9M tokens and just over 1K types, which can be trained almost as fast as the word-level model."}, {"id": 59, "string": "The vocabulary for BPE depends on the fre-quency of character sequences, so it must be computed with respect to a specific corpus."}, {"id": 60, "string": "For English, we use the full 160-hour Spanish-English ST target training text."}, {"id": 61, "string": "For French, we use the Mboshi-French ST target training text."}, {"id": 62, "string": "Model architecture for ASR and ST Speech encoder."}, {"id": 63, "string": "As shown schematically in Figure 1, MFCC feature vectors, extracted using a window size of 25 ms and a step size of 10ms, are fed into a stack of two CNN layers, with 128 and 512 filters with a filter width of 9 frames each."}, {"id": 64, "string": "In each CNN layer we stride with a factor of 2 along time, apply a ReLU activation (Nair and Hinton, 2010) , and apply batch normalization (Ioffe and Szegedy, 2015) ."}, {"id": 65, "string": "The output of the CNN layers is fed into a three-layer bi-directional long short term memory network (LSTM; Hochreiter and Schmidhuber, 1997); each hidden layer has 512 dimensions."}, {"id": 66, "string": "Text decoder."}, {"id": 67, "string": "At each time step, the decoder chooses the most probable token from the output of a softmax layer produced by a fully-connected layer, which in turn receives the current state of a recurrent layer computed from previous time steps and an attention vector computed over the input."}, {"id": 68, "string": "Attention is computed using the global attentional model with general score function and inputfeeding, as described in Luong et al."}, {"id": 69, "string": "(2015) ."}, {"id": 70, "string": "The predicted token is then fed into a 128-dimensional embedding layer followed by a three-layer LSTM to update the recurrent state; each hidden state has 256 dimensions."}, {"id": 71, "string": "While training, we use the predicted token 20% of the time as input to the next decoder step and the training token for the remaining 80% of the time (Williams and Zipser, 1989) ."}, {"id": 72, "string": "At test time we use beam decoding with a beam size of 5 and length normalization (Wu et al., 2016) with a weight of 0.6."}, {"id": 73, "string": "Training and implementation."}, {"id": 74, "string": "Parameters for the CNN and RNN layers are initialized using the scheme from (He et al., 2015) ."}, {"id": 75, "string": "For the embedding and fully-connected layers, we use Chainer's (Tokui et al., 2015) default initialition."}, {"id": 76, "string": "We regularize using dropout (Srivastava et al., 2014) , with a ratio of 0.3 over the embedding and LSTM layers (Gal, 2016) , and a weight decay rate of 0.0001."}, {"id": 77, "string": "The parameters are optimized using Adam (Kingma and Ba, 2015) , with a starting alpha of 0.001."}, {"id": 78, "string": "Following some preliminary experimentation on our development set, we add Gaussian noise with standard deviation of 0.25 to the MFCC features during training, and drop frames with a probability of 0.10."}, {"id": 79, "string": "After 20 epochs, we corrupt the true decoder labels by sampling a random output label with a probability of 0.3."}, {"id": 80, "string": "Our code is implemented in Chainer (Tokui et al., 2015) and is freely available."}, {"id": 81, "string": "4 Evaluation Metrics."}, {"id": 82, "string": "We report BLEU (Papineni et al., 2002) for all our models."}, {"id": 83, "string": "5 In low-resource settings, BLEU scores tend to be low, difficult to interpret, and poorly correlated with model performance."}, {"id": 84, "string": "This is because BLEU requires exact four-gram matches only, but low four-gram accuracy may obscure a high unigram accuracy and inexact translations that partially capture the semantics of an utterance, and these can still be very useful in situations like language documentation and crisis response."}, {"id": 85, "string": "Therefore, we also report word-level unigram precision and recall, taking into account stem, synonym, and paraphrase matches."}, {"id": 86, "string": "To compute these scores, we use METEOR (Lavie and Agarwal, 2007) with default settings for English and French."}, {"id": 87, "string": "6 For example, METEOR assigns \"eat\" a recall of 1 against reference \"eat\" and a recall of 0.8 against reference \"feed\", which it considers a synonym match."}, {"id": 88, "string": "Naive baselines."}, {"id": 89, "string": "We also include evaluation scores for a naive baseline model that predicts the K most frequent words of the training set as a bag of words for each test utterance."}, {"id": 90, "string": "We set K to be the value at which precision/recall are most similar, which is always between 5 and 20 words."}, {"id": 91, "string": "This provides an empirical lower bound on precision and recall, since we would expect any usable model to outperform a system that does not even depend on the input utterance."}, {"id": 92, "string": "We do not compute BLEU for these baselines, since they do not predict sequences, only bags of words."}, {"id": 93, "string": "ment data in Table 1 ."}, {"id": 94, "string": "7 We denote each ASR model by L-Nh, where L is a language code and N is the size of the training set in hours."}, {"id": 95, "string": "For example, en-300h denotes an English ASR model trained on 300 hours of data."}, {"id": 96, "string": "Training ASR models for state-of-the-art performance requires substantial hyper-parameter tuning and long training times."}, {"id": 97, "string": "Since our goal is simply to see whether pre-training is useful, we stopped pretraining our models after around 30 epochs (3 days) to focus on transfer experiments."}, {"id": 98, "string": "As a consequence, our ASR results are far from state-of-the-art: current end-to-end Kaldi systems obtain 16% WER on Switchboard train-dev, and 22.7% WER on the French Globalphone dev set."}, {"id": 99, "string": "8 We believe that better ASR pre-training may produce better ST results, but we leave this for future work."}, {"id": 100, "string": "Spanish-English ST In the following, we denote an ST model by S-T-Nh, where S and T are source and target language codes, and N is the size of the training set in hours."}, {"id": 101, "string": "For example, sp-en-20h denotes a Spanish-English ST model trained using 20 hours of data."}, {"id": 102, "string": "We use the code mb for Mboshi and fr for French."}, {"id": 103, "string": "Figure 2 shows the BLEU and unigram precision/recall scores on the development set for baseline Spanish-English ST models and those trained after initializing with the en-300h model."}, {"id": 104, "string": "Corresponding results on the test set (Table 2) previous results (Bansal et al., 2018) using the same train/test splits, primarily due to better regularization and modeling of subwords rather than words."}, {"id": 105, "string": "Yet transfer learning still substantially improves over these strong baselines."}, {"id": 106, "string": "For sp-en-20h, transfer learning improves dev set BLEU from 10.8 to 19.9, precision from 41% to 51%, and recall from 38% to 49%."}, {"id": 107, "string": "For sp-en-50h, transfer learning improves BLEU from 23.3 to 27.8, precision from 54% to 58%, and recall from 51% to 56%."}, {"id": 108, "string": "Using English ASR to improve ST Very low-resource: 10 hours or less of ST training data."}, {"id": 109, "string": "Figure 2 shows that without transfer learning, ST models trained on less than 10 hours of data struggle to learn, with precision/recall scores close to or below that of the naive baseline."}, {"id": 110, "string": "But with transfer learning, we see gains in precision and recall of between 10 and 20 points."}, {"id": 111, "string": "We also see that with transfer learning, a model trained on only 5 hours of ST data achieves a BLEU of 9.1, nearly as good as the 10.8 of a model trained on 20 hours of ST data without transfer learning."}, {"id": 112, "string": "In other words, fine-tuning an English ASR modelwhich is relatively easy to obtain-produces similar results to training an ST model on four times as N = 0 2.5 5 10 20 50 base 0 2.1 1.8 2.1 10.8 22.7 +asr 0.5 5.7 9.1 14.5 20.2 28.2  much data, which may be difficult to obtain."}, {"id": 113, "string": "We even find that in the very low-resource setting of just 2.5 hours of ST data, with transfer learning the model achieves a precision/recall of around 30% and improves by more than 10 points over the naive baseline."}, {"id": 114, "string": "In very low-resource scenarios with time constraints-such as in disaster relief-it is possible that even this level of performance may be useful, since it can be used to spot keywords in speech and can be trained in just three hours."}, {"id": 115, "string": "Sample translations."}, {"id": 116, "string": "Table 3 shows example translations for models sp-en-20h and sp-en-50h with and without transfer learning using en-300h."}, {"id": 117, "string": "Figure 3 shows the attention weights for the last sample utterance in Table 3 ."}, {"id": 118, "string": "For this utterance, the Spanish and English text have a different word order: mucho tiempo occurs in the middle of the speech utterance, and its translation, long time, is at the end of the English reference."}, {"id": 119, "string": "Similarly, vive aqu\u00ed occurs at the end of the speech utterance, while the translation, living here, is in the middle of the English reference."}, {"id": 120, "string": "The baseline sp-en-50h model translates the words correctly but doesn't get Table 3 , using 50h models with and without pre-training."}, {"id": 121, "string": "The x-axis shows the reference Spanish word positions in the input; the y-axis shows the predicted English subwords."}, {"id": 122, "string": "In the reference, mucho tiempo is translated to long time, and vive aqu\u00ed to living here, but their order is reversed, and this is reflected in (b)."}, {"id": 123, "string": "the English word order right."}, {"id": 124, "string": "With transfer learning, the model produces a shorter but still accurate translation in the correct word order."}, {"id": 125, "string": "Analysis To understand the source of these improvements, we carried out a set of ablation experiments."}, {"id": 126, "string": "For most of these experiments, we focus on Spanish-English ST with 20 hours of training data, with and without transfer learning."}, {"id": 127, "string": "Transfer learning with selected parameters."}, {"id": 128, "string": "In our first set of experiments, we transferred all parameters of the en-300h model, including the speech encoder CNN and LSTM; the text decoder embedding, LSTM and output layer parameters; and attention parameters."}, {"id": 129, "string": "To see which set of parameters has the most impact, we train the sp-en-20h model by transferring only selected parameters from en-300h, and randomly initializing the rest."}, {"id": 130, "string": "The results (Figure 4) show that transferring all parameters is most effective, and that the speech encoder parameters account for most of the gains."}, {"id": 131, "string": "We hypothesize that the encoder learns transferable low-level acoustic features that normalize across variability like speaker and channel differences to better capture meaningful phonetic differences, and that much of this learning is language-independent."}, {"id": 132, "string": "This hypothesis is supported by other work showing the benefits of cross-lingual and multilingual training for speech technology in low-resource target languages (Carlin et al., 2011; Jansen et al., 2010; Deng et al., 2013; Vu et al., 2012; Thomas et al., 2012; Cui et al., 2015; Alum\u00e4e et al., 2016; Renshaw et al., 2015; Hermann and Goldwater, 2018) ."}, {"id": 133, "string": "By contrast, transferring only decoder parameters does not improve accuracy."}, {"id": 134, "string": "Since decoder parameters help when used in tandem with encoder parameters, we suspect that the dependency in parameter training order might explain this: the transferred decoder parameters have been trained to expect particular input representations from the encoder, so transferring only the decoder parameters without the encoder might not be useful."}, {"id": 135, "string": "Figure 4 also suggests that models make strong gains early on in the training when using transfer learning."}, {"id": 136, "string": "The sp-en-20h model initialized with all model parameters (+asr:all) from en-300h reaches a higher BLEU score after just 5 epochs (2 hours) of training than the model without transfer learning trained for 60 epochs/20 hours."}, {"id": 137, "string": "This again can be useful in disaster-recovery scenarios, where the time to deploy a working system must be minimized."}, {"id": 138, "string": "Amount of ASR data required."}, {"id": 139, "string": "Figure 5 shows the impact of increasing the amount of English ASR data used on Spanish-English ST performance for two models: sp-en-20h and sp-en-50h."}, {"id": 140, "string": "For sp-en-20h, we see that using en-100h improves performance by almost 6 BLEU points."}, {"id": 141, "string": "By using more English ASR training data (en-300h) model, the BLEU score increases by almost 9 points."}, {"id": 142, "string": "However, for sp-en-50h, we only see improvements when using en-300h."}, {"id": 143, "string": "This implies that transfer learning is most useful when only a few tens of hours of training data are available for ST. As the amount of ST training data increases, the benefits of transfer learning tail off, although it's possible that using even more monolingual data, or improving the training at the ASR step, could extend the benefits to larger ST data sets."}, {"id": 144, "string": "Impact of code-switching."}, {"id": 145, "string": "We also tried using the en-300h ASR model without any fine-tuning to translate Spanish audio to English text."}, {"id": 146, "string": "This model achieved a BLEU score of 1.1, with a precision of 15 and recall of 21."}, {"id": 147, "string": "The non-zero BLEU score indicates that the model is matching some 4-grams in the reference."}, {"id": 148, "string": "This seems to be due to code-switching in the Fisher-Spanish speech data set."}, {"id": 149, "string": "Looking at the dev set utterances, we find several examples where the Spanish transcriptions match the English translations, indicating that the speaker switched into English."}, {"id": 150, "string": "For example, there is an utterance whose Spanish transcription and English translation are both \"right yeah\", and this English expression is indeed present in the source audio."}, {"id": 151, "string": "The English ASR model correctly translates this utterance, which is unsurprising since the phrase \"right yeah\" occurs nearly 500 times in Switchboard."}, {"id": 152, "string": "Overall, we find that in nearly 500 of the 4,000 development set utterances (14%), the Spanish transcription and English translations share more than half of their tokens, indicating likely codeswitching."}, {"id": 153, "string": "This suggests that transfer learning from English ASR models might help more than from other languages."}, {"id": 154, "string": "To isolate this effect from transfer learning of language-independent speech features, we carried out a further experiment."}, {"id": 155, "string": "Using French ASR to improve Spanish-English ST In this experiment, we pre-train using French ASR data for a Spanish-English translation task."}, {"id": 156, "string": "Here, we can only transfer the speech encoder parameters, and there should be little if any benefit due to codeswitching."}, {"id": 157, "string": "Because our French data set (20 hours) is much smaller than our English one (300 hours), for a fair comparison we used a 20 hour subset of the English data for pre-training in this experiment."}, {"id": 158, "string": "For both the English and French models, we transferred only the encoder parameters."}, {"id": 159, "string": "Table 4 shows that both the English and French 20-hour pre-trained models improve performance on Spanish-English ST."}, {"id": 160, "string": "The English model works slightly better, as would be predicted given our discussion of code-switching, but the French model is also useful, improving BLEU from 10.8 to 12.5."}, {"id": 161, "string": "This result strengthens the claim that ASR pretraining on a completely distinct third language can help low-resource ST."}, {"id": 162, "string": "Presumably benefits would be much greater if we used a larger ASR data set, as we did with English above."}, {"id": 163, "string": "In this experiment, the French pre-trained model used a French BPE output vocabulary, distinct from the English BPE vocabulary used in the ST system."}, {"id": 164, "string": "In the future it would be interesting to try combining the French and English text to create a combined output vocabulary, which would allow transferring both the encoder and decoder parameters, and may be useful for translating names or cognates."}, {"id": 165, "string": "More generally, it would also be possible to pre-train on multiple languages simultaneously using a shared BPE vocabulary."}, {"id": 166, "string": "There is evidence that speech features trained on multiple languages transfer better than those trained on the same amount of data from a single language (Hermann and Goldwater, 2018), so multilingual pretraining for ST could improve results."}, {"id": 167, "string": "baseline +fr-20h +en-20h sp-en-20h 10.8 12.5 13.2 Table 5 shows the ST model scores for Mboshi-French with and without using transfer learning."}, {"id": 168, "string": "The first two rows fr-top-8w, fr-top-10w, show precision and recall scores for the naive baselines where we predict the top 8 or 10 most frequent French words in the Mboshi-French training set."}, {"id": 169, "string": "These show that a precision/recall in the low 20s is easy to achieve, although with no n-gram matches (0 BLEU)."}, {"id": 170, "string": "The pre-trained ASR models by themselves (next two lines) are much worse."}, {"id": 171, "string": "The baseline model trained only on ST data actually has lower precision/recall than the naive baseline, although its non-zero BLEU score indicates that it is able to correctly predict some n-grams."}, {"id": 172, "string": "We see comparable precision/recall to the naive baseline with improvements in BLEU by transferring either French ASR parameters (both encoder and decoder, fr-20h) or English ASR parameters (encoder only, en-300h)."}, {"id": 173, "string": "Finally, to achieve the benefits of both the larger training set size for the encoder and the matching language of the decoder, we tried transferring the encoding parameters from the en-300h model and the decoding parameters from the fr-20h model."}, {"id": 174, "string": "This configuration (en+fr) gives us the best evaluation scores on all metrics, and highlights the flexibility of our framework."}, {"id": 175, "string": "Nevertheless, the 4-hour scenario is clearly a very challenging one."}, {"id": 176, "string": "Conclusion This paper introduced the idea of pre-training an end-to-end speech translation system involving a low-resource language using ASR training data from a higher-resource language."}, {"id": 177, "string": "We showed that large gains are possible: for example, we achieved an improvement of 9 BLEU points for a Spanish-English ST model with 20 hours of parallel data and 300 hours of English ASR data."}, {"id": 178, "string": "Moreover, the pre-trained model trains faster than the baseline, achieving higher BLEU in only a couple of hours, while the baseline trains for more than a day."}, {"id": 179, "string": "We also showed that these methods can be used effectively on a real low-resource language, Mboshi, with only 4 hours of parallel data."}, {"id": 180, "string": "The very small size of the data set makes the task challenging, but by combining parameters from an English encoder and French decoder, we outperformed baseline models to obtain a BLEU score of 7.1 and precision/recall of about 25%."}, {"id": 181, "string": "We believe ours is the first paper to report word-level BLEU scores on this data set."}, {"id": 182, "string": "Our analysis indicates that, other things being equal, transferring both encoder and decoder parameters works better than just transferring one or the other."}, {"id": 183, "string": "However, transferring the encoder parameters is where most of the benefit comes from."}, {"id": 184, "string": "Pre-training using a large ASR corpus from a mismatched language will therefore probably work better than using a smaller ASR corpus that matches the output language."}, {"id": 185, "string": "Our analysis suggests several avenues for further exploration."}, {"id": 186, "string": "On the speech side, it might be even more effective to use multilingual training; or to replace the MFCC input features with pre-trained multilingual features, or features that are targeted to low-resource multispeaker settings (Kamper et al., , 2017 Thomas et al., 2012; Cui et al., 2015; Renshaw et al., 2015) ."}, {"id": 187, "string": "On the language modeling side, simply transferring decoder parameters from an ASR model did not work; it might work better to use pre-trained decoder parameters from a language model, as proposed by Ramachandran et al."}, {"id": 188, "string": "(2017) , or shallow fusion (G\u00fcl\u00e7ehre et al., 2015; Toshniwal et al., 2018a) , which interpolates a pre-trained language model during beam search."}, {"id": 189, "string": "In these methods, the decoder parameters are independent, and can therefore be used on their own."}, {"id": 190, "string": "We plan to explore these strategies in future work."}], "headers": [{"section": "Introduction", "n": "1", "start": 0, "end": 20}, {"section": "Method", "n": "2", "start": 21, "end": 48}, {"section": "Preprocessing", "n": "3.2", "start": 49, "end": 61}, {"section": "Model architecture for ASR and ST", "n": "3.3", "start": 62, "end": 80}, {"section": "Evaluation", "n": "3.4", "start": 81, "end": 99}, {"section": "Spanish-English ST", "n": "5", "start": 100, "end": 107}, {"section": "Using English ASR to improve ST", "n": "5.1", "start": 108, "end": 124}, {"section": "Analysis", "n": "5.2", "start": 125, "end": 154}, {"section": "Using French ASR to improve", "n": "5.3", "start": 155, "end": 175}, {"section": "Conclusion", "n": "7", "start": 176, "end": 190}], "figures": [{"filename": "../figure/image/1360-Figure4-1.png", "caption": "Figure 4: Fisher development set training curves (reported using BLEU) for sp-en-20h using selected parameters from en-300h: none (base); encoder CNN only (+asr:cnn); encoder CNN and LSTM only (+asr:enc); decoder only (+asr:dec); and all: encoder, attention, and decoder (+asr:all). These scores do not use beam search and are therefore lower than the best scores reported in Figure 2.", "page": 5, "bbox": {"x1": 320.15999999999997, "x2": 512.64, "y1": 64.8, "y2": 191.51999999999998}}, {"filename": "../figure/image/1360-Figure3-1.png", "caption": "Figure 3: Attention plots for the final example in Table 3, using 50h models with and without pre-training. The x-axis shows the reference Spanish word positions in the input; the y-axis shows the predicted English subwords. In the reference, mucho tiempo is translated to long time, and vive aqu\u0131\u0301 to living here, but their order is reversed, and this is reflected in (b).", "page": 5, "bbox": {"x1": 89.75999999999999, "x2": 272.15999999999997, "y1": 61.44, "y2": 369.12}}, {"filename": "../figure/image/1360-Figure1-1.png", "caption": "Figure 1: Encoder-decoder with attention model architecture for both ASR and ST. The encoder input is the Spanish speech utterance claro, translated as clearly, represented as BPE (subword) units.", "page": 1, "bbox": {"x1": 317.76, "x2": 509.28, "y1": 69.6, "y2": 221.28}}, {"filename": "../figure/image/1360-Figure5-1.png", "caption": "Figure 5: Spanish-to-English BLEU scores on Fisher dev set, with 0h (no transfer learning), 100h and 300h of English ASR data used.", "page": 6, "bbox": {"x1": 86.39999999999999, "x2": 275.03999999999996, "y1": 65.75999999999999, "y2": 157.92}}, {"filename": "../figure/image/1360-Table4-1.png", "caption": "Table 4: Fisher dev set BLEU scores for sp-en-20h. baseline: model without transfer learning. Last two columns: Using encoder parameters from French ASR (+fr-20h), and English ASR (+en-20h).", "page": 7, "bbox": {"x1": 76.8, "x2": 285.12, "y1": 62.4, "y2": 102.24}}, {"filename": "../figure/image/1360-Table5-1.png", "caption": "Table 5: Mboshi-to-French translation scores, with and without ASR pre-training. Pr. is the precision, and Rec. the recall score. fr-top-8w and fr-top-10w are naive baselines that, respectively, predict the 8 or 10 most frequent training words. For en + fr, we use encoder parameters from en-300h and attention+decoder parameters from fr-20h", "page": 7, "bbox": {"x1": 72.0, "x2": 290.4, "y1": 179.04, "y2": 319.2}}, {"filename": "../figure/image/1360-Table1-1.png", "caption": "Table 1: Word Error Rate (WER, in %) for the ASR models used as pretraining, computed on Switchboard train-dev for English and Globalphone dev for French.", "page": 3, "bbox": {"x1": 317.76, "x2": 515.04, "y1": 62.4, "y2": 102.24}}, {"filename": "../figure/image/1360-Table2-1.png", "caption": "Table 2: BLEU scores for Spanish-English ST on the Fisher test set, usingN hours of training data. base: no transfer learning. +asr: using model parameters from English ASR (en-300h).", "page": 4, "bbox": {"x1": 306.71999999999997, "x2": 526.0799999999999, "y1": 62.4, "y2": 116.16}}, {"filename": "../figure/image/1360-Table3-1.png", "caption": "Table 3: Example translations on selected sentences from the Fisher development set, with stem-level ngram matches to the reference sentence underlined. 20h and 50h are Spanish-English models without pretraining; 20h+asr and 50h+asr are pre-trained on 300 hours of English ASR.", "page": 4, "bbox": {"x1": 306.71999999999997, "x2": 526.56, "y1": 190.56, "y2": 337.91999999999996}}, {"filename": "../figure/image/1360-Figure2-1.png", "caption": "Figure 2: (top) BLEU and (bottom) Unigram precision/recall for Spanish-English ST models computed on Fisher dev set. base indicates no transfer learning; +asr are models trained by fine-tuning en-300h model parameters. naive baseline indicates the score when we predict the 15 most frequent English words in the training set.", "page": 4, "bbox": {"x1": 82.56, "x2": 280.32, "y1": 65.75999999999999, "y2": 315.36}}]}