{"title": "Large-Scale Multi-Domain Belief Tracking with Knowledge Sharing", "abstract": "Robust dialogue belief tracking is a key component in maintaining good quality dialogue systems. The tasks that dialogue systems are trying to solve are becoming increasingly complex, requiring scalability to multi-domain, semantically rich dialogues. However, most current approaches have difficulty scaling up with domains because of the dependency of the model parameters on the dialogue ontology. In this paper, a novel approach is introduced that fully utilizes semantic similarity between dialogue utterances and the ontology terms, allowing the information to be shared across domains. The evaluation is performed on a recently collected multi-domain dialogues dataset, one order of magnitude larger than currently available corpora. Our model demonstrates great capability in handling multi-domain dialogues, simultaneously outperforming existing state-of-the-art models in singledomain dialogue tracking tasks.", "text": [{"id": 0, "string": "Introduction Spoken Dialogue Systems (SDS) are computer programs that can hold a conversation with a human."}, {"id": 1, "string": "These can be task-based systems that help the user achieve specific goals, e.g."}, {"id": 2, "string": "finding and booking hotels or restaurants."}, {"id": 3, "string": "In order for the SDS to infer the user goals/intentions during the conversation, its Belief Tracking (BT) component maintains a distribution of states, called a belief state, across dialogue turns (Young et al., 2010) ."}, {"id": 4, "string": "The belief state is used by the system to take actions in each turn until the conversation is concluded and the user goal is achieved."}, {"id": 5, "string": "In order to extract these belief states from the conversation, traditional approaches use a Spoken Language Understanding (SLU) unit that utilizes a semantic dictionary to hold all the key terms, rephrasings and alternative mentions of a belief state."}, {"id": 6, "string": "The SLU then delexicalises each turn using this semantic dictionary, before it passes it to the BT component (Wang and Lemon, 2013; Henderson et al., 2014b; Williams, 2014; Zilka and Jurcicek, 2015; Perez and Liu, 2016; Rastogi et al., 2017) ."}, {"id": 7, "string": "However, this approach is not scalable to multi-domain dialogues because of the effort required to define a semantic dictionary for each domain."}, {"id": 8, "string": "More advanced approaches, such as the Neural Belief Tracker (NBT), use word embeddings to alleviate the need for delexicalisation and combine the SLU and BT into one unit, mapping directly from turns to belief states ."}, {"id": 9, "string": "Nevertheless, the NBT model does not tackle the problem of mixing different domains in a conversation."}, {"id": 10, "string": "Moreover, as each slot is trained independently without sharing information between different slots, scaling such approaches to large multi-domain systems is greatly hindered."}, {"id": 11, "string": "In this paper, we propose a model that jointly identifies the domain and tracks the belief states corresponding to that domain."}, {"id": 12, "string": "It uses semantic similarity between ontology terms and turn utterances to allow for parameter sharing between different slots across domains and within a single domain."}, {"id": 13, "string": "In addition, the model parameters are independent of the ontology/belief states, thus the dimensionality of the parameters does not increase with the size of the ontology, making the model practically feasible to deploy in multidomain environments without any modifications."}, {"id": 14, "string": "Finally, we introduce a new, large-scale corpora of natural, human-human conversations providing new possibilities to train complex, neural-based models."}, {"id": 15, "string": "Our model systematically improves upon state-of-the-art neural approaches both in single and multi-domain conversations."}, {"id": 16, "string": "Background The belief states of the BT are defined based on an ontology -the structured representation of the database which contains entities the system can talk about."}, {"id": 17, "string": "The ontology defines the terms over which the distribution is to be tracked in the dialogue."}, {"id": 18, "string": "This ontology is constructed in terms of slots and values in a single domain setting."}, {"id": 19, "string": "Or, alternatively, in terms of domains, slots and values in a multi-domain environment."}, {"id": 20, "string": "Each domain consists of multiple slots and each slot contains several values, e.g."}, {"id": 21, "string": "domain=hotel, slot=price, value=expensive."}, {"id": 22, "string": "In each turn, the BT fits a distribution over the values of each slot in each domain, and a none value is added to each slot to indicate if the slot is not mentioned so that the distribution sums up to 1."}, {"id": 23, "string": "The BT then passes these states to the Policy Optimization unit as full probability distributions to take actions."}, {"id": 24, "string": "This allows robustness to noisy environments (Young et al., 2010) ."}, {"id": 25, "string": "The larger the ontology, the more flexible and multi-purposed the system is, but the harder it is to train and maintain a good quality BT."}, {"id": 26, "string": "Related Work In recent years, a plethora of research has been generated on belief tracking (Williams et al., 2016) ."}, {"id": 27, "string": "For the purposes of this paper, two previously proposed models are particularly relevant."}, {"id": 28, "string": "Neural Belief Tracker (NBT) The main idea behind the NBT  is to use semantically specialized pretrained word embeddings to encode the user utterance, the system act and the candidate slots and values taken from the ontology."}, {"id": 29, "string": "These are fed to semantic decoding and context modeling modules that apply a three-way gating mechanism and pass the output to a non-linear classifier layer to produce a distribution over the values for each slot."}, {"id": 30, "string": "It uses a simple update rule, p(s t ) = \u03b2p(s t\u22121 ) + \u03bby, where p(s t ) is the belief state at time step t, y is the output of the binary decision maker of the NBT and \u03b2 and \u03bb are tunable parameters."}, {"id": 31, "string": "The NBT leverages semantic information from the word embeddings to resolve lexical/morphological ambiguity and maximize the shared parameters across the values of each slot."}, {"id": 32, "string": "However, it only applies to a single domain and does not share parameters across slots."}, {"id": 33, "string": "Multi-domain Dialogue State Tracking Recently, Rastogi et al."}, {"id": 34, "string": "(2017) proposed a multidomain approach using delexicalized utterances fed to a two layer stacked bi-directional GRU network to extract features from the user and the system utterances."}, {"id": 35, "string": "These, combined with the candidate slots and values, are passed to a feed-forward neural network with a softmax in the last layer."}, {"id": 36, "string": "The candidate set fed to the network consists of the selected candidates from the previous turn and candidates from the ontology to a limit K, which restricts the maximum size of the chosen set."}, {"id": 37, "string": "Consequently, the model does not need an ad-hoc belief state update mechanism like in the NBT."}, {"id": 38, "string": "The parameters of the GRU network are defined for the domain, whereas the parameters of the feed-forward network are defined per slot, allowing transfer learning across different domains."}, {"id": 39, "string": "However, the model relies on delexicalization to extract the features, which limits the performance of the BT, as it does not scale to the rich variety of the language."}, {"id": 40, "string": "Moreover, the number of parameters increases with the number of slots."}, {"id": 41, "string": "Method The core idea is to leverage semantic similarities between the utterances and ontology terms to compute the belief state distribution."}, {"id": 42, "string": "In this way, the model parameters only learn to model the interactions between turn utterances and ontology terms in the semantic space, rather than the mapping from utterances to states."}, {"id": 43, "string": "Consequently, information is shared between both slots and across domains."}, {"id": 44, "string": "Additionally, the number of parameters does not increase with the ontology size."}, {"id": 45, "string": "Domain tracking is considered as a separate task but is learned jointly with the belief state tracking of the slots and values."}, {"id": 46, "string": "The proposed model uses semantically specialized pre-trained word embeddings (Wieting et al., 2015) ."}, {"id": 47, "string": "To encode the user and system utterances, we employed 7 independent bi-directional LSTMs (Graves and Schmidhuber, 2005) ."}, {"id": 48, "string": "Three of them are used to encode the system utterance for domain, slot and value tracking respectively."}, {"id": 49, "string": "Similarly, three Bi-LSTMs encode the user utterance while and the last one is used to track the user affirmation."}, {"id": 50, "string": "A variant of the CNNs as a feature extractor, similar to the one used in the NBT-CNN  is also employed."}, {"id": 51, "string": "Other variants of the model use CNNs as feature extractors (Kim, 2014; Kalchbrenner et al., 2014) ."}, {"id": 52, "string": "Domain Tracking Figure 1 presents the system architecture with two bi-directional LSTM networks as information encoders running over the word embeddings of the user and system utterances."}, {"id": 53, "string": "The last hidden states of the forward and backward layers are concatenated to produce h d usr , h d sys of size L for the user and system utterances respectively."}, {"id": 54, "string": "In the second variant of the model, CNNs are used to produce these vectors (Kim, 2014; Kalchbrenner et al., 2014) ."}, {"id": 55, "string": "To detect the presence of the domain in the dialogue turn, element-wise multiplication is used as a similarity metric between the hidden states and the ontology embeddings of the domain: d k = h d k tanh(W d e d + b d ), where k \u2208 {usr, sys}, e d is the embedding vector of the domain and W d \u2208 R L\u00d7D transforms the domain word embeddings of dimension D to the hidden representation."}, {"id": 56, "string": "The information about semantic similarity is held by d usr and d sys , which are fed to a non-linear layer to output a binary decision: P t (d) = \u03c3(w d {d usr \u2295 d sys } + b d ), where w d \u2208 R 2L and b d are learnable parameters that map the semantic similarity to a belief state probability P t (d) of a domain d at a turn t. Candidate Slots and Values Tracking Slots and values are tracked using a similar architecture as for domain tracking (Figure 1) ."}, {"id": 57, "string": "However, to correctly model the context of the systemuser dialogue at each turn, three different cases are considered when computing the similarity vectors: 1."}, {"id": 58, "string": "Inform: The user is informing the system about his/her goal, e.g."}, {"id": 59, "string": "'I am looking for a restaurant that serves Turkish food'."}, {"id": 60, "string": "2."}, {"id": 61, "string": "Request: The system is requesting information by asking the user about the value of a specific slot."}, {"id": 62, "string": "If the system utterance is: 'When do you want the taxi to arrive?'"}, {"id": 63, "string": "and the user answers with '19:30'."}, {"id": 64, "string": "3."}, {"id": 65, "string": "Confirm: The system wants to confirm information about the value of a specific slot."}, {"id": 66, "string": "If the system asked: 'Would you like free parking?"}, {"id": 67, "string": "', the user can either affirm positively or negatively."}, {"id": 68, "string": "The model detects the user affirmation, using a separate bi-directional LSTM or CNN to output h a usr ."}, {"id": 69, "string": "The three cases are modelled as following: y s,v inf = w inf {s usr \u2295 v usr } + b inf , y s,v req = w req {s sys \u2295 v usr } + b req , y s,v af = w af {s sys \u2295 v sys \u2295 h a usr } + b af , where s k , v k for k \u2208 {usr, sys} represent semantic similarity between the user and system utterances and the ontology slot and value terms respectively computed as shown in Figure 1 , and w and b are learnable parameters."}, {"id": 70, "string": "The distribution over the values of slot s in domain d at turn t can be computed by summing the unscaled states, y inf , y req and y af for each value v in s, and applying a softmax to normalize the distribution: P t (s, v) = softmax(y s,v inf + y s,v req + y s,v af )."}, {"id": 71, "string": "Belief State Update Since dialogue systems in the real-world operate in noisy environments, a robust BT should utilize the flow of the conversation to reduce the uncertainty in the belief state distribution."}, {"id": 72, "string": "This can be achieved by passing the output of the decision maker, at each turn, as an input to an RNN that runs over the dialogue turns as shown in Figure 1 , which allows the gradients to be propagated across turns."}, {"id": 73, "string": "This alleviates the problem of tuning hyper-parameters for rule-based updates."}, {"id": 74, "string": "To avoid the vanishing gradient problem, three networks were tested: a simple RNN, an RNN with a memory cell (Henderson et al., 2014a ) and a LSTM."}, {"id": 75, "string": "The RNN with a memory cell proved to give the best results."}, {"id": 76, "string": "In addition to the fact that it reduces the vanishing gradient problem, this variant is less complex than an LSTM, which makes training easier."}, {"id": 77, "string": "Furthermore, a variant of RNN used for domain tracking has all its weights of the form: W i = \u03b1 i I, where \u03b1 i is a distinct learnable parameter for hidden, memory and previous state layers and I is the identity matrix."}, {"id": 78, "string": "Similarly, weights of the RNN used to track the slots and values is of the form: W j = \u03b3 j I + \u03bb j (1 \u2212 I), where \u03b3 j and \u03bb j are the learnable parameters."}, {"id": 79, "string": "These two variants of RNN are a combination of Henderson et al."}, {"id": 80, "string": "(2014a) and Mrkvsi\u0107 and Vuli\u0107 (2018) previous works."}, {"id": 81, "string": "The output is P 1:T (d) and P 1:T (s, v), which represents the joint probability distribution of the domains and slots and values respectively over the complete dialogue."}, {"id": 82, "string": "Combining these together produces the full belief state distribution of the dialogue: Training Criteria Domain tracking and slots and values tracking are trained disjointly."}, {"id": 83, "string": "Belief state labels for each turn are split into domains and slots and values."}, {"id": 84, "string": "Thanks to the disjoint training, the learning of slot and value belief states are not restricted to a specific domain."}, {"id": 85, "string": "Therefore, the model shares the knowledge of slots and values across different domains."}, {"id": 86, "string": "The loss function for the domain tracking is: L d = \u2212 N n=1 d\u2208D t n (d)logP n 1:T (d), where d is a vector of domains over the dialogue, t n (d) is the domain label for the dialogue n and N is the number of dialogues."}, {"id": 87, "string": "Similarly, the loss function for the slots and values tracking is: L s,v = \u2212 N n=1 s,v\u2208S,V t n (s, v)logP n 1:T (s, v), where s and v are vectors of slots and values over the dialogue and t n (s, v) is the joint label vector for the dialogue n. Datasets and Baselines Neural approaches to statistical dialogue development, especially in a task-oriented paradigm, are greatly hindered by the lack of large scale datasets."}, {"id": 88, "string": "That is why, following the Wizard-of-Oz (WOZ) approach (Kelley, 1984; , we ran text-based multi-domain corpus data collection scheme through Amazon MTurk."}, {"id": 89, "string": "The main goal of the data collection was to acquire humanhuman conversations between a tourist visiting a city and a clerk from an information center."}, {"id": 90, "string": "At the beginning of each dialogue the user (visitor) was given explicit instructions about the goal to fulfill, which often spanned multiple domains."}, {"id": 91, "string": "The task of the system (wizard) is to assist a visitor having an access to databases over domains."}, {"id": 92, "string": "The WOZ paradigm allowed us to obtain natural and semantically rich multi-topic dialogues spanning over multiple domains such as hotels, attractions, restaurants, booking trains or taxis."}, {"id": 93, "string": "The dialogues cover from 1 up to 5 domains per dialogue greatly varying in length and complexity."}, {"id": 94, "string": "Data Structure The data consists of 2480 single-domain dialogues and 7375 multi-domain dialogues usually spanning from 2 up to 5 domains."}, {"id": 95, "string": "Some domains consists also of sub-domains like booking."}, {"id": 96, "string": "The average sentence lengths are 11.63 and 15.01 for users Evaluation We also used the extended WOZ 2.0 dataset (Wen et al., 2017)."}, {"id": 97, "string": "2 WOZ2 dataset consists of 1200 single topic dialogues constrained to the restaurant domain."}, {"id": 98, "string": "All the weights were initialised using normal distribution of zero mean and unit variance and biases were initialised to zero."}, {"id": 99, "string": "ADAM optimizer (Kingma and Ba, 2014) (with 64 batch size) is used to train all the models for 600 epochs."}, {"id": 100, "string": "Dropout (Srivastava et al., 2014) was used for regularisation (50% dropout rate on all the intermediate representations)."}, {"id": 101, "string": "For each of the two datasets we compare our proposed architecture (using either Bi-LSTM or CNN as encoders) to the NBT model 3 ."}, {"id": 102, "string": "This is because the dialogues in the new dataset are richer and more noisier, as a closer resemblance to real environment dialogues."}, {"id": 103, "string": "Table 2 presents the results on multi-domain dialogues from the new dataset described in Section 5."}, {"id": 104, "string": "To demonstrate the difficulty of the multidomain belief tracking problem, values of a theoretical baseline that samples the belief state uniformly at random are also presented."}, {"id": 105, "string": "Our model gracefully handles such a difficult task."}, {"id": 106, "string": "In most of the cases, CNNs demonstrate better performance than Bi-LSTMs."}, {"id": 107, "string": "We hypothesize that this comes from the effectiveness of extracting local and position-invariant features, which are crucial for semantic similarities (Yin et al., 2017) ."}, {"id": 108, "string": "Results Conclusions In this paper, we proposed a new approach that tackles the issue of multi-domain belief tracking, such as model parameter scalability with the ontology size."}, {"id": 109, "string": "Our model shows improved performance in single-domain tasks compared to the state-ofthe-art NBT method."}, {"id": 110, "string": "By exploiting semantic similarities between dialogue utterances and ontology terms, the model alleviates the need for ontologydependent parameters and maximizes the amount of information shared between slots and across domains."}, {"id": 111, "string": "In future, we intend to investigate introducing new domains and ontology terms without further training thus performing zero-shot learning."}], "headers": [{"section": "Introduction", "n": "1", "start": 0, "end": 15}, {"section": "Background", "n": "2", "start": 16, "end": 24}, {"section": "Related Work", "n": "3", "start": 25, "end": 27}, {"section": "Neural Belief Tracker (NBT)", "n": "3.1", "start": 28, "end": 32}, {"section": "Multi-domain Dialogue State Tracking", "n": "3.2", "start": 33, "end": 40}, {"section": "Method", "n": "4", "start": 41, "end": 51}, {"section": "Domain Tracking", "n": "4.1", "start": 52, "end": 56}, {"section": "Candidate Slots and Values Tracking", "n": "4.2", "start": 57, "end": 70}, {"section": "Belief State Update", "n": "4.3", "start": 71, "end": 81}, {"section": "Training Criteria", "n": "4.4", "start": 82, "end": 87}, {"section": "Datasets and Baselines", "n": "5", "start": 88, "end": 93}, {"section": "Data Structure", "n": "5.1", "start": 94, "end": 95}, {"section": "Evaluation", "n": "5.2", "start": 96, "end": 107}, {"section": "Conclusions", "n": "7", "start": 108, "end": 111}], "figures": [{"filename": "../figure/image/1061-Figure1-1.png", "caption": "Figure 1: The proposed model architecture, using Bi-LSTMs as encoders. Other variants of the model use CNNs as feature extractors (Kim, 2014; Kalchbrenner et al., 2014).", "page": 2, "bbox": {"x1": 93.6, "x2": 503.03999999999996, "y1": 61.44, "y2": 327.36}}, {"filename": "../figure/image/1061-Table2-1.png", "caption": "Table 2: The overall F1 score and accuracy for the multi-domain dialogues test set.4", "page": 4, "bbox": {"x1": 307.68, "x2": 525.12, "y1": 190.56, "y2": 262.08}}, {"filename": "../figure/image/1061-Table1-1.png", "caption": "Table 1: WOZ 2.0 and new dataset test set accuracies of the NBT-CNN and the two variants of the proposed model, for slots food, price range, area and joint goals.", "page": 4, "bbox": {"x1": 112.8, "x2": 485.28, "y1": 61.44, "y2": 147.35999999999999}}]}