{"title": "Unifying Text, Metadata, and User Network Representations with a Neural Network for Geolocation Prediction", "abstract": "We propose a novel geolocation prediction model using a complex neural network. Our model unifies text, metadata, and user network representations with an attention mechanism to overcome previous ensemble approaches. In an evaluation using two open datasets, the proposed model exhibited a maximum 3.8% increase in accuracy and a maximum of 6.6% increase in ac-curacy@161 against previous models. We further analyzed several intermediate layers of our model, which revealed that their states capture some statistical characteristics of the datasets.", "text": [{"id": 0, "string": "Introduction Social media sites have become a popular source of information to analyze current opinions of numerous people."}, {"id": 1, "string": "Many researchers have worked to realize various automated analytical methods for social media because manual analysis of such vast amounts of data is difficult."}, {"id": 2, "string": "Geolocation prediction is one such analytical method that has been studied widely to predict a user location or a document location."}, {"id": 3, "string": "Location information is crucially important information for analyses such as disaster analysis (Sakaki et al., 2010) , disease analysis (Culotta, 2010) , and political analysis (Tumasjan et al., 2010) ."}, {"id": 4, "string": "Such information is also useful for analyses such as sentiment analysis (Mart\u00ednez-C\u00e1mara et al., 2014) and user attribute analysis (Rao et al., 2010) to undertake detailed region-specific analyses."}, {"id": 5, "string": "Geolocation prediction has been performed for Wikipedia (Overell, 2009 ), Flickr (Serdyukov et al., 2009; Crandall et al., 2009 ), Facebook (Backstrom et al., 2010) , and Twitter (Cheng et al., 2010; Eisenstein et al., 2010) ."}, {"id": 6, "string": "Among these sources, Twitter is often preferred because of its characteristics, which are suited for geolocation prediction."}, {"id": 7, "string": "First, some tweets include geotags, which are useful as ground truth locations."}, {"id": 8, "string": "Secondly, tweets include metadata such as timezones and self-declared locations that can facilitate geolocation prediction."}, {"id": 9, "string": "Thirdly, a user network is obtainable by consideration of the interaction between two users as a network link."}, {"id": 10, "string": "Herein, we propose a neural network model to tackle geolocation prediction in Twitter."}, {"id": 11, "string": "Past studies have combined text, metadata, and user network information with ensemble approaches (Han et al., 2013 (Han et al., , 2014 Rahimi et al., 2015a; Jayasinghe et al., 2016) to achieve state-of-the-art performance."}, {"id": 12, "string": "Our model combines text, metadata, and user network information using a complex neural network."}, {"id": 13, "string": "Neural networks have recently shown effectiveness to capture complex representations combining simpler representations from large-scale datasets (Goodfellow et al., 2016) ."}, {"id": 14, "string": "We intend to obtain unified text, metadata, and user network representations with an attention mechanism  that is superior to the earlier ensemble approaches."}, {"id": 15, "string": "The contributions of this paper are the following: 1."}, {"id": 16, "string": "We propose a neural network model that learns unified text, metadata, and user network representations with an attention mechanism."}, {"id": 17, "string": "2."}, {"id": 18, "string": "We show that the proposed model outperforms the previous ensemble approaches in two open datasets."}, {"id": 19, "string": "3."}, {"id": 20, "string": "We analyze some components of the proposed model to gain insight into the unification processes of the model."}, {"id": 21, "string": "Our model specifically emphasizes geolocation prediction in Twitter to use benefits derived from the characteristics described above."}, {"id": 22, "string": "However, our model can be readily extended to other social media analyses such as user attribute analysis and political analysis, which can benefit from metadata and user network information."}, {"id": 23, "string": "In subsequent sections of this paper, we explain the related works in four perspectives in Section 2."}, {"id": 24, "string": "The proposed neural network model is described in Section 3 along with two open datasets that we used for evaluations in Section 4."}, {"id": 25, "string": "Details of an evaluation are reported in Section 5 with discussions in Section 6."}, {"id": 26, "string": "Finally, Section 7 concludes the paper with some future directions."}, {"id": 27, "string": "2 Related Works 2.1 Text-based Approach Probability distributions of words over locations have been used to estimate the geolocations of users."}, {"id": 28, "string": "Maximum likelihood estimation approaches (Cheng et al., 2010 and language modeling approaches minimizing KL-divergence (Wing and Baldridge, 2011; Kinsella et al., 2011; Roller et al., 2012) have succeeded in predicting user locations using word distributions."}, {"id": 29, "string": "Topic modeling approaches to extract latent topics with geographical regions (Eisenstein et al., 2010 (Eisenstein et al., , 2011 Hong et al., 2012; Ahmed et al., 2013) have also been explored considering word distributions."}, {"id": 30, "string": "Supervised machine learning methods with word features are also popular in text-based geolocation prediction."}, {"id": 31, "string": "Multinomial Naive Bayes (Han et al., 2012 (Han et al., , 2014 Wing and Baldridge, 2011) , logistic regression (Wing and Baldridge, 2014; Han et al., 2014) , hierarchical logistic regression (Wing and Baldridge, 2014) , and a multilayer neural network with stacked denoising autoencoder (Liu and Inkpen, 2015) have realized geolocation prediction from text."}, {"id": 32, "string": "A semi-supervised machine learning approach by Cha et al."}, {"id": 33, "string": "(2015) has also been produced using a sparse-coding and dictionary learning."}, {"id": 34, "string": "User-network-based Approach Social media often include interactions of several kinds among users."}, {"id": 35, "string": "These interactions can be regarded as links that form a network among users."}, {"id": 36, "string": "Several studies have used such user network information to predict geolocation."}, {"id": 37, "string": "Backstrom et al."}, {"id": 38, "string": "(2010) introduced a probabilistic model to predict the location of a user using friendship information in Facebook."}, {"id": 39, "string": "Friend and follower information in Twitter were used to predict user locations with a most frequent friend algorithm (Davis Jr. et al., 2011) , a unified descriptive model (Li et al., 2012b) , location-based generative models (Li et al., 2012a) , dynamic Bayesian networks (Sadilek et al., 2012) , a support vector machine (Rout et al., 2013) , and maximum likelihood estimation (McGee et al., 2013) ."}, {"id": 40, "string": "Mention information in Twitter is also used with label propagation models (Jurgens, 2013; Compton et al., 2014) and an energy and social local coefficient model (Kong et al., 2014) ."}, {"id": 41, "string": "Jurgens et al."}, {"id": 42, "string": "(2015) compared nine user-network-based approaches targeting Twitter, controlling data conditions."}, {"id": 43, "string": "Metadata-based Approach Metadata such as location fields are useful as effective clues to predict geolocation."}, {"id": 44, "string": "Hecht et al."}, {"id": 45, "string": "(2011) reported that decent accuracy of geolocation prediction can be achieved using location fields."}, {"id": 46, "string": "Approaches to combine metadata with texts are also proposed to extend text-based approaches."}, {"id": 47, "string": "Combinatory approaches such as a dynamically weighted ensemble method (Mahmud et al., 2012) , polygon stacking (Schulz et al., 2013) , stacking (Han et al., 2013 (Han et al., , 2014 , and average pooling with a neural network (Miura et al., 2016) have strengthened geolocation prediction."}, {"id": 48, "string": "Combinatory Approach Extending User-network-based Approach Several attempts have been made to combine usernetwork-based approaches with other approaches."}, {"id": 49, "string": "A text-based approach with logistic regression was combined with label propagation approaches to enhance geolocation prediction (Rahimi et al., 2015a (Rahimi et al., ,b, 2016 ."}, {"id": 50, "string": "Jayasinghe et al."}, {"id": 51, "string": "(2016) combined nine components including text-based approaches, metadata-based approaches, and a usernetwork-based approach with a cascade ensemble method."}, {"id": 52, "string": "Comparisons with Proposed Model A model we propose in Section 3 which combines text, metadata, and user network information with a neural network, can be regarded as an alternative to approaches using text and metadata (Mahmud et al., 2012; Schulz et al., 2013; Han et al., 2013 Han et al., , 2014 Miura et al., 2016) , approaches with text and user network information (Rahimi et al., 2015a,b) , and an approach with text, metadata, and user network information (Jayasinghe et al., 2016) ."}, {"id": 53, "string": "In Section 5, we demonstrate that our model outperforms earlier models."}, {"id": 54, "string": "In terms of machine learning methods, our model is a neural network model that shares some similarity with previous neural network models (Liu and Inkpen, 2015; Miura et al., 2016) ."}, {"id": 55, "string": "Our model and these previous models have two key differences."}, {"id": 56, "string": "First, our model integrates user network information along with other information."}, {"id": 57, "string": "Secondly, our model combines text and metadata with an attention mechanism ."}, {"id": 58, "string": "Figure 1 presents an overview of our model: a complex neural network for classification with a city as a label."}, {"id": 59, "string": "For each user, the model accepts inputs of messages, a location field, a description field, a timezone, linked users, and the cities of linked users."}, {"id": 60, "string": "Model Proposed Model User network information is incorporated by city embeddings and user embeddings of linked users."}, {"id": 61, "string": "User embeddings are introduced along with city embeddings because linked users with city information 1 are limited."}, {"id": 62, "string": "We chose to let the model learn geolocation representations of linked users directly via user embeddings."}, {"id": 63, "string": "The model can be broken down to several components, details of which are described in Section 3.1.1-3.1.4."}, {"id": 64, "string": "Text Component We describe the text component of the model, which is the \"TEXT\" section in Figure 1 ."}, {"id": 65, "string": "Figure 2 presents an overview of the text component."}, {"id": 66, "string": "The component consists of a recurrent neural network (RNN) (Graves, 2012) layer and attention layers."}, {"id": 67, "string": "An input of the component is a timeline of a user, which consists of messages in a time sequence."}, {"id": 68, "string": "As an implementation of RNN, we used Gated Recurrent Unit (GRU)  with a bidirectional setting."}, {"id": 69, "string": "In the RNN layer, word embeddings x of a message are processed with the following transition functions: Word Embedding Attention M computes a message representation m as a weighted sum of g t with weight \u03b1 t : z t = \u03c3 (W z x t + U z h t\u22121 + b z ) (1) r t = \u03c3 (W r x t + U r h t\u22121 + b r ) (2) h t = tanh (W h x t + U h (r t \u2299 h t\u22121 ) + b h ) (3) h t = (1 \u2212 z t ) \u2299 h t\u22121 + z t \u2299h t (4) where z t is an update gate, r t is a reset gate,h t is a candidate state, h t is a state, W z , W r , W h , U z , U r , U h are weight matrices, b z , b r , b x 1 x T \u2026 input h 1 bi-directional recurrent states \u2026 g 1 g 2 g T RNN features \u2026 x 2 u 1 context vectors + \u2026 \u03b1 1 g 1 \u03b1 2 g 2 \u03b1 T g T Attention features m u 2 u T Attention Layer RNN Layer m = \u2211 t \u03b1 t g t (5) \u03b1 t = exp ( v T \u03b1 u t ) \u2211 t exp (v T \u03b1 u t ) (6) u t = tanh (W \u03b1 g t + b \u03b1 ) (7) where v \u03b1 is a weight vector, W \u03b1 is a weight matrix, and b \u03b1 a bias vector."}, {"id": 70, "string": "u t is an attention context vector calculated from g t with a single fullyconnected layer (Eq."}, {"id": 71, "string": "7)."}, {"id": 72, "string": "u t is normalized with softmax to obtain \u03b1 t as a probability (Eq."}, {"id": 73, "string": "6)."}, {"id": 74, "string": "The message representation m is passed to the second attention layer Attention TL to obtain a timeline representation from message representations."}, {"id": 75, "string": "Text and Metadata Component We describe text and metadata components of the model, which is the \"TEXT&META\" section in Figure 1 ."}, {"id": 76, "string": "This component considers the following three types of metadata along with text: location a text field in which a user is allowed to write the user location freely, description a text field a user can use for self-description, and timezone a selective field from which a user can choose a timezone."}, {"id": 77, "string": "Note that certain percentages of these fields are not available 2 , and unknown tokens are used for inputs in such cases."}, {"id": 78, "string": "We process location fields and description fields similarly to messages using an RNN layer and an attention layer."}, {"id": 79, "string": "Because there is only one location and one description per user, a second attention layer is not required, as it is in the text component."}, {"id": 80, "string": "We also chose to share word embeddings among the messages, the location, and the description processes because these inputs are all textual information."}, {"id": 81, "string": "For the timezone, an embedding is assigned for each timezone value."}, {"id": 82, "string": "A processed timeline representation, a location representation, and a description representation are then passed to the attention layer Attention U with a timezone representation."}, {"id": 83, "string": "Attention U combines these four representations and outputs a user representation."}, {"id": 84, "string": "This combination is done as in Attention TL with four representations as g 1 ."}, {"id": 85, "string": "."}, {"id": 86, "string": "."}, {"id": 87, "string": "g 4 in Eq."}, {"id": 88, "string": "5."}, {"id": 89, "string": "User Network Component We describe the user network component of the model, which is the \"USERNET\" section in Figure 1 ."}, {"id": 90, "string": "Figure 3 presents an overview of the user network component."}, {"id": 91, "string": "The model has two inputs linked cities and linked users."}, {"id": 92, "string": "Users connected with a user network are extracted as linked users."}, {"id": 93, "string": "We treat their cities 3 as linked cities."}, {"id": 94, "string": "Linked cities and linked users are assigned with city embeddings c and user embeddings a respectively."}, {"id": 95, "string": "c and a are then processed to output p = c \u2295 a, where \u2295 is an element-wise addition operator."}, {"id": 96, "string": "p is then passed to the subsequent attention layer Attention N to obtain a user network representa- Construction of the User Network We construct mention networks (Jurgens, 2013; Compton et al., 2014; Rahimi et al., 2015a,b) from datasets as user networks."}, {"id": 97, "string": "To do so, we follow the approach of Rahimi et al."}, {"id": 98, "string": "(2015a) and Rahimi et al."}, {"id": 99, "string": "(2015b) who use uni-directional mention to set edges of a mention network."}, {"id": 100, "string": "An edge is set between the two users nodes if a user mentions another user."}, {"id": 101, "string": "The number of unidirectional mention edges for TwitterUS and W-NUT can be found in Table 1 ."}, {"id": 102, "string": "The uni-directional setting results to large numbers of edges, which often are computationally expensive to process."}, {"id": 103, "string": "We restricted edges to satisfy one of the following conditions to reduce the size: (1) both users have ground truth locations or (2) one user has a ground truth location and another user is mentioned 5 times or more in a training set."}, {"id": 104, "string": "The number of reduced-edges with these conditions in TwitterUS and W-NUT can be confirmed in Table 1 ."}, {"id": 105, "string": "Evaluation 5.1 Implemented Baselines 5.1.1 LR LR is an l 1 -regularized logistic regression model with k-d tree regions (Roller et al., 2012) used in Rahimi et al."}, {"id": 106, "string": "(2015a) ."}, {"id": 107, "string": "The model uses tfidf weighted bag-of-words unigrams for features."}, {"id": 108, "string": "This model is simple, but it has shown state-ofthe-art performance in cases when only text is available."}, {"id": 109, "string": "MADCEL-B-LR MADCEL-B-LR, a model presented by (Rahimi et al., 2015a) , combines LR with Modified Adsorption (MAD) (Talukdar and Crammer, 2009) ."}, {"id": 110, "string": "MAD is a graph-based label propagation algorithm that optimizes an objective with a prior term, a smoothness term, and an uninformativeness term."}, {"id": 111, "string": "LR is combined with MAD by introducing LR results as dongle nodes to MAD."}, {"id": 112, "string": "This model includes an algorithm for the construction of a mention network."}, {"id": 113, "string": "The algorithm removes celebrity users 5 and collapses a mention network 6 ."}, {"id": 114, "string": "We use binary edges for user network edges because they performed slightly better than weighted edges by accuracy@161 metric in Rahimi et al."}, {"id": 115, "string": "(2015a) ."}, {"id": 116, "string": "LR-STACK LR-STACK is an ensemble learning model that combines four LR classifiers (LR-MSG, LR-LOC, LR-DESC, LR-TZ) with an l 2 -regularized logistic regression meta-classifier (LR-2ND)."}, {"id": 117, "string": "LR-MSG, LR-LOC, LR-DESC, and LR-TZ respectively use messages, location fields, description fields, and timezones as their inputs."}, {"id": 118, "string": "This model is similar to the stacking (Wolpert, 1992) approach taken in Han et al."}, {"id": 119, "string": "(2013) and Han et al."}, {"id": 120, "string": "(2014) , which showed superior performance compared to a feature concatenation approach."}, {"id": 121, "string": "The model takes the following three steps to combine text and metadata: Step 1 LR-MSG, LR-LOC, LR-DESC, and LR-TZ are trained using a training set, Step 2 the outputs of the four classifiers on the training set are obtained with 10-fold cross validation, and Step 3 LR-2ND is trained using the outputs of the four classifiers."}, {"id": 122, "string": "MADCEL-B-LR-STACK MADCEL-B-LR-STACK is a combined model of MADCEL-B-LR and LR-STACK."}, {"id": 123, "string": "LR-STACK results are introduced as dongle nodes to MAD instead of LR results to combine text, metadata, and network information."}, {"id": 124, "string": "Model Configurations 5.2.1 Text Processor We applied a lower case conversion, a unicode normalization, a Twitter user name normalization, and a URL normalization for text pre-processing."}, {"id": 125, "string": "The pre-processed text is then segmented using Twokenizer (Owoputi et al., 2013) to obtain words."}, {"id": 126, "string": "Pre-training of Embeddings We pre-trained word embeddings using messages, location fields, and description fields of a training set using fastText (Bojanowski et al., 2016) with the skip-gram algorithm."}, {"id": 127, "string": "We also pre-trained user embeddings using the non-reduced mention network described in Section 4.2 of a training set with LINE (Tang et al., 2015) ."}, {"id": 128, "string": "The detail of pre-training parameters are described in Appendix A.1."}, {"id": 129, "string": "Neural Network Optimization We chose an objective function of our models to cross-entropy loss."}, {"id": 130, "string": "l 2 regularization was applied to the RNN layers, the attention context vectors, and the FC layers of our models to avoid overfitting."}, {"id": 131, "string": "The objective function was minimized through stochastic gradient descent over shuffled mini-batches with Adam (Kingma and Ba, 2014)."}, {"id": 132, "string": "Model Parameters The layers and the embeddings in our models have unit size and embedding dimension parameters."}, {"id": 133, "string": "Our models and the baseline models have regularization parameter \u03b1, which is sensitive to a dataset."}, {"id": 134, "string": "The baseline models have additional k-d tree bucket size c, celebrity threshold t, and MAD parameters \u00b5 1 , \u00b5 2 , and \u00b5 3 , which are also data sensitive."}, {"id": 135, "string": "We chose optimal values for these parameters in terms of accuracy with a grid search using the development sets of TwitterUS and W-NUT."}, {"id": 136, "string": "Details of the parameter selection strategies and the selected values are described in Appendix A.2."}, {"id": 137, "string": "Metrics We evaluate the models in the following four commonly used metrics in geolocation prediction: accuracy the percentage of correctly predicted cities, accuracy@161 a relaxed accuracy that takes prediction errors within 161 km as correct predictions, median error distance median value of error distances in predictions, and mean error distance mean value of error distances in predictions."}, {"id": 138, "string": "Table 2 : Performances of our models and the baseline models on TwitterUS."}, {"id": 139, "string": "Significance tests were performed between models with same Sign."}, {"id": 140, "string": "Test IDs."}, {"id": 141, "string": "The shaded lines represent values copied from related papers."}, {"id": 142, "string": "Asterisks denote significant improvements against paired counterparts with 1% confidence (**) and 5% confidence (*)."}, {"id": 143, "string": "Model Sign."}, {"id": 144, "string": "Test ID Accuracy Accuracy @161 Error Distance Median Mean Baselines ( Table 3 : Performance of our models and baseline models on W-NUT."}, {"id": 145, "string": "The same notations as those in Table 2 are used in this table."}, {"id": 146, "string": "Table 2 presents results of our models and the implemented baseline models on TwitterUS."}, {"id": 147, "string": "We also list values from earlier reports (Han et al., 2012; Wing and Baldridge, 2014; Rahimi et al., 2015a Rahimi et al., ,b, 2016 to make our results readily comparable with past reported values."}, {"id": 148, "string": "Result Performance on TwitterUS We performed some statistical significance tests among model pairs that share the same inputs."}, {"id": 149, "string": "The values in the Sign."}, {"id": 150, "string": "Test ID column of Table  2 represent the IDs of these pairs."}, {"id": 151, "string": "As a preparation of statistical significance tests, accuracies, accuracy@161s, and error distances of each test user were calculated for each model pair."}, {"id": 152, "string": "Twosided Fisher-Pittman Permutation tests were used for testing accuracy and accuracy@161."}, {"id": 153, "string": "Mood's median test was used for testing error distance in terms of median."}, {"id": 154, "string": "Paired t-tests were used for testing error distance in terms of mean."}, {"id": 155, "string": "We confirmed the significance of improvements in accuracy@161 and mean distance error for all of our models."}, {"id": 156, "string": "Three of our models also improved in terms of accuracy."}, {"id": 157, "string": "Especially, the proposed model achieved a 2.8% increase in accuracy and a 2.4% increase in accuracy@161 against the counterpart baseline model MADCEL-B-LR-STACK."}, {"id": 158, "string": "One negative result we found was the median error distance between SUB-NN-META and LR-STACK."}, {"id": 159, "string": "The baseline model LR-STACK performed 4.5 km significantly better than our model."}, {"id": 160, "string": "Table 3 presents the results of our models and the implemented baseline models on W-NUT."}, {"id": 161, "string": "As for TwitterUS, we listed values from Miura et al."}, {"id": 162, "string": "(2016) and Jayasinghe et al."}, {"id": 163, "string": "(2016) ."}, {"id": 164, "string": "We tested the significance of these results in the same way as we did for TwitterUS."}, {"id": 165, "string": "We confirmed significant improvement in the four metrics for all of our models."}, {"id": 166, "string": "The proposed model achieved a 4.8% increase in accuracy and a 6.6% increase in accuracy@161 against the counterpart baseline model MADCEL-B-LR-STACK."}, {"id": 167, "string": "The accuracy is 3.8% higher against the previously reported best value (Jayasinghe et al., 2016) which combined texts, metadata, and user network information with an ensemble method."}, {"id": 168, "string": "6 Discussion 6.1 Analyses of Attention Probabilities Performance on W-NUT Unification Strategies In the evaluation, the proposed model has implicitly shown effectiveness at unifying text, metadata, and user network representations through improvements in the four metrics."}, {"id": 169, "string": "However, details of the unification processes are not clear from the model outputs because they are merely the probabilities of estimated locations."}, {"id": 170, "string": "To gain insight into the unification processes, we analyzed the states of two attention layers: Attention U and Attention UN in Figure 1 ."}, {"id": 171, "string": "Figure 4 presents the estimated probability density functions (PDFs) of the four input representations for Attention U ."}, {"id": 172, "string": "These PDFs are estimated with kernel density estimation from the development sets of TwitterUS and W-NUT, where all four representations are available."}, {"id": 173, "string": "From the PDFs, it is apparent that the model assigns higher probabilities to time line representations than to other three representations in TwitterUS compared to W-NUT."}, {"id": 174, "string": "This finding is reasonable because timelines in TwitterUS consist of more tweets (tweet/user in Table 1 ) and are likely to be more informative than in W-NUT."}, {"id": 175, "string": "Figure 5 presents the estimated PDFs of user network representations for Attention UN ."}, {"id": 176, "string": "These PDFs are estimated from the development sets of TwitterUS and W-NUT, where both input representations are available."}, {"id": 177, "string": "Strong preference of network representation for TwitterUS against W-NUT is found in the PDFs."}, {"id": 178, "string": "This finding is intuitive because TwitterUS has substantially more user network edges (reduced-edge/user in Table 1 ) than W-NUT, which is likely to benefit more from user network information."}, {"id": 179, "string": "Attention Patterns We further analyzed the proposed model by clustering attention probabilities to capture typical attention patterns."}, {"id": 180, "string": "For each user, we assigned six attention probabilities of Attention U and Attention UN as features for a clustering."}, {"id": 181, "string": "A kmeans clustering was performed over these users with 9 clusters."}, {"id": 182, "string": "The clustering clearly separated the users to 5 clusters for TwitterUS users and 4 clusters for W-NUT users."}, {"id": 183, "string": "We extracted typical users of each cluster by selecting the closest users of the cluster centroids."}, {"id": 184, "string": "Figure 6 shows a clustering result and the attention probabilities of these users."}, {"id": 185, "string": "These attention probabilities can be considered as typical attention patterns of the proposed model and match with the previously estimated PDFs."}, {"id": 186, "string": "For example, cluster 2 and 3 represent an attention pattern that processes users by balancing the representations of locations along with the representations of timelines."}, {"id": 187, "string": "Additionally, the location probabilities in this pattern are in the right tail region of the location PDF."}, {"id": 188, "string": "Limitations of Proposed Model City Prediction The evaluation produced improvements in most of our models in the four metrics."}, {"id": 189, "string": "One exception we found was the median distance error between SUB-NN-META and LR-STACKING in TwitterUS."}, {"id": 190, "string": "Because the median distance error of SUB-NN-META was quite low (46.8 km), we Table 4 denotes this oracle performance."}, {"id": 191, "string": "The oracle mean error distance is 31.4 km."}, {"id": 192, "string": "Its standard deviation is 30.1."}, {"id": 193, "string": "Note that ground truth locations of TwitterUS are geotags and will not exactly match the oracle city centers."}, {"id": 194, "string": "These oracle values imply that the current median error distances are close to the lower bound of the city classification approach and that they are difficult to improve."}, {"id": 195, "string": "Errors with High Confidences The proposed model still contains 28-30% errors even in accuracy@161."}, {"id": 196, "string": "A qualitative analysis of errors with high confidences was performed to investigate cases that the model fails."}, {"id": 197, "string": "We found two common types of error in the error analysis."}, {"id": 198, "string": "The first is a case when a location field is incorrect due to a reason such as a house move."}, {"id": 199, "string": "For example, the model predicted \"Hong Kong\" for a user with a location field of \"Hong Kong\" but has the gold location of \"Toronto\"."}, {"id": 200, "string": "The second is a case when a user tweets a place name of a travel."}, {"id": 201, "string": "For example, the model predicted \"San Francisco\" for a user who tweeted about a travel to \"San Francisco\" but has the gold location of \"Boston\"."}, {"id": 202, "string": "These two types of error are difficult to handle with the current architecture of the proposed model."}, {"id": 203, "string": "The architecture only supports single location field which disables the model to track location changes."}, {"id": 204, "string": "The architecture also treats each tweet independently which forbids the model to express a temporal state like traveling."}, {"id": 205, "string": "Conclusion As described in this paper, we proposed a complex neural network model for geolocation prediction."}, {"id": 206, "string": "The model unifies text, metadata, and user network information."}, {"id": 207, "string": "The model achieved the maximum of a 3.8% increase in accuracy and a maximum of 6.6% increase in accuracy@161 against several previous state-of-the-art models."}, {"id": 208, "string": "We further analyzed the states of several attention layers, which revealed that the probabilities assigned to timeline representations and user network representations match to some statistical characteristics of datasets."}, {"id": 209, "string": "As future works of this study, we are planning to expand the proposed model to handle multiple locations and a temporal state to capture location changes and states like traveling."}, {"id": 210, "string": "Additionally, we plan to apply the proposed model to other social media analyses such as gender analysis and age analysis."}, {"id": 211, "string": "In these analyses, metadata like location fields and timezones may not be effective like in geolocation prediction."}, {"id": 212, "string": "However, a user network is known to include various user attributes information including gender and age (McPherson et al., 2001) which suggests the unification of text and user network information to result in a success as in geolocation prediction."}, {"id": 213, "string": "A Supplemental Materials A.1 Parameters of Embedding Pre-training Word embeddings were pre-trained with the parameters of learning rate=0.025, window size=5, negative sample size=5, and epoch=5."}, {"id": 214, "string": "User embeddings were pre-trained with the parameters of initial learning rate=0.025, order=2, negative sample size=5, and training sample size=100M."}, {"id": 215, "string": "A.2 Model Parameters and Parameter Selection Strategies Unit Sizes, Embedding Dimensions, and a Max Tweet Number The layers and the embeddings in our models have unit size and embedding dimension parameters."}, {"id": 216, "string": "We also restricted the maximum number of tweets per user for TwitterUS to reduce memory footprints."}, {"id": 217, "string": "Table 5 shows the values for these parameters."}, {"id": 218, "string": "Smaller values were set for TwitterUS because TwitterUS is approximately 2.6 times larger in terms of tweet number."}, {"id": 219, "string": "It was computationally expensive to process TwiiterUS in the same settings as W-NUT."}, {"id": 220, "string": "Regularization Parameters and Bucket Sizes We chose optimal values of \u03b1 using a grid search with the development sets of TwitterUS and W-NUT."}, {"id": 221, "string": "The range of \u03b1 was set as the following: \u03b1 \u2208 {1e \u22124 , 5e \u22125 , 1e \u22125 , 5e \u22126 , 1e \u22126 , 5e \u22127 , 1e \u22127 , 5e \u22128 , 1e \u22128 }."}, {"id": 222, "string": "We also chose optimal values of c using grid search with the development sets of TwitterUS and W-NUT for the baseline models."}, {"id": 223, "string": "The range of c was set as the following for TwitterUS: c \u2208 {50, 100, 150, 200, 250, 300, 339}."}, {"id": 224, "string": "The following was set for W-NUT: c \u2208 {100, 200, 300, 400, 500, 600, 700, 800, 900, 1000, 1500, 2000, 2500, 3000, 3028} ."}, {"id": 225, "string": "Table 6 presents selected values of \u03b1 and c. For LR-STACK and MADCEl-B-LR-STACK, different parameters of \u03b1 and c were selected for each logistic regression classifier."}, {"id": 226, "string": "MAD Parameters and Celebrity Threshold The MAD parameters \u00b5 1 , \u00b5 2 , and \u00b5 3 and celebrity threshold t were also chosen using grid search with the development sets of TwitterUS and W-NUT."}, {"id": 227, "string": "The ranges of \u00b5 1 , \u00b5 2 , and \u00b5 3 were set as the following: \u00b5 1 \u2208 {1.0}, \u00b5 2 \u2208 {0.001, 0.01, 0.1, 1.0, 10.0}, \u00b5 3 \u2208 {0.0, 0.001, 0.01, 0.1, 1.0, 10.0}."}, {"id": 228, "string": "The range of t for TwitterUS was set as t \u2208 {2, ."}, {"id": 229, "string": "."}, {"id": 230, "string": "."}, {"id": 231, "string": ", 16}."}, {"id": 232, "string": "The range of t for W-NUT was set Table 6 : Regularization parameters and bucket sizes selected for our models and baseline models."}, {"id": 233, "string": "Table 7 : MAD parameters and celebrity threshold selected for baseline models."}, {"id": 234, "string": "as t \u2208 {2, ."}, {"id": 235, "string": "."}, {"id": 236, "string": "."}, {"id": 237, "string": ", 6}."}, {"id": 238, "string": "Table 6 presents selected values of \u00b5 1 , \u00b5 2 , \u00b5 3 , and t."}], "headers": [{"section": "Introduction", "n": "1", "start": 0, "end": 33}, {"section": "User-network-based Approach", "n": "2.2", "start": 34, "end": 42}, {"section": "Metadata-based Approach", "n": "2.3", "start": 43, "end": 47}, {"section": "Combinatory Approach Extending", "n": "2.4", "start": 48, "end": 51}, {"section": "Comparisons with Proposed Model", "n": "2.5", "start": 52, "end": 59}, {"section": "Proposed Model", "n": "3.1", "start": 60, "end": 63}, {"section": "Text Component", "n": "3.1.1", "start": 64, "end": 74}, {"section": "Text and Metadata Component", "n": "3.1.2", "start": 75, "end": 88}, {"section": "User Network Component", "n": "3.1.3", "start": 89, "end": 95}, {"section": "Construction of the User Network", "n": "4.2", "start": 96, "end": 104}, {"section": "Evaluation", "n": "5", "start": 105, "end": 108}, {"section": "MADCEL-B-LR", "n": "5.1.2", "start": 109, "end": 115}, {"section": "LR-STACK", "n": "5.1.3", "start": 116, "end": 121}, {"section": "MADCEL-B-LR-STACK", "n": "5.1.4", "start": 122, "end": 122}, {"section": "Model Configurations 5.2.1 Text Processor", "n": "5.2", "start": 123, "end": 125}, {"section": "Pre-training of Embeddings", "n": "5.2.2", "start": 126, "end": 128}, {"section": "Neural Network Optimization", "n": "5.2.3", "start": 129, "end": 131}, {"section": "Model Parameters", "n": "5.2.4", "start": 132, "end": 136}, {"section": "Metrics", "n": "5.2.5", "start": 137, "end": 147}, {"section": "Result Performance on TwitterUS", "n": "5.3", "start": 148, "end": 167}, {"section": "Unification Strategies", "n": "6.1.1", "start": 168, "end": 178}, {"section": "Attention Patterns", "n": "6.1.2", "start": 179, "end": 187}, {"section": "City Prediction", "n": "6.2.1", "start": 188, "end": 194}, {"section": "Errors with High Confidences", "n": "6.2.2", "start": 195, "end": 204}, {"section": "Conclusion", "n": "7", "start": 205, "end": 238}], "figures": [{"filename": "../figure/image/1264-Table2-1.png", "caption": "Table 2: Performances of our models and the baseline models on TwitterUS. Significance tests were performed between models with same Sign. Test IDs. The shaded lines represent values copied from related papers. Asterisks denote significant improvements against paired counterparts with 1% confidence (**) and 5% confidence (*).", "page": 6, "bbox": {"x1": 98.88, "x2": 498.24, "y1": 61.44, "y2": 240.0}}, {"filename": "../figure/image/1264-Table3-1.png", "caption": "Table 3: Performance of our models and baseline models on W-NUT. The same notations as those in Table 2 are used in this table.", "page": 6, "bbox": {"x1": 98.88, "x2": 498.24, "y1": 302.88, "y2": 441.12}}, {"filename": "../figure/image/1264-Figure1-1.png", "caption": "Figure 1: Overview of the proposed model. RNN denotes a recurrent neural network layer. FC denotes a fully connected layer. The striped layers are message-level processes. \u2295 represents element-wise addition.", "page": 2, "bbox": {"x1": 138.72, "x2": 458.4, "y1": 62.879999999999995, "y2": 321.12}}, {"filename": "../figure/image/1264-Table6-1.png", "caption": "Table 6: Regularization parameters and bucket sizes selected for our models and baseline models.", "page": 12, "bbox": {"x1": 306.71999999999997, "x2": 527.04, "y1": 224.64, "y2": 444.0}}, {"filename": "../figure/image/1264-Table7-1.png", "caption": "Table 7: MAD parameters and celebrity threshold selected for baseline models.", "page": 12, "bbox": {"x1": 306.71999999999997, "x2": 527.04, "y1": 484.79999999999995, "y2": 601.92}}, {"filename": "../figure/image/1264-Table5-1.png", "caption": "Table 5: Unit sizes, embedding dimensions, and max tweet numbers of our models.", "page": 12, "bbox": {"x1": 310.56, "x2": 521.28, "y1": 61.44, "y2": 185.28}}, {"filename": "../figure/image/1264-Figure4-1.png", "caption": "Figure 4: Estimated probability density functions of the four representations in AttentionU.", "page": 7, "bbox": {"x1": 66.72, "x2": 299.03999999999996, "y1": 64.8, "y2": 246.23999999999998}}, {"filename": "../figure/image/1264-Figure5-1.png", "caption": "Figure 5: Estimated probability density functions of user network representations in AttentionUN.", "page": 7, "bbox": {"x1": 353.76, "x2": 479.03999999999996, "y1": 63.839999999999996, "y2": 161.28}}, {"filename": "../figure/image/1264-Figure3-1.png", "caption": "Figure 3: Overview of the user network component with a detailed description of the elementwise addition and AttentionN.", "page": 3, "bbox": {"x1": 301.44, "x2": 525.12, "y1": 62.879999999999995, "y2": 244.32}}, {"filename": "../figure/image/1264-Figure2-1.png", "caption": "Figure 2: Overview of the text component with detailed description of RNNM and AttentionM.", "page": 3, "bbox": {"x1": 72.0, "x2": 296.15999999999997, "y1": 63.839999999999996, "y2": 240.0}}, {"filename": "../figure/image/1264-Table4-1.png", "caption": "Table 4: Error distance values in TwitterUS with oracle predictions. \u03c3 in the table denotes the standard deviation.", "page": 8, "bbox": {"x1": 110.88, "x2": 251.04, "y1": 232.79999999999998, "y2": 274.08}}, {"filename": "../figure/image/1264-Figure6-1.png", "caption": "Figure 6: A k-means clustering result and the attention probabilities of users that are closest to the cluster centroids. The underlined values are the max values of the two datasets for each column.", "page": 8, "bbox": {"x1": 72.96, "x2": 522.24, "y1": 67.67999999999999, "y2": 192.95999999999998}}, {"filename": "../figure/image/1264-Table1-1.png", "caption": "Table 1: Some properties of TwitterUS (train) and W-NUT (train). We were able to obtain approximately 70\u201378% of the full datasets because of accessibility changes in Twitter.", "page": 4, "bbox": {"x1": 100.8, "x2": 261.12, "y1": 61.44, "y2": 179.04}}]}