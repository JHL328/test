{"title": "Using Natural Language Relations between Answer Choices for Machine Comprehension", "abstract": "When evaluating an answer choice for Reading Comprehension task, other answer choices available for the question and the answers of related questions about the same paragraph often provide valuable information. In this paper, we propose a method to leverage the natural language relations between the answer choices, such as entailment and contradiction, to improve the performance of machine comprehension. We use a stand-alone question answering (QA) system to perform QA task and a Natural Language Inference (NLI) system to identify the relations between the choice pairs. Then we perform inference using an Integer Linear Programming (ILP)-based relational framework to re-evaluate the decisions made by the standalone QA system in light of the relations identified by the NLI system. We also propose a multitask learning model that learns both the tasks jointly.", "text": [{"id": 0, "string": "Introduction Given an input text and a set of related questions with multiple answer choices, the reading comprehension (RC) task evaluates the correctness of each answer choice."}, {"id": 1, "string": "Current approaches to the RC task quantify the relationship between each question and answer choice independently and pick the highest scoring option."}, {"id": 2, "string": "In this paper, we follow the observation that when humans approach such RC tasks, they tend to take a holistic view ensuring that their answers are consistent across the given questions and answer choices."}, {"id": 3, "string": "In this work we attempt to model these pragmatic inferences, by leveraging the entailment and contradiction relations between the answer choices to improve machine comprehension."}, {"id": 4, "string": "To help clarify these concepts, consider the following examples: How can the military benefit from the existence of the CIA?"}, {"id": 5, "string": "c 1 : They can use them c 2 : These agencies are keenly attentive to the military's strategic and tactical requirements () c 3 : The CIA knows what intelligence the military requires and has the resources to obtain that intelligence () The above example contains multiple correct answer choices, some are easier to capture than others."}, {"id": 6, "string": "For example, identifying that c 3 is true might be easier than c 2 based on its alignment with the input text."}, {"id": 7, "string": "However, capturing that c 3 entails c 2 allows us to predict c 2 correctly as well."}, {"id": 8, "string": "Classification of the answer in red (marked ) could be corrected using the blue (marked ) answer choice."}, {"id": 9, "string": "Q1: When were the eggs added to the pan to make the omelette?"}, {"id": 10, "string": "c 1 1 : When they turned on the stove c 1 2 : When the pan was the right temperature () Q2: Why did they use stove to cook omelette?"}, {"id": 11, "string": "c 2 1 : They didn't use the stove but a microwave c 2 2 : Because they needed to heat up the pan () Similarly, answering Q1 correctly helps in answering Q2."}, {"id": 12, "string": "Our goal is to leverage such inferences for machine comprehension."}, {"id": 13, "string": "Our approach contains three steps."}, {"id": 14, "string": "First, we use a stand-alone QA system to classify the answer choices as true/false."}, {"id": 15, "string": "Then, we classify the relation between each pair of choices for a given question as entailment, contradiction or neutral."}, {"id": 16, "string": "Finally, we re-evaluate the labels assigned to choices using an Integer Linear Programming based inference procedure."}, {"id": 17, "string": "We discuss different training protocols and representation choices for the combined decision problem."}, {"id": 18, "string": "An overview is in figure 1."}, {"id": 19, "string": "We empirically evaluate on two recent datasets, MultiRC (Khashabi et al., 2018) and SemEval-2018 task-11 (Ostermann et al., 2018 and show that it improves machine comprehension in both."}, {"id": 20, "string": "Related Work Recently, several QA datasets have been proposed to test machine comprehension (Richardson, 2013; Weston et al., 2015; Rajpurkar et al., 2016; Trischler et al., 2016a; Nguyen et al., 2016) ."}, {"id": 21, "string": "Yatskar (2018) showed that a high performance on these datasets could be achieved without necessarily achieving the capability of making commonsense inferences."}, {"id": 22, "string": "Trischler et al."}, {"id": 23, "string": "(2016b) , Kumar et al."}, {"id": 24, "string": "(2016) , Liu and Perez (2017) , Min et al."}, {"id": 25, "string": "(2018) and Xiong et al."}, {"id": 26, "string": "(2016) proposed successful models on those datasets."}, {"id": 27, "string": "To address this issue, new QA datasets which require commonsense reasoning have been proposed (Khashabi et al., 2018; Ostermann et al., 2018; ."}, {"id": 28, "string": "Using common sense inferences in Machine Comprehension is a far from solved problem."}, {"id": 29, "string": "There have been several attempts in literature to use inferences to answer questions."}, {"id": 30, "string": "Most of the previous works either attempt to infer the answer from the given text (Sachan and Xing, 2016; or an external commonsense knowledge base (Das et al., 2017; Mihaylov and Frank, 2018; Bauer et al., 2018; Weissenborn et al., 2017) ."}, {"id": 31, "string": "While neural models can capture some dependencies between choices through shared representations, to the best of our knowledge, inferences capturing the dependencies between answer choices or different questions have been not explicitly modeled."}, {"id": 32, "string": "Model Formally, the task of machine comprehension can be defined as: given text P and a set of n related questions Q = {q 1 , q 2 , ."}, {"id": 33, "string": "."}, {"id": 34, "string": "."}, {"id": 35, "string": ", q n } each having m choices C = {c i 1 , c i 2 , ."}, {"id": 36, "string": "."}, {"id": 37, "string": "."}, {"id": 38, "string": ", c i m }\u2200q i \u2208 Q, the task is to assign true/false value for each choice c i j ."}, {"id": 39, "string": "Model Architecture Our model consists of three separate systems, one for each step, namely, the stand-alone question answering (QA) system, the Natural Language Inference (NLI) system and the inference framework connecting the two."}, {"id": 40, "string": "First, we assign a true/false label to each question-choice pair using the standalone QA system along with an associated confidence score s 1 ."}, {"id": 41, "string": "Consequently, we identify the natural language relation (entailment, contradiction or neutral) between each ordered pair of choices for a given question, along with an associated confidence score s 2 ."}, {"id": 42, "string": "Then, we use a relational framework to perform inference using the information obtained from the stand-alone QA and the NLI systems."}, {"id": 43, "string": "Each of the components is described in detail in the following sub-sections."}, {"id": 44, "string": "We further propose a joint model whose parameters are trained jointly on both the tasks."}, {"id": 45, "string": "The joint model uses the answer choice representation generated by the stand-alone QA system as input to the NLI detection system."}, {"id": 46, "string": "The architecture of our joint model is shown in figure 2."}, {"id": 47, "string": "Stand-alone QA system We use the TriAN-single model proposed by  for SemEval-2018 task-11 as our stand-alone QA system."}, {"id": 48, "string": "We use the implementation 1 provided by  for our experiments."}, {"id": 49, "string": "The system is a tri-attention model that takes passage-question-choice triplet as input and produces the probability of the choice being true as its output."}, {"id": 50, "string": "NLI System Our NLI system is inspired from decomposableattention model proposed by Parikh et al."}, {"id": 51, "string": "(2016) ."}, {"id": 52, "string": "We modified the architecture proposed in Parikh et al."}, {"id": 53, "string": "(2016) to accommodate the question-choice pairs as opposed to sentence pairs in the original model."}, {"id": 54, "string": "We added an additional sequence-attention layer for the question-choice pairs to allow for the Att seq (u, {v i } n i=1 ) = n i=1 \u03b1 i v i \u03b1 i = sof tmax i (f (W 1 u) T f (W 1 v i )) (1) where u and v i are word embeddings, W 1 is the associated weight parameter and f is non-linearity."}, {"id": 55, "string": "Self-attention is Att seq of a vector onto itself."}, {"id": 56, "string": "The embedding of each word in the answer choice is attended to by the sequence of question word embeddings."}, {"id": 57, "string": "We use pre-trained GloVe (Pennington et al., 2014) embeddings to represent the words."}, {"id": 58, "string": "The question-attended choices are then passed through the decomposable-attention layer proposed in Parikh et al."}, {"id": 59, "string": "(2016) ."}, {"id": 60, "string": "Inference using DRAIL We use Deep Relational Learning (DRaiL) framework proposed by  to perform the final inference."}, {"id": 61, "string": "The framework allows for declaration of predicate logic rules to perform relational inference."}, {"id": 62, "string": "The rules are scored by the confidence scores obtained from the stand-alone QA and the NLI systems."}, {"id": 63, "string": "DRaiL uses an Integer Linear Programming (ILP) based inference procedure to output binary prediction for each of the choices."}, {"id": 64, "string": "We use the following constraints for our inference: 1. c i is true & c i entails c j =\u21d2 c j is true."}, {"id": 65, "string": "2. c i is true & c i contradicts c j =\u21d2 c j is false."}, {"id": 66, "string": "On the MultiRC dataset, we use the dependencies between the answer choices for a given question."}, {"id": 67, "string": "On SemEval dataset, we use the dependencies between different questions about the same paragraph."}, {"id": 68, "string": "Joint Model The design of our joint model is motivated by the two objectives: 1) to obtain a better representation for the question-choice pair for NLI detection and 2) to leverage the benefit of multitask learning."}, {"id": 69, "string": "Hence, in the joint model, choice representation from stand-alone QA system is input to the decomposable-attention layer of the NLI system."}, {"id": 70, "string": "The joint model takes two triplets (p, q i , c i ) and (p, q j , c j ) as input."}, {"id": 71, "string": "It outputs a true/false for each choice and an NLI relation (entailment, contradiction or neutral) between the choices."}, {"id": 72, "string": "The representations for passage, question and choice are obtained using Bi-LSTMs."}, {"id": 73, "string": "The hidden states of the Bi-LSTM are concatenated to generate the representation."}, {"id": 74, "string": "This part of the model is similar to TriAN model proposed in ."}, {"id": 75, "string": "The choice representations of c i and c j are passed as input to the decomposable attention layer proposed in Parikh et al."}, {"id": 76, "string": "(2016) ."}, {"id": 77, "string": "The architecture of the joint model is shown in figure 2."}, {"id": 78, "string": "Training We train the stand-alone QA system using the MultiRC and SemEval datasets for respective experiments."}, {"id": 79, "string": "We experiment with 2 different training settings for the NLI system."}, {"id": 80, "string": "In the first setting, we use SNLI dataset (Bowman et al., 2015) to train the NLI system."}, {"id": 81, "string": "The sequence-attention layer is left untrained during this phase."}, {"id": 82, "string": "Hence, we only use the answer choice and do not consider the question for NLI detection."}, {"id": 83, "string": "Self-Training: Subsequently, to help the system adapt to our settings, we devise a self-training protocol over the RC datasets to train the NLI sys-tem."}, {"id": 84, "string": "Self-training examples for the NLI system were obtained using the following procedure: if the SNLI-trained NLI model predicted entailment and the gold labels of the ordered choice pair were true-true, then the choice pair is labeled as entailment."}, {"id": 85, "string": "Similarly, if the SNLI-trained NLI model predicted contradiction and the gold labels of the ordered choice pair were true-false, then the choice pair is labeled as contradiction."}, {"id": 86, "string": "This is noisy labelling as the labels do not directly indicate the presence of NLI relations between the choices."}, {"id": 87, "string": "The NLI model was additionally trained using this data."}, {"id": 88, "string": "To train the joint model we use ordered choice pairs, labeled as entailment if the gold labels are true-true and labeled as contradiction if the gold labels are true-false."}, {"id": 89, "string": "This data was also used to test the effectiveness of the self-training procedure."}, {"id": 90, "string": "The results on the development set of MultiRC dataset are in table 1."}, {"id": 91, "string": "The NLI model trained on SNLI dataset achieves 55.11% accuracy."}, {"id": 92, "string": "Training the N LI model on the data from MultiRC data increases the overall accuracy to 66.31%."}, {"id": 93, "string": "Further discussion about self-training is provided in section 5."}, {"id": 94, "string": "Experiments We perform experiments in four phases."}, {"id": 95, "string": "In the first phase, we evaluate the stand-alone QA system."}, {"id": 96, "string": "In the second phase, we train the NLI system on SNLI data and evaluate the approach shown in figure 1."}, {"id": 97, "string": "In the third phase, we train the NLI system using the self-training data."}, {"id": 98, "string": "In the fourth phase, we evaluate the proposed joint model."}, {"id": 99, "string": "We evaluate all models on MultiRC dataset."}, {"id": 100, "string": "The results are shown in table 2."}, {"id": 101, "string": "We evaluate the joint model on SemEval dataset, shown in table 3."}, {"id": 102, "string": "Datasets We use two datasets for our experiments, MultiRC dataset 2 and the SemEval 2018 task 11 dataset 3 ."}, {"id": 103, "string": "MultiRC dataset consisted of a training and development set with a hidden test set."}, {"id": 104, "string": "We split the given training set into training and development sets and use the given development set as test set."}, {"id": 105, "string": "Each question in the MultiRC dataset has approximately 5 choices on average."}, {"id": 106, "string": "Multiple of them may be true for a given question."}, {"id": 107, "string": "The training split of MultiRC consisted of 433 paragraphs and 4, 853 questions with 25, 818 answer choices."}, {"id": 108, "string": "The development split has 23 paragraphs and 275 questions with 1, 410 answer choices."}, {"id": 109, "string": "Test set has 83 paragraphs and 953 questions with 4, 848 answer choices."}, {"id": 110, "string": "SemEval dataset has 2 choices for each question, exactly one of them is true."}, {"id": 111, "string": "The training set consists of 1, 470 paragraphs with 9, 731 questions."}, {"id": 112, "string": "The development set has 219 paragraphs with 1, 411 questions."}, {"id": 113, "string": "And the test set has 430 paragraphs with 2, 797 questions."}, {"id": 114, "string": "Evaluation Metrics For MultiRC dataset, we use two metrics for evaluating our approach, namely EM 0 and EM 1."}, {"id": 115, "string": "EM 0 refers to the percentage of questions for which all the choices have been correctly classified."}, {"id": 116, "string": "EM 1 is the the percentage of questions for which at most one choice is wrongly classified."}, {"id": 117, "string": "For the SemEval dataset, we use accuracy metric."}, {"id": 118, "string": "Results Results of our experiments are summarized in tables 2 & 3."}, {"id": 119, "string": "EM 0 on MC task improves from 18.15% to 19.41% when we use the NLI model trained over SNLI data and it further improves to 21.62% when we use MultiRC self-training data."}, {"id": 120, "string": "Joint model achieves 20.36% on EM 0 but achieves the highest EM 1 of 57.08%."}, {"id": 121, "string": "Human EM 0 is 56.56%."}, {"id": 122, "string": "."}, {"id": 123, "string": "The results we obtained using their implementation are stand-alone QA results."}, {"id": 124, "string": "With the same setting, joint model got 85.4% on dev set and 82.1% on test set."}, {"id": 125, "string": "The difference in performance of the models in tables 2 and 3 is statistically significant according to Mc-Nemar's chi-squared test."}, {"id": 126, "string": "Method Model Dev Test TriAN-single  83.84% 81.94% Stand-alone QA 83.20% 80.80% Joint Model 85.40% 82.10% Table 3 : Accuracy of various models on SemEval'18 task-11 dataset Discussion We have shown that capturing the relationship between various answer choices or subsequent questions helps in answering questions better."}, {"id": 127, "string": "Our experimental results, shown in tables 2 & 3, are only a first step towards leveraging this relationship to help construct better machine reading systems."}, {"id": 128, "string": "We suggest two possible extensions to our model, that would help realize the potential of these relations."}, {"id": 129, "string": "1."}, {"id": 130, "string": "Improving the performance of entailment and contradiction detection."}, {"id": 131, "string": "2."}, {"id": 132, "string": "Using the information given in the text to identify the relations between choices better."}, {"id": 133, "string": "As shown in table 1, identification of entailment/contradiction is far from perfect."}, {"id": 134, "string": "Entailment detection is particularly worse because often the system returns entailment when there is a high lexical overlap."}, {"id": 135, "string": "Moreover, the presence of a strong negation word (not) causes the NLI system to predict contradiction even for entailment and neutral cases."}, {"id": 136, "string": "This issue impedes the performance of our model on SemEval'18 dataset as roughly 40% of the questions have yes/no answers."}, {"id": 137, "string": "Naik et al."}, {"id": 138, "string": "(2018) show that this is a common issue with stateof-the-art NLI detection models."}, {"id": 139, "string": "Self-training (table 1) results suggest that there are other types of relationships present among answer choice pairs that do not come under the strict definitions of entailment or contradiction."}, {"id": 140, "string": "Upon investigating, we found that although some answer hypotheses do not directly have an inference relation between them, they might be related in context of the given text."}, {"id": 141, "string": "For example, consider the sentence, 'I snack when I shop' and the answer choices: c 1 : 'She went shopping this extended weekend' and c 2 : 'She ate a lot of junk food recently'."}, {"id": 142, "string": "Although the sentences don't have an explicit relationship when considered in isolation, the text suggests that c 1 might entail c 2 ."}, {"id": 143, "string": "Capturing these kinds of relationships could potentially improve MC further."}, {"id": 144, "string": "Conclusion In this paper we take a first step towards modeling an accumulative knowledge state for machine comprehension, ensuring consistency between the model's answers."}, {"id": 145, "string": "We show that by adapting NLI to the MC task using self-training, performance over multiple tasks improves."}, {"id": 146, "string": "In the future, we intend to generalize our model to other relationships beyond strict entailment and contradiction relations."}], "headers": [{"section": "Introduction", "n": "1", "start": 0, "end": 19}, {"section": "Related Work", "n": "2", "start": 20, "end": 31}, {"section": "Model", "n": "3", "start": 32, "end": 38}, {"section": "Model Architecture", "n": "3.1", "start": 39, "end": 46}, {"section": "Stand-alone QA system", "n": "3.1.1", "start": 47, "end": 49}, {"section": "NLI System", "n": "3.2", "start": 50, "end": 59}, {"section": "Inference using DRAIL", "n": "3.2.1", "start": 60, "end": 67}, {"section": "Joint Model", "n": "3.3", "start": 68, "end": 77}, {"section": "Training", "n": "3.4", "start": 78, "end": 93}, {"section": "Experiments", "n": "4", "start": 94, "end": 101}, {"section": "Datasets", "n": "4.1", "start": 102, "end": 113}, {"section": "Evaluation Metrics", "n": "4.2", "start": 114, "end": 117}, {"section": "Results", "n": "4.3", "start": 118, "end": 126}, {"section": "Discussion", "n": "5", "start": 127, "end": 142}, {"section": "Conclusion", "n": "6", "start": 143, "end": 146}], "figures": [{"filename": "../figure/image/1356-Figure2-1.png", "caption": "Figure 2: Architecture of the Joint Model", "page": 2, "bbox": {"x1": 96.96, "x2": 265.44, "y1": 61.44, "y2": 452.15999999999997}}, {"filename": "../figure/image/1356-Table3-1.png", "caption": "Table 3: Accuracy of various models on SemEval\u201918 task-11 dataset", "page": 4, "bbox": {"x1": 84.96, "x2": 277.44, "y1": 166.56, "y2": 238.07999999999998}}, {"filename": "../figure/image/1356-Figure1-1.png", "caption": "Figure 1: Proposed Approach", "page": 1, "bbox": {"x1": 88.8, "x2": 274.08, "y1": 99.84, "y2": 298.08}}, {"filename": "../figure/image/1356-Table1-1.png", "caption": "Table 1: Accuracy of entailment and contradiction detection on the development set of self-training data for NLI model trained on SNLI data (NLISNLI ) vs training set of selftraining data (NLIMultiRC )", "page": 3, "bbox": {"x1": 72.0, "x2": 298.08, "y1": 250.56, "y2": 293.28}}, {"filename": "../figure/image/1356-Table2-1.png", "caption": "Table 2: Summary of the results on MultiRC dataset. EM0 is the percentage of questions for which all the choices are correct. EM1 is the the percentage of questions for which at most one choice is wrong.", "page": 3, "bbox": {"x1": 329.76, "x2": 503.03999999999996, "y1": 583.68, "y2": 668.16}}]}